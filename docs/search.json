[
  {
    "objectID": "chapters/version_control.html#version-control",
    "href": "chapters/version_control.html#version-control",
    "title": "5  Version Control with Git and Github",
    "section": "5.1 Version Control",
    "text": "5.1 Version Control\nImagine you’re working on a crucial research paper or a script for an important presentation. You’ve spent hours perfecting it, and it’s almost complete. Then, you suddenly realize you liked the version you had two days ago better, but you’ve already overwritten it. Or, even worse, your computer crashes, and you lose all your progress. Panic ensues!\nFear not, version control is here to save the day! Think of version control as a superhero that helps you keep track of every change you make to your documents, code, or data files. It’s like having a time machine that allows you to travel back to any point in your project’s history and recover any previous version of your work. Sounds magical, right?\nHere’s how version control works:\n\nSnapshots: Every time you save your work, version control takes a snapshot, preserving that particular version. You can add a short message to each snapshot, describing the changes you made, making it easier to remember what you did.\nBranching: Want to try out a bold new idea, but afraid it might not work? No problem! With version control, you can create a separate “branch” and experiment without affecting the main version. If your idea works, you can “merge” the changes back into the main branch. If not, just discard the experimental branch and pretend it never happened.\nCollaboration: Working with a team? Version control makes collaborating a breeze. Each team member can work on their part of the project, and the superhero will intelligently combine all the changes into a cohesive whole. No more messy email chains with countless attachments and confusing file names like “Final_Version_3_revised_edited_FINAL.”\nBackup: Version control also acts as a backup system, ensuring your work is safely stored in a remote location. So even if your computer decides to give up on you, your project remains secure and accessible."
  },
  {
    "objectID": "chapters/version_control.html#git-and-github",
    "href": "chapters/version_control.html#git-and-github",
    "title": "5  Version Control with Git and Github",
    "section": "5.2 Git and Github",
    "text": "5.2 Git and Github\nWhen introducing GitHub to beginning economists as a version control and collaboration tool, you should cover the following key points:\n\nWhat is GitHub?\n\nExplain that GitHub is a web-based platform built on Git, a distributed version control system.\nMention that it allows users to create, manage, and collaborate on repositories (projects) in an organized and efficient manner.\n\nWhy use GitHub for version control?\n\nEmphasize the importance of version control for tracking changes in code, data, and documents.\nDiscuss how GitHub helps avoid conflicts and overwriting each other’s work when multiple people are collaborating on a project.\nExplain that GitHub maintains a complete history of changes, making it easy to revert to previous versions if needed.\n\nBasic GitHub terminology:\n\nRepository: A project folder containing all files and their history.\nClone: Creating a local copy of a repository on your computer.\nCommit: Saving changes to a file or a set of files in the repository.\nPush: Uploading local commits to the remote repository on GitHub.\nPull: Updating your local repository with the latest changes from the remote repository.\nBranch: A separate version of the repository, often used to work on a specific feature or bugfix.\nMerge: Combining changes from different branches into a single branch.\nPull request: Requesting that changes from one branch be merged into another branch, typically used for collaboration and code reviews.\n\nGetting started with GitHub:\n\nSigning up for a GitHub account.\nCreating and cloning repositories.\nMaking changes, committing, and pushing to the remote repository.\nCollaborating with others by inviting collaborators, creating branches, and using pull requests.\n\nGitHub integrations and tools:\n\nExplain that GitHub can be integrated with various tools and platforms, such as continuous integration services, project management tools, and code editors.\nIntroduce GitHub Desktop, a graphical user interface for managing repositories, as an alternative to using the command line.\n\nPractical examples for economists:\n\nShare examples of how economists can use GitHub for managing research papers, data analysis scripts (e.g., in R or Python), and teaching materials.\nDiscuss how GitHub can facilitate collaboration among researchers, making it easier to share and reproduce research findings.\n\n\nBy introducing these key points, you will provide beginning economists with a solid foundation for understanding and using GitHub for version control and collaboration in their projects."
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/summary.html",
    "href": "chapters/summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/setup.html#rstudio-set-up",
    "href": "chapters/setup.html#rstudio-set-up",
    "title": "3  Set up",
    "section": "3.1 RStudio Set Up",
    "text": "3.1 RStudio Set Up\nWe will be using R and RStudio IDE to download, clean, explore, and model our data. R language was developed by statisticians for statisticians and had intuitive syntax and workflow for creating readable and reproducible code."
  },
  {
    "objectID": "chapters/setup.html#download-r",
    "href": "chapters/setup.html#download-r",
    "title": "3  Set up",
    "section": "3.2 Download R",
    "text": "3.2 Download R\nR is maintained through The Comprehensive R Archive Network.\n\n3.2.1 For macOS\n\nGo to https://cran.r-project.org/\nSelect “Download R for macOS”\nIf you have Apple Silicon mac (M1,M2…) download the latest version that has -arm64 in its name (R-4.2.2-arm64.pkg)\nIf you have Intel Max Download one without -arm64 (R-4.2.2.pkg)\nFollow the steps from the installation wizzard\nThe installer lets you customize your installation, but the defaults will be suitable for most users\nYour computer might ask for your password before installing new programs\n\n\n\n3.2.2 For Windows\n\nGo to https://cran.r-project.org/\nSelect “Download R for Windows”\nClick on “base”\nClick the first link at the top of the new page (Download R-4.2.2 for Windows)\nink downloads an installer program, which installs the most up-to-date version of R for Windows.\nFollow the steps from the installation wizzard\nThe installer lets you customize your installation, but the defaults will be suitable for most users\nYou might need administration privileges to install new software on your machine"
  },
  {
    "objectID": "chapters/setup.html#download-rstudio",
    "href": "chapters/setup.html#download-rstudio",
    "title": "3  Set up",
    "section": "3.3 Download RStudio",
    "text": "3.3 Download RStudio\nRstudio is like a Microsoft Word for writing text, but instead of text RStudio helps you write in R. RStudio is also free and easy to install! Go to RStudio Website, click on DOWNLOAD RSTUDIO or select your operation system below, and follow the isntallation instructions."
  },
  {
    "objectID": "chapters/setup.html#configure-rstudio",
    "href": "chapters/setup.html#configure-rstudio",
    "title": "3  Set up",
    "section": "3.4 Configure RStudio",
    "text": "3.4 Configure RStudio\nFirst watch an introduction to RStudio: https://www.youtube.com/watch?v=FIrsOBy5k58 and introduction to projects: https://www.youtube.com/watch?v=MdTtTN8PUqU\nNow the most crucial part!!! Change the default theme of RStudio from light to dark! To do this\n\nGo to the top panel\nSelect the Tools tab\nNavigate to Global Options\nSelect Appearance\nIn Editor Theme, select “Dracula”\nClick Apply\n\nNow let’s install a cooler font with ligatures! Install fira-code: https://github.com/tonsky/FiraCode/wiki/Installing. Restart RStudio for the font to load. Then go back to Appearance, choose FiraCode, and hit apply."
  },
  {
    "objectID": "chapters/setup.html#install-packages",
    "href": "chapters/setup.html#install-packages",
    "title": "3  Set up",
    "section": "3.5 Install Packages",
    "text": "3.5 Install Packages\nAn R package is a collection of useful functions, documentation, and data sets that can be used in your own R code after it is loaded. These packages typically focus on a specific task and make use of pre-written routines for various data science tasks.\nYou can install packages with a single line of code:\ninstall.packages(\"tidyverse\")\nYou can also install multiple packages at the same time using c():\ninstall.packages(c(\"tidyverse\",\"gapminder\"))\nYou can load packages using the library() function:\nlibrary(\"tidyverse\")\nRun the following command to install packages we will use in class\ninstall.packages(c(\"tidyverse\", \"janitor\", \"esquisse\", \"modelsummary\", \"styler\"))"
  },
  {
    "objectID": "chapters/repres.html#literate-programming",
    "href": "chapters/repres.html#literate-programming",
    "title": "4  Reproducible Research",
    "section": "4.1 Literate Programming",
    "text": "4.1 Literate Programming\n\n\n\n\n\nflowchart TD\n    A[Measured Data] -->|Processing Code| B(Analytic Data)\n    B --> |Analytic Code| C(Computational Results)\n    C --> D{Presentation Code}\n    D --> G[Figures]\n    D --> E[Tables]\n    D --> F[Summaries]\n\n    G --> H[Article]\n    E --> H\n    F --> H\n\n    I[Text] --> H\n\n\n\n\n\n\n\n\nLiterate programming integrates text and code chunks, creating a seamless blend of human-readable explanations and machine-executable code. The code loads data, generates graphs, and runs models, while the text provides context and interprets the results. This approach enables researchers to produce documents that have both human- and machine-readable.\nOne of the earliest implementations of literate programming was Sweave, which combined LaTeX and R for documentation and programming. Since then, the field has evolved with the introduction of RMarkdown, Jupyter Notebooks, Python Markdown, etc. The latest development in this area is Quarto, which will be covered in this book. This tool continues to advance the concept of literate programming, offering researchers a comprehensive solution for creating transparent, reproducible, and well-documented research outputs."
  },
  {
    "objectID": "chapters/litreview.html#search",
    "href": "chapters/litreview.html#search",
    "title": "6  Literature Research",
    "section": "6.1 Search",
    "text": "6.1 Search\nAnd now the hunt begins!\nBegin your literature search by consulting a professor or an expert in the field for guidance. Review articles are also an excellent starting point, as they provide an overview of recent developments, summarize key findings, and identify gaps in knowledge.\nNext, explore peer-reviewed databases with your University’s Subscription such as Scopus and Web of Science, which offer high-quality articles due to their thorough peer-review process. Your school or university library is another valuable resource for accessing a wide range of academic materials, often for free.\n\n\n\n\n\n\nWarning\n\n\n\nLook out for predatory journals! These journals might sound legit, but they will publish anything for a fee. If you feel suspicious, look at the web page of the journal and check Beall’s List.\n\n\nAfter exhausting these resources, turn to Google Scholar for a larger collection of articles, but be cautious of questionable publications. If you’re interested in a book, check the author’s personal website for free chapters or the entire book.\nTo jump start your literature review, try AI platforms like Paper Digest or Elicit, which provide summaries of your topic and help find relevant papers to your research question. Visualize and explore connections between articles using tools like Research Rabbit or Lit Maps, both of which integrate with Zotero for easy library management.\nFor efficient research organization, consider using the Arc browser, which allows you to create organized spaces for each project and easily navigate folders for all your links and notes."
  },
  {
    "objectID": "chapters/litreview.html#reference-management",
    "href": "chapters/litreview.html#reference-management",
    "title": "6  Literature Research",
    "section": "6.2 Reference Management",
    "text": "6.2 Reference Management\nCitation managers are indispensable tools for organizing and managing your literature sources. The greatest of all time is Zotero, a free, open-source, and cross-platform tool with a vast library of extensions. With Zotero, you can add tags, related references, notes, annotations, and effortlessly export everything into your preferred note-taking app. It is also compatible with most writing programs, making citation a breeze in any environment. To simplify obtaining references from within the browser, take advantage of the Zotero web extension. Additionally, download the BetterBibTeX plugin, which enhances Zotero’s capabilities by generating more accurate and efficient citation keys and automating the export of your library to BibTeX. Numerous online resources are available to help you get started, so be sure to make the most of them!"
  },
  {
    "objectID": "chapters/litreview.html#reading",
    "href": "chapters/litreview.html#reading",
    "title": "6  Literature Research",
    "section": "6.3 Reading",
    "text": "6.3 Reading\nNow you have amassed your literature and all ready to go through them one-by-one. Before you start, make sure you filter all the papers based on their quality, impact, and relevance, concentrating on the most critical literature. A quick skim of title, abstract, conclusions, and figures should assist you with that. Once you did that the most relevant papers should boil up to the top and now it is time for deep work. Don’t be fooled by the structure of the papers, the section are not meant to be read in the order they appear. When reading a paper, consider approaching the conclusion, discussion, and methods sections last, allowing you to comprehend the key findings and implications before delving into technical details. To aid with the (sometime rather boring) reading using Speechify, a text-to-speech app that will read the articles out loud, keeping you on pace and making information easier to absorb. It is also common to have an article open in Speechify and Zotero to take notes."
  },
  {
    "objectID": "chapters/litreview.html#taking-notes",
    "href": "chapters/litreview.html#taking-notes",
    "title": "6  Literature Research",
    "section": "6.4 Taking notes",
    "text": "6.4 Taking notes\nAs you read, it’s crucial to write simultaneously. The approach you take will depend on the scope of your project. For shorter papers, consider writing notes in a Google document and later refining them into a coherent essay. For more extensive research projects, efficient note-taking and organization are essential.\nTry using tools like Obsidian to arrange notes and create networks of ideas using local plain markdown files. Alternatively, Notion is a versatile platform for organizing notes, tasks, and databases. Make the most of Zotero’s export capabilities to transfer your notes into Obsidian or other software.\nDon’t overlook the power of AI tools, such as ChatGPT, to enhance your literature review process. Use them to summarize text sections, improve your writing, or convert copied text into markdown tables. By leveraging these cutting-edge tools, you’ll avoid slowing down your work and ensure a more efficient review process. However, do not overly depend on them to do your work, always proofread, edit the output and paraphrase the output. It is a tool not a replacement of you expertise."
  },
  {
    "objectID": "chapters/write.html#wysiwyg",
    "href": "chapters/write.html#wysiwyg",
    "title": "7  Write",
    "section": "7.1 WYSIWYG",
    "text": "7.1 WYSIWYG\nWYSIWYG (What You See Is What You Get) editors are programs that allow you to make and edit content visually without having to know how to code. This means that you can see how your content will look as you’re creating it. They’re very helpful for people who don’t have experience with coding or markup languages. Examples of WYSIWYG editors that you may already be familiar with include Google Docs and Microsoft Word. Learning how to navigate these widely used tools will teach you valuable skills such as formatting and writing. Plus, after using them for a while, you’ll start to appreciate the simplicity and efficiency of markup languages.\nI strongly recommend that you develop expertise in these tools, as the skills you acquire are highly transferable to other similar editors. Additionally, it’s a valuable investment as your colleagues are likely to use them as well. To get started, I suggest checking out the MOS Certification Series on LinkedIn Learning. These tutorials are designed for those preparing to take MOS exams and cover a broad range of functionality in Word, Excel, and PowerPoint. Even if you don’t plan on taking the exams, I highly encourage you to watch the tutorials. They’re a great resource for building your skills and improving your proficiency with these essential tools!\nhere is the link"
  },
  {
    "objectID": "chapters/write.html#markup-languages",
    "href": "chapters/write.html#markup-languages",
    "title": "7  Write",
    "section": "7.2 Markup Languages",
    "text": "7.2 Markup Languages\nMarkup languages are sets of codes that provide structure and formatting to documents, such as web pages, eBooks, and scientific papers. Unlike WYSIWYG editors, which allow users to create content visually, markup languages require the use of specific tags or codes to indicate how the content should be displayed. These tags define the document structure, formatting, and other attributes. Some of the most commonly used markup languages include HTML, LaTeX, and Markdown. By learning a markup language, you will have greater control over the appearance and functionality of your digital documents. Additionally, markup languages can help you focus more on writing, allowing templates to handle all the formatting. I promise after you switch, there is no coming back! Your assignments and web pages will look beautiful every time.\n\n7.2.1 HTML\nWhen you hear markup language, you might think of Hypertext Markup Language (HTML), which is used everywhere on the web to build structure and content. HTML uses tags to describe how to display content in a browser. For instance, <br> will create a line break, <img> will add an image and so on. HTML is an essential component of web development, and learning how to use HTML is the first step towards customizing your own web pages and web applications. It is also often used in manipulating text in your graphs.\n\n\n7.2.2 LaTeX\nHave you ever wondered why academic papers look so beautiful or why every textbook looks the same? They are all written in LaTeX, a typesetting system used for academic and scientific publishing. It is a markup language that enables precise formatting of complex technical documents. It is also famous for beautiful mathematical equations. With LaTeX, users can create professional-looking documents with a high degree of customization and flexibility. It is easy to pick up, but hard to master. You can find many introductory tutorials online. Also, ChatGPT does a great job at helping with LaTeX questions. If you have an equation you want to transfer into LaTeX go to MathPix. It lets you convert pictures or screenshots into code.\nNow you need to find an editor! The default is to use the online editor Over Leaf. You can start writing without much headache, perfect! And if you have any questions, refer to their extensive collection of guides.\n\n\n7.2.3 Markdown\nMarkdown is perfect down to the last-minute detail. It was designed as a lightweight markup language for creating formatted text using a plain-text editor. It appeals to human readers in its source code form in comparison to the complexity and inefficiency of existing markup languages like HTML.\nMarkdown is popular among developers, writers, and bloggers due to its simplicity and flexibility. It can also include links, images, and other multimedia elements. Markdown is widely supported and can be used with a variety of tools and platforms, including text editors, note-taking apps, and content management systems. By learning Markdown, you can create well-formatted and easy-to-read content quickly and efficiently without the need for complex formatting tools or specialized software. This book is written entirely in Markdown!\nYou can write Markdown in almost any text editor. Nonetheless, I recommend MacDown for Mac and Markdown Pad for Windows, or if you are willing to pay, try Typora.\n\n\n7.2.4 Yet Another Markup Language (YAML)\nYAML (Yet Another Markup Language) is a human-readable data serialization language often used for configuration files and data exchange. It uses indentation, key-value pairs, and directories and supports different data types. Directories are used to organize data as key-value pairs with indentation to indicate hierarchy. The key represents a unique identifier or name, and the value represents associated data or content. YAML is easy to read and write and is popular in web development and software configuration. In YAML,\n\n\nbook:\n  title: \"Dead Souls\"\n  author: \"Nikolai Gogol\"\n  date: today\n  chapters:\n    - index.qmd\n    - chapter1.qmd\n  appendix:\n    - appendix1.qmd\n\n% Key value pairs\ntitle: \"Dead Souls\"\nauthor: \"Nikolai Gogol\"\ndate: today\n\n% Directory/Map  \nchapters:\n  - index.qmd\n  - chapter1.qmd\n\n\n\n\n7.2.5 Pandoc\nPandoc (all documents) converts files from one markup format into another. It supports a wide range of formats, including HTML, Markdown, LaTeX, PDF, and Microsoft Word. With Pandoc, users can convert documents from one markup language to another quickly and easily without having to edit or reformat the content manually.\nPandoc can be used for a variety of purposes, such as converting a Markdown file to HTML for a web page or converting a LaTeX document to PDF for printing. It can also be used to merge multiple documents into a single file or to extract content from a document in a specific format.\nPandoc is highly customizable, with many options and settings available for controlling the output format and appearance of the converted document. It is widely used in academic and scientific publishing, as well as in web development and documentation. By using Pandoc, you can save time and effort by automating the conversion process between different markup formats.\nPandoc is a command-line tool, but it can be integrated into some markdown editors. It extends the capabilities of Markdown by adding support for tables, footnotes, citations, and more. For example, this book was written in Markdown and converted into HTML and PDF using Pandoc."
  },
  {
    "objectID": "chapters/write.html#quarto",
    "href": "chapters/write.html#quarto",
    "title": "7  Write",
    "section": "7.3 Quarto",
    "text": "7.3 Quarto\nAt this point, you might be wondering, “why did I have to read about these technologies?” Because the combination of LaTeX, Markdown, Pandoc, and YAML provides a powerful and flexible set for the creation of documents. Those familiar with Sweave, Rmarkdown, and Jupyter Notebooks will find Quarto to be a similar tool. Quarto builds upon the success of Rmarkdown and Jupyter Notebooks, combining the best features of Markdown with new functionality and eliminating the need for additional packages. It provides attractive default formatting options and easy customization. If you have experience writing in Markdown, Quarto will be a breeze to use.\nWhat can it do?\n\nCreate Reproducible Documents and Reports.\nCreate Dynamic Content with Python, R, Julia, and Observable.\nCreate professional-grade content, including articles, reports, presentations, websites, blogs, and books, in a variety of formats such as HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\nCreate interactive tutorials and notebooks.\n\n\n7.3.1 Installing\nTo use Quarto you don’t need any special software, if you would like, you can even use a text editor to create your .qmd files and command line to render the document. However, working in IDE will make your life much easier. I prefer to use RStudio, after all, Quarto is a product of Posit (former RStudio), but Visual Studio Code also works well. If you have not installed RStudio nor Visual Studio Code yet, follow (setup.qmd?).\n\n7.3.1.1 RStudio\nFrom within Rstudio you can install Quarto as you install any other package. Additionally, you should install TinyTeX, a minimal set of packages required to compile LaTeX documents. You can get both by running the following command in R.\ninstall.packages(c(\"quarto\", \"tinytex\"))\n\n\n7.3.1.2 Visual Studio Code\nInstall the Quarto extension by going to Extensions -> search for “Quarto” -> install. Then run the following command in the terminal if you don’t have TinyTeX installed:\nquarto install tinytex\n\n\n\n7.3.2 Your First Document\nWrite Create a Quarto project. Go to top panel -> File -> New Project -> Directory -> Quarto Project -> Create\nFile -> Quarto Document -> Write the Title of your document -> Select the format you want (HTML is the default) -> Create\nAn (optional) YAML header is demarcated by three dashes (—) on either end. You can learn more about YAML on Quarto’s website. There are a lot of useful options for display.\n---\ntitle: \"Title\"\neditor: visual\nformat: pdf\n---\nIf you prefer the WYSIWYG style, you can switch to the visual editor in the top left corner. Additionally, if you tick “Render to Save,” a new document preview will be updated after each save.\nNow it is time to get writing! To start, H1 heading can be defined in yaml title: \"Title\" and as # Title.\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n\n# Header 1\n# Chapter {.heading-output}\n\n\n\n## Header 2\n7.4 Section\n\n\n\n### Header 3\n7.4.1 Subsection\n\n\n\n#### Header 4\n7.4.1.1 Subsubsection\n\n\n\n##### Header 5\n7.4.1.1.1 Paragraph\n\n\n\n###### Header 6\n7.4.1.1.1.1 Subparagraph\n\n\n\n*italics*\nitalics\n\n\n\n**bold**\nbold\n\n\n\nsuperscript^2^\nsuperscript2\n\n\n\n<https://nber.org>\nhttps://nber.org\n\n\n\n[NBER](https://nber.org)\nNBER\n\n\n\n![caption](monalisa.jpeg)\n\n\n\n\n* unordered list\n* Item 2\n\nsub-item 1\nsub-sub item\n\n\nunordered list\nitem 1\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\nordered list\nitem 2\n\nsub-item 1 A. sub-sub-item 1\n\n\n\nordered list\nitem 2\n\nsub-item 1 A. sub-sub-item 1\n\n\n\n\n\ninline math: $E = mc^{2}$\ninline math: \\(E=mc^{2}\\)\n\n\n\ndisplay math:\n$$E = mc^{2}$$\ndisplay math:\n\\[E = mc^{2}\\]\n\n\n\n\nIf you want to add a line break add an empty line between your paragraphs, otherwise it will continue as the same text.\nUse `…` to add inline code: print(hi, friend)\nUse ``` to delimit blocks of source code:\n```\nprint(hi, friend)\n```\nMaking tables in Markdown is not complicated. The most frequently used table is the pipe table. It allows you to see alignment and captions with :. Tables can get complicated pretty quickly, if you ever get stuck, refer to Quarto’s table documentation.\n\n\n| Default | Left | Middle | Right   | \n|---------|:-----|:------:|--------:|\n| Hola    | Pitt | 3.141  | Nile    | \n| Bonjour | Li   | 2.718  | Amazon  | \n| Salut   | Roth | 4.123  | Yangtze | \n\n: Table Demonstration\n\n\nTable Demonstration\n\n\nDefault\nLeft\nMiddle\nRight\n\n\n\n\nHola\nPitt\n3.141\nNile\n\n\nBonjour\nLi\n2.718\nAmazon\n\n\nSalut\nRoth\n4.123\nYangtze\n\n\n\n\n\nIf you ever feel lost or struggle with formatting, consider using the visual editor. It provides a familiar interface and is particularly useful for creating and previewing tables. To adjust options, for example, the number of list options, simply click on the circle icon with an ellipsis next to it, and a selection menu will appear. In addition to this, the visual editor offers extensive customization options for other elements, such as images and tables.\n::: {.callout-note}\nYou can copy-paste (Ctrl + C; Ctrl + V) a picture in Visual Mode!\n:::\nYou can learn more at Quarto’s website."
  },
  {
    "objectID": "chapters/layout_refs.html#knitr",
    "href": "chapters/layout_refs.html#knitr",
    "title": "8  Layout and References",
    "section": "8.1 Knitr",
    "text": "8.1 Knitr\nKnitr is a package that takes care of the middle step between evaluating code and producing pdf/html. The package runs your code and places its output into a final markdown file, which is later converted by pandoc.\nKnitr lets you sen cell options that influence code blocks’ execution and output. They are put at the top of a block within comments. For example:\n\n\nCollapse Code\nplot(sunspot.year)\nplot(uspop)\n\n\n\n\n\n\n\n\n(a) Sunspot\n\n\n\n\n\n\n\n(b) US Population\n\n\n\n\nFigure 8.1: Plots\n\n\n\nThere is large number of options, but I will show the most commonly used ones. To begin in Figure 8.1, label is a unique id for a code cell, which can be referred to in text with @fig-plots. Similarly, you can refer to tables, chapter, and files. fig-cap defines a caption for the entire plot. fig-subcap gives the two plots their individual sub-captions. layout-ncol let’s us display our plots, pictures, etc. in separate columns. And plot() makes the plots. If you would like your code to fold use code-fold = true, above option show was used to have it opened by default. code-summary defines text for collapsed code blocks.\nAnother common options to use within code blocks are:\n\nfrom R Markdown Cheat Sheet\n\n\n\n\n\n\n\nOption\nValue\nExplanation\n\n\n\n\neval\ntrue\nWhether to evaluate the code and include its results\n\n\necho\ntrue\nWhether to display code along with its results\n\n\nwarning\ntrue\nWhether to display warnings\n\n\nerror\nfalse\nWhether to display errors\n\n\nmessage\ntrue\nWhether to display messages\n\n\ninclude\ntrue\nPrevents any output (code, results) from being included\n\n\ntidy\nfalse\nWhether to reformat code in a tidy way when displaying it\n\n\nresults\n“markup”\nType of output format: “markup”, “asis”, “hold”, or “hide”\n\n\ncache\nfalse\nWhether to cache results for future renders\n\n\n\ntidy: true is super useful once you want to include code inside your document as it will format it nicely."
  },
  {
    "objectID": "chapters/layout_refs.html#div-blocks",
    "href": "chapters/layout_refs.html#div-blocks",
    "title": "8  Layout and References",
    "section": "8.2 Div Blocks",
    "text": "8.2 Div Blocks\nIf you are familiar with HTML you will recognize <div> blocks. You can add div blocks with wrapping text in ::: or more semicolons. It is useful when you want put pictures in a grid. Here is a simple example:\n::: {layout-ncol=\"2\"}\n![Surus](surus.png)\n\n![Hanno](hanno.png)\n:::\nIt must be separated from the preceding and following blocks by blank lines. Divs can be nested inside other Divs. For example, here we put a note and some text onto the margin.\n:::: column-margin\n\n::: callout-note\nHere is a Note!\n:::\n\nMore content.\n::::\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a Note!\n\n\nMore content.\nThe  short code enables you to insert a native page break into a document that will be compatible with all the other formats:\nBecause R, YAML, HTML, LaTeX have different notations for  commenting. So, the one that will work universally within quarto is HTML’s <!-- comment here -->."
  },
  {
    "objectID": "chapters/layout_refs.html#diagrams",
    "href": "chapters/layout_refs.html#diagrams",
    "title": "8  Layout and References",
    "section": "8.3 Diagrams",
    "text": "8.3 Diagrams\nYou can also create beautiful UML (Unified Modeling Language) diagrams within quarto with Mermaid and Graphviz. The flow chart below was maid with Mermaid!\n\n\n\n\nflowchart LR\n  A[Hard edge] --> B(Round edge)\n  B --> C{Decision}\n  C --> D[Result one]\n  C --> E[Result two]\n\n\n\n\n\n\n\n\nThis might be your first time hearing that there is a language behind diagrams. UML is a standard graphical notation to describe software designs. It is a powerful tools for planning, visualizing and documents your projects. There are different types of diagrams to depict structures, behaviors and interactions with the standard set of symbols and notation. We will meet some of them in the chapter on Relational Databases."
  },
  {
    "objectID": "chapters/layout_refs.html#citations",
    "href": "chapters/layout_refs.html#citations",
    "title": "8  Layout and References",
    "section": "8.4 Citations",
    "text": "8.4 Citations\n\n“Proper citation adds credibility to your work and acknowledges the work of others.” - Chat GPT\n\nAdding citations to your work shouldn’t be stressful or confusing. With Quarto’s seamless integration with Zotero, you can easily add citations in your preferred style and create a reference list, all without hassle. How cool is that? I think pretty cool.\nQuarto utilizes Pandoc to generate citations and bibliographies in your preferred style. To source your citations, you’ll need a .bib or .bibtex file, and optionally a .csl file for the citation style. Simply, add bibliography: references.bib to you YAML header in _quarto.yml.\n\nbibliography: references.bib\n\nYou can easily cite your article using @yourcitation9999. Visual mode also provides suggestions, and entering the article’s DOI will help locate and insert it even if it is not in your bibliography. For more information on citation methods, see Quarto Citation and Pandoc Citations.\n\n\n\n\n\n\n\nMarkdown Format\nOutput (author-date format)\n\n\n\n\n@abadie2017 says cluster you SE.\nAbadie et al. (2017) says cluster you SE.\n\n\nSome thing smart [@abadie2017; @bai2009].\nSome thing smart (Abadie et al. 2017; Bai 2009).\n\n\nAbadie says cluster [-@abadie2017].\nAbadie says cluster (2017).\n\n\n\n\nAbadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge. 2017. “When Should You Adjust Standard Errors for Clustering?” Cambridge, MA. https://doi.org/10.3386/w24003.\n\nBai, Jushan. 2009. “Panel Data Models with Interactive Fixed Effects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ecta6135.\nIf you’ve successfully created your bibliography in Zotero, adding citations to your document will be a breeze. Simply start typing and Zotero will suggest citations to add to your bibliography file. For a paper with more than 10 citations, I recommend using Better Bibtex, which allows you to connect citation keys to the paper as you write, just make sure Zotero is open.\nTo generate your citations from a document (say cited in Obsidian) without having to re-cite everything, you can use the bbt_update_bib() function from the rbbt package. Ensure that Zotero is running and that you’re in the markdown document where you want to update citations. Run the bbt_update_bib() function to create a bibliography, and specify any additional arguments as needed.\n\nbbt_update_bib(\n  path_rmd, # Path to your Markdown document.\n  path_bib = bbt_guess_bib_file(path_rmd), # Path to the references.bib file\n  translator = bbt_guess_translator(path_bib), # type of bibliography file to generate: CSL-JSON, BibLaTeX, BibTeX, and CSL YAML.\n)"
  },
  {
    "objectID": "chapters/template.html#getting-started",
    "href": "chapters/template.html#getting-started",
    "title": "9  Thesis Template",
    "section": "9.1 Getting Started",
    "text": "9.1 Getting Started\nTo use this template, you will need to install Quarto and TinyTeX.\n\n9.1.1 RStudio\nInstall Quarto and TinyTeX by running the following command in R:\n\ninstall.packages(c(\"quarto\", \"tinytex\"))\n\n\n\n9.1.2 Visual Studio Code\nInstall the Quarto extension by going to Extensions -> search for “Quarto” -> install. Then run the following command in the terminal if you don’t have TinyTeX installed:\n\nquarto install tinytex"
  },
  {
    "objectID": "chapters/template.html#adding-the-quarto-template",
    "href": "chapters/template.html#adding-the-quarto-template",
    "title": "9  Thesis Template",
    "section": "9.2 Adding the Quarto Template",
    "text": "9.2 Adding the Quarto Template\nTo add the template to your project, run the following command in your terminal:\n\nquarto add template nikitoshina/USFCA-Thesis-Template\n\nThis will download all the necessary folders with the LaTeX files for your thesis. You can render your project from within your folder by running the following command in terminal:\n\nquarto render"
  },
  {
    "objectID": "chapters/template.html#usage",
    "href": "chapters/template.html#usage",
    "title": "9  Thesis Template",
    "section": "9.3 Usage",
    "text": "9.3 Usage\nYour first chapter should be written in index.qmd, and you can add additional chapters to the /Chapters folder. You can add information about the document such as abstract in the _quarto.yml."
  },
  {
    "objectID": "chapters/collaboration.html#trackdown-and-google-docs",
    "href": "chapters/collaboration.html#trackdown-and-google-docs",
    "title": "10  Collaboration",
    "section": "10.1 trackdown and Google Docs",
    "text": "10.1 trackdown and Google Docs\nProjects often involve collaborators with varying technical proficiency, and using markdown files might pose challenges for some stakeholders. One approach could be to render a Microsoft Word document and transfer edits back into markdown later. However, this method can be laborious, and our goal is to work smarter, not harder. A solution to this issue is to use a package called trackdown, which enables collaboration on narrative text through Google Docs. To edit .qmd files, you will need the version of trackdown >= 1.3, which can be downloaded from GitHub using the following command:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"claudiozandonella/trackdown\",\n                         build_vignettes = TRUE)\nlibrary(trackdown)\n\nAt the moment, the API credentials for the package have been exhausted, so you’ll need to set up your own. To do this, follow the straightforward guide provided by the developers: https://claudiozandonella.github.io/trackdown/articles/oauth-app-configuration.html.\nAfter setting everything up, you can upload your file to Google Docs using upload_file(\"your_file.qmd\") and share it with collaborators. If you need to make last-minute changes, use update_file(\"your_file.qmd\") to update the Google Doc, beware it will overwrite the document. When the review process is complete, execute trackdown:download_file(), and the changes will be automatically integrated.\n\n\n\n\n\n\nNote\n\n\n\nSave the file before uploading and updating!\n\n\nWhen you’re ready to add code for figures, tables, or analysis results, avoid doing so in Google Docs. Instead, first download the document. Ensure all changes made by collaborators in Google Docs are accepted (or rejected) before downloading. To accept all changes at once, use “Tools > Review suggested edits > Accept all”. Then, download the edited document from Google Drive using download_file(file = \"your_file.qmd\"). Once you are done implementing the changes you can again use update_file(\"your_file.qmd\") to update the file in Google Drive. One handy trick is to use render_file(file = \"your_file.qmd\") to both download and render file. You can accept all the changes run render_file() and check whether the file renders correctly and undo all the changes in the google doc to selectively accept the changes.\nWhile collaborating on .qmd documents, use Google Docs for narrative text and Git for code. Avoid writing or editing code in Google Docs, as it’s prone to errors. Write code in an IDE like RStudio instead. When using trackdown remember that formatting done in google docs will be lost. Use proper Markdown or LaTeX syntax for formatting.\nThe workflow is iterative, with the document being uploaded/updated on Google Drive for narrative text editing and downloaded locally for code writing with Git. Note that simultaneous collaboration on narrative text and code is not possible with trackdown, as changes in both versions cannot be automatically merged. Structuring the workflow sequentially ensures a smooth experience. When adopting such workflow, limit the use of R code in the R Markdown file and separate code into different files to prevent interference between code and narrative text. This way you will have to separate system for working on code and narrative avoiding the clash.\nFor more information on the package have a look at the vignettes and visit its GitHub page https://quarto.org/.\nhttps://docs.google.com/document/d/1vieKJe0na6sDeIu2XWBfE38LlGFswUNRkX7arRSa4Is/edit?usp=sharing\nhttps://docs.google.com/document/d/1GWGlcXW6hUT6l7IW-ttNcUbMq97xQLcMkZ9LyEMPSLM/edit?usp=sharing"
  },
  {
    "objectID": "chapters/qualtrics_api.html#core-functions",
    "href": "chapters/qualtrics_api.html#core-functions",
    "title": "11  Qualtrics API",
    "section": "11.1 Core Functions",
    "text": "11.1 Core Functions\nCurrently, the package contains three core functions:\n\nall_surveys() shows surveys you can access.\nfetch_survey() downloads the survey.\nread_survey() reads CSV files you downloaded manually from Qualtrics.\n\nIt also contains a number of helper functions, including:\n\nqualtrics_api_credentials() stores your API key and base url in environment variables.\nsurvey_questions() retrieves a data frame containing questions and question IDs for a survey;\nextract_colmap() retrieves a similar data frame with more detailed mapping from columns to labels.\nmetadata() retrieves metadata about your survey, such as questions, survey flow, number of responses etc.\n\nNote that you can only export surveys that you own, or to which you have been given administration rights."
  },
  {
    "objectID": "chapters/qualtrics_api.html#connecting-to-the-api",
    "href": "chapters/qualtrics_api.html#connecting-to-the-api",
    "title": "11  Qualtrics API",
    "section": "11.2 Connecting to the API",
    "text": "11.2 Connecting to the API\nIf you have received API access, now you can connect to the API. To get api_key and base_url go to Qualtrics Home Page > Account Settings > Qualtrics IDs or click this link. Under “API” click “Generate Topic” and you will be issued a Token. Copy this string a put in “YOUR_API_KEY”. Then look at “User” module, copy “Datacenter ID” and “.qualtrics.com” after it. Your Base Url should look something like this: “lad2.qualtrics.com”.\n\nqualtrics_api_credentials(api_key = \"YOUR_API_KEY\", \n                          base_url = \"YOUR_BASE_URL\",\n                          install = TRUE,\n                        # overwrite = TRUE # If you need to update your credentials\n                          )\n\n\nqualtrics_api_credentials(api_key = \"CEOj8Iwh6BUTNZ9J7PXDJLZpStQaXkmlwLPYzCgu\", \n                          base_url = \"lad2.qualtrics.com\",\n                          install = TRUE)\n\nAfter qualtrics_api_credintials stored your credentials, you can use all_surveys() to fetch information on your surveys.\n\n(surveys <- all_surveys()) \n\n# A tibble: 8 × 6\n  id                 name                        ownerId lastM…¹ creat…² isAct…³\n  <chr>              <chr>                       <chr>   <chr>   <chr>   <lgl>  \n1 SV_0rearXjH2Ri6umq Student Satisfaction        UR_2tz… 2023-0… 2023-0… FALSE  \n2 SV_2gWVWLn2vCCDJGu Luvuyo                      UR_2tz… 2022-0… 2022-0… FALSE  \n3 SV_3h0HPHQbJStqb8a Pilot (USA) - Giver Motive… UR_2tz… 2022-0… 2022-0… FALSE  \n4 SV_3Q011O6gNqwn1Nc Pen Experiment              UR_2tz… 2022-1… 2022-1… FALSE  \n5 SV_4YDoTFLN61nKecS Pen_Showcase                UR_2tz… 2023-0… 2023-0… TRUE   \n6 SV_4Yhvzriag8syz7U Prosociality Experiment     UR_2tz… 2023-0… 2022-0… FALSE  \n7 SV_bJIs8lwz4CfAAgS test                        UR_2tz… 2023-0… 2023-0… TRUE   \n8 SV_dnBQ3YB4rLC03J4 test2                       UR_2tz… 2023-0… 2023-0… FALSE  \n# … with abbreviated variable names ¹​lastModified, ²​creationDate, ³​isActive\n\n\nOnce you select the questionnaire you want you can refer to it using id. If you want redownload the data set force_request = TRUE, otherwise it will load prior saved download.\n\nsurvey_data <- fetch_survey(surveyID = \"SV_bJIs8lwz4CfAAgS\", \n             verbose = TRUE,\n             force_request = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nsurvey_data %>% glimpse()\n\nRows: 22\nColumns: 89\n$ StartDate                 <dttm> 2023-02-07 16:00:52, 2023-02-07 16:01:17, 2…\n$ EndDate                   <dttm> 2023-02-07 16:04:08, 2023-02-07 16:04:08, 2…\n$ Status                    <chr> \"IP Address\", \"IP Address\", \"IP Address\", \"I…\n$ IPAddress                 <chr> \"138.202.129.171\", \"138.202.129.164\", \"138.2…\n$ Progress                  <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ `Duration (in seconds)`   <dbl> 196, 171, 104, 187, 189, 198, 210, 209, 180,…\n$ Finished                  <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ RecordedDate              <dttm> 2023-02-07 16:04:09, 2023-02-07 16:04:09, 2…\n$ ResponseId                <chr> \"R_3r385toH6wjBmyr\", \"R_2usVBjQ7yXrmY72\", \"R…\n$ RecipientLastName         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RecipientFirstName        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RecipientEmail            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ExternalReference         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LocationLatitude          <dbl> 37.78, 37.78, 37.78, 37.78, 37.78, 37.78, 37…\n$ LocationLongitude         <dbl> -122.465, -122.465, -122.465, -122.465, -122…\n$ DistributionChannel       <chr> \"anonymous\", \"anonymous\", \"anonymous\", \"anon…\n$ UserLanguage              <chr> \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"E…\n$ Consent                   <ord> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ Gender                    <ord> Female, Female, Male, Male, Male, Male, Male…\n$ Name                      <chr> \"Tasha\", \"Khushboo Patel\", \"Nikita\", \"Lawren…\n$ Competitive               <ord> Competitive, Competitive, Competitive, Compe…\n$ Pizzas_1                  <chr> \"Margherita\", NA, NA, \"Margherita\", \"Margher…\n$ Pizzas_2                  <chr> \"Pepperoni\", NA, NA, \"Pepperoni\", \"Pepperoni…\n$ Pizzas_3                  <chr> NA, \"BBQ Chicken\", NA, NA, \"BBQ Chicken\", \"B…\n$ Pizzas_4                  <chr> \"Hawaiian\", NA, NA, NA, \"Hawaiian\", NA, \"Haw…\n$ Pizzas_5                  <chr> \"Veggie\", \"Veggie\", \"Veggie\", \"Veggie\", \"Veg…\n$ Pizzas_6                  <chr> NA, NA, NA, NA, NA, \"I also love that one:\",…\n$ Pizzas_7                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Pizzas_6_TEXT             <chr> NA, NA, NA, NA, NA, \"Garlic White Chinese be…\n$ Pizzas_DO_1               <dbl> 5, 1, 2, 4, 5, 3, 1, 1, 5, 5, 3, 1, 5, 1, 4,…\n$ Pizzas_DO_2               <dbl> 3, 3, 5, 1, 3, 2, 3, 3, 1, 4, 2, 5, 3, 3, 5,…\n$ Pizzas_DO_3               <dbl> 2, 2, 3, 5, 4, 1, 4, 5, 2, 2, 1, 3, 1, 2, 2,…\n$ Pizzas_DO_4               <dbl> 1, 4, 1, 3, 2, 5, 2, 2, 3, 1, 5, 4, 2, 5, 1,…\n$ Pizzas_DO_5               <dbl> 4, 5, 4, 2, 1, 4, 5, 4, 4, 3, 4, 2, 4, 4, 3,…\n$ Pizzas_DO_6               <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ Pizzas_DO_7               <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n$ `1_Taste`                 <dbl> 5, NA, NA, 5, 3, NA, 3, 4, 4, 5, 4, NA, NA, …\n$ `1_Healthiness`           <dbl> 5, NA, NA, 4, 4, NA, 2, 3, 3, 5, 2, NA, NA, …\n$ `1_Ease_Of_Preparation`   <dbl> 5, NA, NA, 4, 4, NA, 4, 3, 4, 3, 3, NA, NA, …\n$ `2_Taste`                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `2_Healthiness`           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `2_Ease_Of_Preparation`   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `3_Taste`                 <dbl> NA, NA, NA, NA, NA, 4, 5, NA, 4, NA, NA, NA,…\n$ `3_Healthiness`           <dbl> NA, NA, NA, NA, NA, 3, 3, NA, 2, NA, NA, NA,…\n$ `3_Ease_Of_Preparation`   <dbl> NA, NA, NA, NA, NA, 3, 4, NA, 3, NA, NA, NA,…\n$ `9_Taste`                 <dbl> 5, NA, NA, 5, 5, NA, 5, NA, NA, NA, 5, NA, 4…\n$ `9_Healthiness`           <dbl> 3, NA, NA, 3, 2, NA, 2, NA, NA, NA, 2, NA, 4…\n$ `9_Ease_Of_Preparation`   <dbl> 5, NA, NA, 5, 4, NA, 4, NA, NA, NA, 4, NA, N…\n$ `10_Taste`                <dbl> NA, 5, NA, NA, 5, 5, 3, 4, 4, NA, 4, NA, 4, …\n$ `10_Healthiness`          <dbl> NA, NA, NA, NA, 2, 3, 2, 3, 3, NA, 2, NA, 4,…\n$ `10_Ease_Of_Preparation`  <dbl> NA, NA, NA, NA, 3, 3, 2, 3, 2, NA, 3, NA, NA…\n$ `11_Taste`                <dbl> 5, NA, NA, NA, 4, NA, 5, 4, NA, NA, 4, 5, NA…\n$ `11_Healthiness`          <dbl> 4, NA, NA, NA, 2, NA, 2, 3, NA, NA, 3, 4, NA…\n$ `11_Ease_Of_Preparation`  <dbl> 5, NA, NA, NA, 3, NA, 3, 2, NA, NA, 2, 3, NA…\n$ `12_Taste`                <dbl> 5, 4, 4, 4, 4, NA, 3, NA, NA, NA, 4, NA, NA,…\n$ `12_Healthiness`          <dbl> 5, 2, 5, 5, 4, NA, 3, NA, NA, NA, 4, NA, NA,…\n$ `12_Ease_Of_Preparation`  <dbl> 4, 3, 3, 3, 2, NA, 2, NA, NA, NA, 3, NA, NA,…\n$ `match timer_First Click` <dbl> 0.000, 0.000, 0.000, 0.000, 20.317, 0.000, 0…\n$ `match timer_Last Click`  <dbl> 0.000, 0.000, 0.000, 0.000, 20.317, 0.000, 0…\n$ `match timer_Page Submit` <dbl> 8.296, 12.572, 7.955, 22.206, 21.188, 7.224,…\n$ `match timer_Click Count` <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ transfer                  <dbl> 50, NA, 35, NA, 49, NA, NA, 35, NA, 10, 50, …\n$ `get timer_First Click`   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `get timer_Last Click`    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `get timer_Page Submit`   <dbl> 4.999, 15.967, 32.000, 2.787, 3.006, 10.538,…\n$ `get timer_Click Count`   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ decision...67             <chr> NA, \"I accept A's offer.\\n(You get ${e://Fie…\n$ Q23                       <ord> Yes, Absolutely, Absolutely, Yes, Yes, Yes, …\n$ researcherID              <chr> \"hTtZNw8TA0\", \"hTtZNw8TA0\", \"hTtZNw8TA0\", \"h…\n$ studyID                   <chr> \"test\", \"test\", \"test\", \"test\", \"test\", \"tes…\n$ groupID                   <dbl> 6, 6, 5, 5, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11…\n$ participantID             <chr> \"R_3r385toH6wjBmyr\", \"R_2usVBjQ7yXrmY72\", \"R…\n$ groupSize                 <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ numStages                 <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ roles                     <chr> \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A…\n$ participantRole           <chr> \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\",…\n$ timeOutLog                <chr> \"OK -- no issues\", \"OK -- no issues\", \"OK --…\n$ botMatch                  <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ total                     <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ offer                     <dbl> 50, 50, 35, 35, 49, 49, 35, 35, 10, 10, 50, …\n$ decision...81             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ payoff                    <dbl> 50, 50, 65, 35, 51, 49, 35, 65, 10, 90, 50, …\n$ sendStage                 <dbl> 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2,…\n$ sendData                  <chr> \"offer\", \"decision\", \"offer\", \"decision\", \"o…\n$ getStage                  <dbl> 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1,…\n$ getData                   <chr> \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\",…\n$ defaultData               <dbl> 2, 20, 2, 5, 2, 71, 61, 1, 83, 2, 2, 81, 1, …\n$ saveData                  <chr> \"decision\", \"offer\", \"decision\", \"offer\", \"d…\n$ randomPercent             <dbl> NA, 20, NA, 5, NA, 71, 61, NA, 83, NA, NA, 8…\n\n\nIn case you want to see text of the questions use survey_questions().\n\nsurvey_questions <- survey_questions(surveyID = \"SV_bJIs8lwz4CfAAgS\")\nhead(survey_questions, n = 5)\n\n# A tibble: 5 × 4\n  qid   qname        question                                            force…¹\n  <chr> <chr>        <chr>                                               <lgl>  \n1 QID25 Introduction \"Welcome to the <strong>University of San Francisc… FALSE  \n2 QID26 Consent      \"Do you agree to participate in the survey?\"        TRUE   \n3 QID21 Gender       \"What is your gender\"                               TRUE   \n4 QID23 Name         \"What is your Name?\"                                FALSE  \n5 QID22 Competitive  \"Would you consider yourself competitive?\"          FALSE  \n# … with abbreviated variable name ¹​force_resp"
  },
  {
    "objectID": "chapters/qualtrics_api.html#example",
    "href": "chapters/qualtrics_api.html#example",
    "title": "11  Qualtrics API",
    "section": "11.3 Example",
    "text": "11.3 Example\n\nsurvey_data <- fetch_survey(surveyID = \"SV_bJIs8lwz4CfAAgS\", \n                            verbose = TRUE,force_request = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nsurvey_data <- survey_data %>% janitor::clean_names()\n\ngraph_data <- survey_data %>%select(gender, offer,  decision_81, participant_id, participant_role) %>% \n  mutate(participant_role = recode(participant_role,\"A\" = \"dictator\", \"B\" = \"recipient\" ),\n         decision = recode(decision_81, \"1\" = \"Accepted\", \"2\" = \"Declined\")) %>% \n  mutate(interval = cut_width(offer, width = 10, center = 45)) %>%\n  filter(participant_role == \"recipient\") %>% count(decision, interval) \n\n\n\nShow the code\ngraph_data %>%\n  ggplot(aes(x = interval, y = n, fill = as.factor(decision))) + \n  geom_col(position = position_stack()) + \n  theme_minimal(base_size = 20) + \n  scale_y_continuous(breaks = scales::breaks_extended(n = max(graph_data$n))) + \n  theme(panel.grid.major.x = element_blank(), \n        panel.grid.minor.x = element_blank(),\n        legend.position = \"top\") +\n  labs(x = \"Offer size\", y = \"Count\", title = \"Results of the Ultimatum Game\", fill = \"Result\")\n\n\n\n\n\n\n\nShow the code\npizza_table <- survey_data[c(22:29,72)] %>% select(-pizzas_6) %>% pivot_longer(-participant_id) %>% count(value) %>% drop_na()\n \n pizza_table %>% ggplot(aes(x = fct_reorder(value, n), y = n)) + \n   geom_col(fill = \"steelblue\") + \n   theme_minimal(base_size = 20) + \n   scale_y_continuous(breaks = scales::breaks_extended(n = max(pizza_table$n))) + \n   theme(panel.grid.major.y = element_blank(), \n         panel.grid.minor.y = element_blank(),\n         panel.grid.minor.x = element_blank(),\n         legend.position = \"top\") +\n   labs(x = NULL, y = \"Count\", title = \"What Pizzas do you like?\") + \n   coord_flip()"
  },
  {
    "objectID": "chapters/data_manipulation.html#basics",
    "href": "chapters/data_manipulation.html#basics",
    "title": "12  Data Manipulation",
    "section": "12.1 Basics",
    "text": "12.1 Basics\nLet us start with some basic concepts! We can use R as a basic calculator\n\n# This is a comment use \"#\" to comment something!\n2+2\n\n[1] 4\n\n2*4 \n\n[1] 8\n\n2^8\n\n[1] 256\n\n(1+3)/(3+5)\n\n[1] 0.5\n\nlog(10) # This takes a natural log of 10! \n\n[1] 2.302585\n\n\nWe can define variables and perform operations on them. R uses = or <- to assign values to a variable name. It is stylistically preferred to use <- to avoid confusion and some errors.\n\nx <- 2 # same as x = 2\nx * 4\n\n[1] 8\n\n\nx <- 2 stored 2 in x. Later when we wrote x * 4 R substituted x for 2 evaluating 2 * 4 to get 8. We can update value of x as much as we want using = or <-. Keep in mind R is case sensitive so X and x are different.\n\nx\n\n[1] 2\n\nx <- x * 5\n\n\n12.1.1 Data Types\nR has a number of different data types and classes such as data.frames, which are similar to excel spreadsheets with columns and rows. We will first look at vectors. Vectors can hold multiple values of the same types. Most basic ones are numeric, character and logical.\n\nx\n\n[1] 10\n\nclass(x)\n\n[1] \"numeric\"\n\n\n\n(name <- \"Parsa Rahimi\") # wrapping with (...) will print the variable\n\n[1] \"Parsa Rahimi\"\n\nclass(name)\n\n[1] \"character\"\n\n\n\n(true_or_false <- TRUE)\n\n[1] TRUE\n\nclass(true_or_false)\n\n[1] \"logical\"\n\n\nNote that name is stored as a single character string. What if we want store name and surname separately in the same object? We can use concatenate c() to combine objects of similar class into a vector!\n\n(name_surname <- c(\"Parsa\",\"Rahimi\"))\n\n[1] \"Parsa\"  \"Rahimi\"\n\nlength(name) \n\n[1] 1\n\nlength(name_surname)\n\n[1] 2\n\n\nNotice how length of the name is 1 and length of the name_surname is 2! Let’s make a numeric vector and do some operations on it!\n\n(i <- c(1, 2, 3, 4))\n\n[1] 1 2 3 4\n\ni + 10 # add 10 to each elements\n\n[1] 11 12 13 14\n\ni * 10 # multiply each element by 10\n\n[1] 10 20 30 40\n\ni + c(2, 4, 6, 8) # add elements together in matching positions\n\n[1]  3  6  9 12\n\n\nWe haven’t modified i with any of those operations. The results are just printed and not stored. If we want to preserve the results we have to store them in a variable.\n\nname\n\n[1] \"Parsa Rahimi\"\n\nname <- i + c(5,4,2,1)\nname\n\n[1] 6 6 5 5\n\n\nNotice that name is no longer “Parsa Rahimi”. It has been overwritten by assigning a numeric vector instead of a character string. But be careful we can perform numeric operations only on numeric objects otherwise we will receive an error. You can use str() to get structure of the object such as type, length and other.\n\nname_surname + 2\n\nError in name_surname + 2: non-numeric argument to binary operator\n\nstr(name_surname)\n\n chr [1:2] \"Parsa\" \"Rahimi\""
  },
  {
    "objectID": "chapters/data_manipulation.html#downloading-data",
    "href": "chapters/data_manipulation.html#downloading-data",
    "title": "12  Data Manipulation",
    "section": "12.2 Downloading Data",
    "text": "12.2 Downloading Data\nIf you are familiar with Base R (functions that come with the R on installation) you know about read.csv(). readr provides a number of function that solve common issues of base R read function. read_csv loads data 10 time faster and produces a tibble instead of a data frame while avoiding inconsistencies of the read.csv. ‘Wait what is tibble?’ you might ask. Tibbles is special type of data frame. There are superior to regular data frames as they load faster, maintain input types, permit columns as lists, allow non-standard variable names, and never create row names. Okay you have your data lets load it! First, you need to know the path to your data. You can go find you file and check its location and then copy paste it. If you are windows user your path might have “\\”, which is an escape character. To fix that replace “\\” with “/”. By copying the path you are getting absolute path “/Users/User/Documents/your_project/data/file.csv” alternatively you can use local a local path from the folder of the project “/data/file.csv”. Let’s read the data! readr:: specifies which packages to use. Replace the text between “…” to your path.\n\n12.2.1 Example Data\nI will be using a sample of experiment’s results from Climate and Cooperation Experiment from Mexico. During the experiments, subjects were asked to complete three series of ravens matrices, four dictator games, and a single lottery game. Sessions below 30 Celsius are labeled as control, and sessions above 30 Celsius as treatment. We will be primarily using results from Raven’s matrices games: 3 sets of 12 matrices. First set, pr_, is piece-rate round where participants received points for each correctly solved matrix. Second set, tr_, is tournament round where participants competed againts a random opponent and the winner received double point and the loser received nothing. Third set, ch_, is choice round. Participants were asked to decide whether they want to play piece-rate or tournament againts a different opponent’s score from tournament round.\nYou can find data in the data in GitHub repository. We will need tidyverse.\n\nlibrary(tidyverse)\n\n\ndata <- readr::read_csv(\"https://raw.githubusercontent.com/nikitoshina/ECON-623-Lab-2023/main/data/mexico_sample_data.csv?token=GHSAT0AAAAAAB5WTPULI26TZP545VNUFQE6Y6O4XVA\") #Download data from git hub\n\nWe can use glimpse() to get a glimpse at the data. It will give us a sample and type of the column. Another very common way is to use head() to get a slice of the top rows or tail() to get a slice of bottom rows. You can also view the entire data set with View()\n\ndata %>% glimpse()\n\nRows: 114\nColumns: 70\n$ id                 <chr> \"001018001\", \"001018002\", \"001018005\", \"001018009\",…\n$ country_city       <chr> \"mexico_chapingo\", \"mexico_chapingo\", \"mexico_chapi…\n$ start_time         <chr> \"12H 16M 0S\", \"12H 16M 0S\", \"12H 16M 0S\", \"12H 16M …\n$ end_time           <chr> \"13H 14M 0S\", \"13H 14M 0S\", \"13H 14M 0S\", \"13H 14M …\n$ date               <date> 2022-06-20, 2022-06-20, 2022-06-20, 2022-06-20, 20…\n$ mean_temp_celsius  <dbl> 28.58772, 28.58772, 28.58772, 28.58772, 28.58772, 2…\n$ incentive_local    <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1…\n$ exchange_usd_local <dbl> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,…\n$ incentive_usd      <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ gender             <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ point_value        <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ site_id            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ session_n          <dbl> 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19,…\n$ subject_n          <dbl> 1, 2, 5, 9, 10, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8,…\n$ tr_opponet_n       <dbl> 2, 1, 9, 5, 13, 10, 15, 14, 2, 1, 4, 3, 5, 4, 8, 7,…\n$ ch_opponent_n      <dbl> 9, NA, NA, 1, 9, NA, NA, NA, NA, 3, 2, NA, NA, 8, N…\n$ version            <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ treatment          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dc_cl_ps           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dc_cl_envy         <dbl> 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, …\n$ dc_c_ps            <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, …\n$ dc_c_envy          <dbl> 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, …\n$ dc_cl_ps_points    <dbl> 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,…\n$ dc_cl_envy_points  <dbl> 20, 20, 20, 20, 16, 16, 16, 20, 20, 20, 16, 20, 16,…\n$ dc_c_ps_points     <dbl> 16, 16, 16, 16, 12, 20, 16, 16, 12, 20, 12, 20, 16,…\n$ dc_c_envy_points   <dbl> 23, 23, 23, 23, 21, 18, 23, 23, 23, 23, 18, 21, 23,…\n$ pr_correct         <dbl> 7, 5, 6, 1, 7, 2, 6, 7, 5, 6, 8, 2, 2, 5, 6, 5, 2, …\n$ pr_wrong           <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 0, …\n$ pr_not_attempted   <dbl> 5, 6, 6, 11, 5, 10, 6, 5, 7, 4, 4, 9, 9, 6, 6, 6, 1…\n$ tr_correct         <dbl> 3, 6, 7, 5, 9, 7, 6, 7, 4, 7, 6, 3, 6, 7, 8, 6, 6, …\n$ tr_wrong           <dbl> 1, 1, 1, 4, 0, 0, 2, 1, 2, 2, 2, 0, 3, 1, 1, 4, 1, …\n$ tr_not_attempted   <dbl> 8, 5, 4, 3, 3, 5, 4, 4, 6, 3, 4, 9, 3, 4, 3, 2, 5, …\n$ tr_die             <dbl> 2, 5, 2, 6, 6, 2, 2, 6, 2, 4, 5, 3, 6, 6, 3, 6, 1, …\n$ tr_total           <dbl> 5, 11, 9, 11, 15, 9, 8, 13, 6, 11, 11, 6, 12, 13, 1…\n$ tr_guess_correct   <dbl> 5, 7, 8, 6, 11, 5, 8, 5, 5, 6, 8, 5, 5, 8, 8, 7, 3,…\n$ tr_guess_die       <dbl> 3, 4, 1, 4, 2, 2, 5, 5, 4, 3, 4, 2, 4, 2, 3, 1, 2, …\n$ tr_guess_total     <dbl> 8, 11, 9, 10, 13, 7, 13, 10, 9, 9, 12, 7, 9, 10, 11…\n$ tr_other_correct   <dbl> 6, 3, 5, 7, 7, 9, 7, 6, 7, 4, 3, 6, 6, 3, 6, 8, 8, …\n$ tr_other_die       <dbl> 5, 2, 6, 2, 2, 6, 6, 2, 4, 2, 3, 5, 6, 3, 6, 3, 3, …\n$ tr_other_total     <dbl> 11, 5, 11, 9, 9, 15, 13, 8, 11, 6, 6, 11, 12, 6, 12…\n$ tr_result          <dbl> 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 1, 2, 0, 2, 0, …\n$ tr_points          <dbl> 0, 22, 0, 22, 30, 0, 0, 26, 0, 22, 22, 0, 12, 26, 0…\n$ f_happy            <dbl> 5, 9, 8, 4, 8, 8, 3, 7, 6, 10, 8, 9, 4, 5, 9, 8, 6,…\n$ f_energetic        <dbl> 7, 8, 7, 3, 10, 9, 3, 5, 7, 10, 9, 8, 5, 6, 9, 6, 8…\n$ f_frustrated       <dbl> 8, 3, 3, 0, 4, 8, 8, 2, 7, 0, 3, 0, 5, 0, 1, 2, 0, …\n$ f_last_meal_min    <dbl> 180, 240, 240, 90, 30, 120, 90, 120, 30, 120, 30, 3…\n$ ch_correct         <dbl> 9, 4, 7, 7, 10, 5, 7, 6, 4, 4, 8, 6, 5, 6, 6, 6, 5,…\n$ ch_wrong           <dbl> 0, 0, 0, 5, 0, 2, 1, 2, 4, 2, 3, 0, 1, 2, 2, 2, 0, …\n$ ch_not_attempted   <dbl> 3, 8, 5, 0, 2, 5, 4, 4, 4, 6, 1, 6, 6, 4, 4, 4, 7, …\n$ ch_die             <dbl> 3, 3, 5, 6, 2, 4, 1, 3, 6, 1, 5, 4, 3, 2, 4, 2, 6, …\n$ ch_total           <dbl> 12, 7, 12, 13, 12, 9, 8, 9, 10, 5, 13, 10, 8, 8, 10…\n$ ch_tournament      <dbl> 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, …\n$ ch_other_correct   <dbl> 9, 4, 4, 7, 9, 4, 4, 4, 4, 4, 8, 4, 4, 6, 4, 6, 5, …\n$ ch_other_die       <dbl> 3, 3, 3, 6, 3, 3, 3, 3, 6, 1, 5, 6, 6, 2, 6, 2, 6, …\n$ ch_other_total     <dbl> 12, 7, 7, 13, 12, 7, 7, 7, 10, 5, 13, 10, 10, 8, 10…\n$ ch_result          <dbl> 1, NA, NA, 1, 1, NA, NA, NA, NA, 1, 1, NA, NA, 1, N…\n$ ch_points          <dbl> 12, 7, 12, 13, 12, 9, 8, 9, 10, 5, 13, 10, 8, 8, 10…\n$ ct_selected        <dbl> 2, 2, 3, 6, 6, 2, 1, 1, 1, 6, 2, 1, 1, 1, 6, 6, 6, …\n$ ct_tails           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ct_points          <dbl> 9.5, 9.5, 8.0, 1.0, 1.0, 9.5, 11.0, 11.0, 11.0, 1.0…\n$ task_paid          <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ complete           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ total_points       <dbl> 23, 23, 23, 23, 21, 18, 23, 23, 0, 22, 22, 0, 12, 2…\n$ total_local_paid   <dbl> 330, 330, 330, 330, 310, 280, 330, 330, 100, 320, 3…\n$ total_usd_paid     <dbl> 16.5, 16.5, 16.5, 16.5, 15.5, 14.0, 16.5, 16.5, 5.0…\n$ final_version      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ comment            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ f_last_meal_time   <chr> \"3 0\", \"4 0\", \"4 0\", \"1 30\", \"0 30\", \"2 0\", \"1 30\",…\n$ start_time_h       <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 14, 14, 14, 14, 14,…\n$ end_time_h         <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 15, 15, 15, 15, 15,…"
  },
  {
    "objectID": "chapters/data_manipulation.html#basic-data-management",
    "href": "chapters/data_manipulation.html#basic-data-management",
    "title": "12  Data Manipulation",
    "section": "12.3 Basic Data Management",
    "text": "12.3 Basic Data Management\ndplyr uses a collection of verbs to manipulate data that are piped (chained) into each other with a piping operator %>% from magrittr package. The way you use functions in base R is you wrap new function over the previous one, such as k(g(f(x))) this will become impossible to read very quickly as you stack up functions and their arguments. To solve this we will use pipes x %>% f() %>% g() %>% k()! Now you can clearly see that we take x and apply f(), then g(), then k(). Note: base R now also has its own pipe |>, but we will stick to %>% for compatibility across packages.\n\n12.3.1 select()\nselect() selects only the columns that you want, removing all other columns. You can use column position (with numbers) or name. The columns will be displayed in the order you list them. We will select subject_id, temperature, gender and results of raven’s matrices games.\n\nid is a unique subject identification number, where site_id.session_n.subject_n (001.001.001).\nmean_temp_celsius is mean temperature through the session\ngender is gender of the subject.\npr_correct is number of correct answers in piece-rate round.\ntr_correct is number of correct answers in tournament round.\nch_correct is number of correct answers in choice round.\nch_tournament is 1 if participant decided to play tournament and 0 if choice.\n\n\ndata_raven <- data %>% select(id, mean_temp_celsius,gender, pr_correct, tr_correct, ch_tournament, ch_correct) \nhead(data_raven)\n\n# A tibble: 6 × 7\n  id        mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                 <dbl> <chr>       <dbl>      <dbl>         <dbl>   <dbl>\n1 001018001              28.6 Female          7          3             1       9\n2 001018002              28.6 Male            5          6             0       4\n3 001018005              28.6 Male            6          7             0       7\n4 001018009              28.6 Male            1          5             1       7\n5 001018010              28.6 Male            7          9             1      10\n6 001018013              28.6 Female          2          7             0       5\n# … with abbreviated variable name ¹​ch_correct\n\n\nYou can also exclude columns or select everything else with select using -\n\ndata_raven %>% select(-gender) %>% head()\n\n# A tibble: 6 × 6\n  id        mean_temp_celsius pr_correct tr_correct ch_tournament ch_correct\n  <chr>                 <dbl>      <dbl>      <dbl>         <dbl>      <dbl>\n1 001018001              28.6          7          3             1          9\n2 001018002              28.6          5          6             0          4\n3 001018005              28.6          6          7             0          7\n4 001018009              28.6          1          5             1          7\n5 001018010              28.6          7          9             1         10\n6 001018013              28.6          2          7             0          5\n\n\n\n\n12.3.2 filter()\nfilter() keeps only rows that meet the specified criteria. Let’s filter and make 2 data sets one for Males and, one for Females.\n\ndata_male <- data_raven %>% filter(gender == \"Male\") # we use == for comparison\ndata_female <- data_raven %>% filter(gender == \"Female\")\nhead(data_male)\n\n# A tibble: 6 × 7\n  id        mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                 <dbl> <chr>       <dbl>      <dbl>         <dbl>   <dbl>\n1 001018002              28.6 Male            5          6             0       4\n2 001018005              28.6 Male            6          7             0       7\n3 001018009              28.6 Male            1          5             1       7\n4 001018010              28.6 Male            7          9             1      10\n5 001019002              30.7 Male            6          7             1       4\n6 001019008              30.7 Male            5          6             1       6\n# … with abbreviated variable name ¹​ch_correct\n\nhead(data_female)\n\n# A tibble: 6 × 7\n  id        mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                 <dbl> <chr>       <dbl>      <dbl>         <dbl>   <dbl>\n1 001018001              28.6 Female          7          3             1       9\n2 001018013              28.6 Female          2          7             0       5\n3 001018014              28.6 Female          6          6             0       7\n4 001018015              28.6 Female          7          7             0       6\n5 001019001              30.7 Female          5          4             0       4\n6 001019003              30.7 Female          8          6             1       8\n# … with abbreviated variable name ¹​ch_correct\n\n\nWe can chain multiple requirements. Here we will look at Males, temperature over 30 celsius or Females, temperature below 30. Notice that we use & as “and”, | as “or, and wrap the two conditions into”()” to avoid confusion. It could read it as mean_temp_celsius > 30 | gender == \"Female\".\n\ndata_raven %>% \n  filter( \n    (gender == \"Male\" & mean_temp_celsius > 30) | (gender == \"Female\" & mean_temp_celsius < 30)\n        )\n\n# A tibble: 61 × 7\n   id        mean_temp_celsius gender pr_correct tr_correct ch_tournam…¹ ch_co…²\n   <chr>                 <dbl> <chr>       <dbl>      <dbl>        <dbl>   <dbl>\n 1 001018001              28.6 Female          7          3            1       9\n 2 001018013              28.6 Female          2          7            0       5\n 3 001018014              28.6 Female          6          6            0       7\n 4 001018015              28.6 Female          7          7            0       6\n 5 001019002              30.7 Male            6          7            1       4\n 6 001019008              30.7 Male            5          6            1       6\n 7 001019009              30.7 Male            2          6            1       5\n 8 001019010              30.7 Male            6          8            0       8\n 9 001019011              30.7 Male            7          7            1       7\n10 001019012              30.7 Male            4          5            1       6\n# … with 51 more rows, and abbreviated variable names ¹​ch_tournament,\n#   ²​ch_correct\n\n\n\n\n12.3.3 arrange()\narrange() allows you to order the table using a variable. Let’s see which subject scored the worst in pr_correct.\n\ndata_raven %>% arrange(pr_correct) %>% head()\n\n# A tibble: 6 × 7\n  id        mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                 <dbl> <chr>       <dbl>      <dbl>         <dbl>   <dbl>\n1 001018009              28.6 Male            1          5             1       7\n2 001018013              28.6 Female          2          7             0       5\n3 001019004              30.7 Female          2          3             0       6\n4 001019005              30.7 Female          2          6             0       5\n5 001019009              30.7 Male            2          6             1       5\n6 001021013              30.4 Male            2          3             1       2\n# … with abbreviated variable name ¹​ch_correct\n\n\nWe can also sort in descending order using desc() modifier. Let’s look at who scored the most!\n\ndata_raven %>% arrange(desc(pr_correct)) %>% head()\n\n# A tibble: 6 × 7\n  id        mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                 <dbl> <chr>       <dbl>      <dbl>         <dbl>   <dbl>\n1 001019003              30.7 Female          8          6             1       8\n2 001020002              31.6 Male            8          6             1       5\n3 001020013              31.6 Male            8          7             0       7\n4 001021006              30.4 Male            8          4             0       7\n5 001025009              26.8 Male            8          9             1       9\n6 001027015              32.2 Female          8          7             0       8\n# … with abbreviated variable name ¹​ch_correct\n\n\n\n\n12.3.4 mutate()\nmutate() adds new columns and modifies current variables in the data set. Let’s create a dataset with 10 rows and make three new variable columns as an example\n\ntibble(rows = 1:10) %>% mutate(\n  One = 1,\n  Comment = \"Something\",\n  Approved = TRUE\n)\n\n# A tibble: 10 × 4\n    rows   One Comment   Approved\n   <int> <dbl> <chr>     <lgl>   \n 1     1     1 Something TRUE    \n 2     2     1 Something TRUE    \n 3     3     1 Something TRUE    \n 4     4     1 Something TRUE    \n 5     5     1 Something TRUE    \n 6     6     1 Something TRUE    \n 7     7     1 Something TRUE    \n 8     8     1 Something TRUE    \n 9     9     1 Something TRUE    \n10    10     1 Something TRUE    \n\n\nmutate() can use existing variables from the data set to create new ones! Lets convert Celsius to Fahrenheit, see how many point more people scored in tournament over piece-rate round, and check how far from the mean they scored in piece-rate round!\n\ndata_raven %>% mutate(mean_temp_fahrenheit = (mean_temp_celsius * 9/5) + 32,\n                      impovement = tr_correct - pr_correct,\n                      pr_deviation = pr_correct - mean(pr_correct))\n\n# A tibble: 114 × 10\n   id     mean_…¹ gender pr_co…² tr_co…³ ch_to…⁴ ch_co…⁵ mean_…⁶ impov…⁷ pr_de…⁸\n   <chr>    <dbl> <chr>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 00101…    28.6 Female       7       3       1       9    83.5      -4   1.66 \n 2 00101…    28.6 Male         5       6       0       4    83.5       1  -0.342\n 3 00101…    28.6 Male         6       7       0       7    83.5       1   0.658\n 4 00101…    28.6 Male         1       5       1       7    83.5       4  -4.34 \n 5 00101…    28.6 Male         7       9       1      10    83.5       2   1.66 \n 6 00101…    28.6 Female       2       7       0       5    83.5       5  -3.34 \n 7 00101…    28.6 Female       6       6       0       7    83.5       0   0.658\n 8 00101…    28.6 Female       7       7       0       6    83.5       0   1.66 \n 9 00101…    30.7 Female       5       4       0       4    87.2      -1  -0.342\n10 00101…    30.7 Male         6       7       1       4    87.2       1   0.658\n# … with 104 more rows, and abbreviated variable names ¹​mean_temp_celsius,\n#   ²​pr_correct, ³​tr_correct, ⁴​ch_tournament, ⁵​ch_correct,\n#   ⁶​mean_temp_fahrenheit, ⁷​impovement, ⁸​pr_deviation\n\n\nNotice that we can nest functions within mutate(): first we took mean() of the entire column and then subtracted it from pr_correct.\n\n\n12.3.5 recode()\nrecode() modifies the values within a variable. Here is a template:\n\ndata %>% mutate(Variable = recode(Variable, “old value” = “new value”))\n\nLet’s use recode() to change “Male” to “M” and “Female” to “F”.\n\ndata_raven <- data_raven %>% mutate(gender = recode(gender, \"Male\" = \"M\", \"Female\" = \"F\"))"
  },
  {
    "objectID": "chapters/data_manipulation.html#summarize",
    "href": "chapters/data_manipulation.html#summarize",
    "title": "12  Data Manipulation",
    "section": "12.4 summarize()",
    "text": "12.4 summarize()\nsummarize() collapses all rows and returns a one-row summary. We will use summary to calculation what percentage of participant were male, median score in piece-rate round, max score in tournament, percentage of people choosing tournament in choice and mean score in choice round.\n\ndata_raven %>% \n  summarize(perc_male = sum(gender == \"Male\", na.rm = T) / n(),\n            pr_median = median(pr_correct),\n            tr_max = max(tr_correct),\n            ch_ratio = sum(ch_tournament) / n(),\n            ch_mean = mean(ch_correct))\n\n# A tibble: 1 × 5\n  perc_male pr_median tr_max ch_ratio ch_mean\n      <dbl>     <dbl>  <dbl>    <dbl>   <dbl>\n1         0         5      9    0.456    6.01"
  },
  {
    "objectID": "chapters/data_manipulation.html#group_by-and-ungroup",
    "href": "chapters/data_manipulation.html#group_by-and-ungroup",
    "title": "12  Data Manipulation",
    "section": "12.5 group_by() and ungroup()",
    "text": "12.5 group_by() and ungroup()\n\n12.5.1 group_by()\ngroup_by() groups data by specific variables for future operations. We can use group_by() and summarize() to calculate different summary statistics for genders!\n\ndata_raven %>% \n  drop_na(gender) %>%  # removes NA gender\n  group_by(gender) %>% \n  summarize(pr_mean = mean(pr_correct),\n            tr_mean = mean(tr_correct),\n            ch_mean = mean(ch_correct),\n            pr_sd = sd(pr_correct),\n            n = n()) %>%\n  ungroup()\n\n# A tibble: 2 × 6\n  gender pr_mean tr_mean ch_mean pr_sd     n\n  <chr>    <dbl>   <dbl>   <dbl> <dbl> <int>\n1 F         5.22    6.17    5.93  1.58    58\n2 M         5.47    6.4     6.09  1.59    55\n\n\nLet’s group by gender and choice in choice round and look at points in choice round!\n\ndata_raven %>%\n  drop_na(gender) %>%  # removes NA gender\n  group_by(gender, ch_tournament) %>% \n  summarize(ch_mean = mean(ch_correct),\n            pr_sd = sd(ch_correct),\n            n = n()) %>%\n  ungroup()\n\n# A tibble: 4 × 5\n  gender ch_tournament ch_mean pr_sd     n\n  <chr>          <dbl>   <dbl> <dbl> <int>\n1 F                  0    5.73  1.70    33\n2 F                  1    6.2   1.73    25\n3 M                  0    6.07  1.69    29\n4 M                  1    6.12  2.25    26\n\n\n\n\n12.5.2 ungroup()\nungroup() does exactly what you think – removes the grouping! Always ungroup your data after you are done with operation that required grouping, else it will get messy. Look at this example\n\ndata_raven %>%\n  drop_na(gender) %>%  # removes NA gender\n  group_by(gender) %>% \n  mutate(n = n()) %>%\n  mutate(mean_male = mean(gender == \"Male\")) %>%\n  ungroup() %>%\n  select(id, gender, n, mean_male) %>% head(n = 5)\n\n# A tibble: 5 × 4\n  id        gender     n mean_male\n  <chr>     <chr>  <int>     <dbl>\n1 001018001 F         58         0\n2 001018002 M         55         0\n3 001018005 M         55         0\n4 001018009 M         55         0\n5 001018010 M         55         0\n\n\nNotice how mean_male (ratio of male to total) is 0 for Female and 1 for Male. It is because the data was grouped and we performed operation on Males and Females separately.\n\ndata_raven %>%\n  drop_na(gender) %>%  # removes NA gender\n  group_by(gender) %>% \n  mutate(n = n()) %>%\n  ungroup() %>%\n  mutate(mean_male = mean(gender == \"Male\")) %>%\n  select(id, gender, n, mean_male) %>% head(n = 5)\n\n# A tibble: 5 × 4\n  id        gender     n mean_male\n  <chr>     <chr>  <int>     <dbl>\n1 001018001 F         58         0\n2 001018002 M         55         0\n3 001018005 M         55         0\n4 001018009 M         55         0\n5 001018010 M         55         0\n\n\nThis time on the other hand we ungrouped the data, correctly calculating the ratio!\n\n\n12.5.3 rowwise()\nrowwise() produces a row-wise grouping. Later you might want to run a calculation row-wise instead of column-wise, but your column will be filled with an aggregate result. This is where rowwise() comes in to save the day! To demonstrate, we will make a dataframe with a column of lists and try to find the length of each list.\n\ndf <- tibble(\n  x = list(1, 2:3, 4:6,7:11)\n)\n\ndf %>% mutate(length = length(x))\n\n# A tibble: 4 × 2\n  x         length\n  <list>     <int>\n1 <dbl [1]>      4\n2 <int [2]>      4\n3 <int [3]>      4\n4 <int [5]>      4\n\n\nHmmm… Instead of lists’ lengths we got the total number of rows in the data set (length of column x). Now let’s use rowwise()\n\ndf %>% rowwise() %>%\n  mutate(length = length(x))\n\n# A tibble: 4 × 2\n# Rowwise: \n  x         length\n  <list>     <int>\n1 <dbl [1]>      1\n2 <int [2]>      2\n3 <int [3]>      3\n4 <int [5]>      5\n\n\nYey! Here R runs length() on each list separately, giving us correct lengths! Alternatively you can use lengths(), which loops length() over each list."
  },
  {
    "objectID": "chapters/data_manipulation.html#count",
    "href": "chapters/data_manipulation.html#count",
    "title": "12  Data Manipulation",
    "section": "12.6 count()",
    "text": "12.6 count()\ncount() is similar to group_by() and summarize() combo – it collapses the rows and counts the number of observations per group of values.\n\ndata_raven %>% count(gender)\n\n# A tibble: 3 × 2\n  gender     n\n  <chr>  <int>\n1 F         58\n2 M         55\n3 <NA>       1\n\n\nSimilar to group_by() you can select multiple columns to group.\n\ndata_raven %>% count(gender, ch_tournament)\n\n# A tibble: 5 × 3\n  gender ch_tournament     n\n  <chr>          <dbl> <int>\n1 F                  0    33\n2 F                  1    25\n3 M                  0    29\n4 M                  1    26\n5 <NA>               1     1"
  },
  {
    "objectID": "chapters/data_manipulation.html#rename",
    "href": "chapters/data_manipulation.html#rename",
    "title": "12  Data Manipulation",
    "section": "12.7 rename()",
    "text": "12.7 rename()\nrename() renames a column. Notice that the “New Name” is on the left and “Old Name” is on the right. Let’s rename id to subject_id and gender to sex.\n\ndata_raven %>% rename(\"subject_id\" = \"id\", \"sex\" = \"gender\") %>% head(n = 5)\n\n# A tibble: 5 × 7\n  subject_id mean_temp_celsius sex   pr_correct tr_correct ch_tournament ch_co…¹\n  <chr>                  <dbl> <chr>      <dbl>      <dbl>         <dbl>   <dbl>\n1 001018001               28.6 F              7          3             1       9\n2 001018002               28.6 M              5          6             0       4\n3 001018005               28.6 M              6          7             0       7\n4 001018009               28.6 M              1          5             1       7\n5 001018010               28.6 M              7          9             1      10\n# … with abbreviated variable name ¹​ch_correct"
  },
  {
    "objectID": "chapters/data_manipulation.html#row_number",
    "href": "chapters/data_manipulation.html#row_number",
    "title": "12  Data Manipulation",
    "section": "12.8 row_number()",
    "text": "12.8 row_number()\nrow_number() fills a column with consecutive numbers. It is especially useful if you need to create an id column. Let’s remove id column and make a new one with row_number().\n\ndata_raven %>% \n  select(-id) %>%\n  mutate(id = row_number()) %>% \n  relocate(id) %>% # used to move id at the beginning\n  head(n = 5)\n\n# A tibble: 5 × 7\n     id mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  <int>             <dbl> <chr>       <dbl>      <dbl>         <dbl>      <dbl>\n1     1              28.6 F               7          3             1          9\n2     2              28.6 M               5          6             0          4\n3     3              28.6 M               6          7             0          7\n4     4              28.6 M               1          5             1          7\n5     5              28.6 M               7          9             1         10"
  },
  {
    "objectID": "chapters/tidy_data.html#example",
    "href": "chapters/tidy_data.html#example",
    "title": "13  Tidy Data",
    "section": "13.1 Example",
    "text": "13.1 Example\nTidy data is not simply a theoretical idea, it has a practical application to structuring data. For example, take the following method of storing data on payments during an experiment:\n\nWhen computer reads the data it does not understand what we are trying to say, so all column and will be read as is. If we have to enter multiple days, we will need to recreate similar tables, increasing chances of mistakes. What if we want to get a total for the entire experiment? We would have to manually and sum all cells. Compare this to:\n\nHere data input is extremely easy, and if we want to get any summary tables it is as easy as making a pivot table. Tidy data approach from the start will help you create robust tables and save you time in analysis."
  },
  {
    "objectID": "chapters/tidy_data.html#pivot_longer",
    "href": "chapters/tidy_data.html#pivot_longer",
    "title": "13  Tidy Data",
    "section": "13.2 pivot_longer()",
    "text": "13.2 pivot_longer()\nA common problem is a dataset where column names are not names of variables, but value of a variable. This is true for pr_correct, tr_correct, ch_correct as the column names represent name of the game variable, the values in the columns represent number of correct answers, and each row represents two observations, not one.\nTo tidy a dataset, we need to pivot the offending columns into a new pair of variables. To carry out the operation we need to supply:\n\nThe columns whose names are values, not variables – columns we want to pivot. Here pr_correct, tr_correct, ch_correct.\nThe name of the variable to move the column names to. Here game. The default name.\nThe name of the variable to move the column values to. Here n_correct. The default value.\n\n\ndata_raven %>% \n  pivot_longer(c(pr_correct, tr_correct, ch_correct), names_to = \"game\", values_to = \"n_correct\") %>%\n  select(id,game,n_correct) %>% \n  head(n = 5)\n\n# A tibble: 5 × 3\n  id        game       n_correct\n  <chr>     <chr>          <dbl>\n1 001018001 pr_correct         7\n2 001018001 tr_correct         3\n3 001018001 ch_correct         9\n4 001018002 pr_correct         5\n5 001018002 tr_correct         6"
  },
  {
    "objectID": "chapters/tidy_data.html#pivot_wider",
    "href": "chapters/tidy_data.html#pivot_wider",
    "title": "13  Tidy Data",
    "section": "13.3 pivot_wider()",
    "text": "13.3 pivot_wider()\npivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows. For example, let’s have a look at data_raven_accident, where gnomes stacked mean_temp_celsius and ch_tournament. Here an observation is spread across two rows.\n\n\n\n\ndata_raven_accident %>% head(n=5)\n\n# A tibble: 5 × 3\n  id        name              value\n  <chr>     <chr>             <dbl>\n1 001018001 mean_temp_celsius  28.6\n2 001018001 ch_tournament       1  \n3 001018002 mean_temp_celsius  28.6\n4 001018002 ch_tournament       0  \n5 001018005 mean_temp_celsius  28.6\n\n\nTo tidy this up, we need two parameters:\n\nThe column to take variable names from. Here, it’s name.\nThe column to take values from. Here it’s value.\n\n\ndata_raven_accident %>% pivot_wider(names_from = name, values_from = value) %>% head(n = 5)\n\n# A tibble: 5 × 3\n  id        mean_temp_celsius ch_tournament\n  <chr>                 <dbl>         <dbl>\n1 001018001              28.6             1\n2 001018002              28.6             0\n3 001018005              28.6             0\n4 001018009              28.6             1\n5 001018010              28.6             1\n\n\nIt is evident from their names that pivot_wider() and pivot_longer() are inverse functions. pivot_longer() converts wide tables to a longer and narrower format; pivot_wider() converts long tables to a shorter and wider format."
  },
  {
    "objectID": "chapters/tidy_data.html#separate-and-unite",
    "href": "chapters/tidy_data.html#separate-and-unite",
    "title": "13  Tidy Data",
    "section": "13.4 separate() and unite()",
    "text": "13.4 separate() and unite()\nSome times data comes with columns united, so we want to separate() them to make the data tidy.\n\ndata_raven_sep %>% head(n=5)\n\n# A tibble: 5 × 2\n  id        `gender/pr_correct`\n  <chr>     <chr>              \n1 001018001 Female/7           \n2 001018002 Male/5             \n3 001018005 Male/6             \n4 001018009 Male/1             \n5 001018010 Male/7             \n\n\n\ndata_raven_sep %>% separate(col = \"gender/pr_correct\", into = c(\"gender\",\"pr_correct\"), sep = \"/\") %>% head(n=5)\n\n# A tibble: 5 × 3\n  id        gender pr_correct\n  <chr>     <chr>  <chr>     \n1 001018001 Female 7         \n2 001018002 Male   5         \n3 001018005 Male   6         \n4 001018009 Male   1         \n5 001018010 Male   7         \n\n\nWhat if we have one column split across multiple columns? Our subject id code is made out of site_id, session_n, subject_n. But playful gnomes broke it down into three columns, so now we have to unite() them into one.\n\ndata_raven_uni %>% head(n=5)\n\n# A tibble: 5 × 4\n  site_id session_n subject_n gender\n  <chr>   <chr>     <chr>     <chr> \n1 001     018       001       Female\n2 001     018       002       Male  \n3 001     018       005       Male  \n4 001     018       009       Male  \n5 001     018       010       Male  \n\n\n\ndata_raven_uni %>% unite(c(site_id, session_n, subject_n),col = \"id\", sep = \"\") %>% head(n = 5)\n\n# A tibble: 5 × 2\n  id        gender\n  <chr>     <chr> \n1 001018001 Female\n2 001018002 Male  \n3 001018005 Male  \n4 001018009 Male  \n5 001018010 Male"
  },
  {
    "objectID": "chapters/tidy_data.html#tibble-and-tribble",
    "href": "chapters/tidy_data.html#tibble-and-tribble",
    "title": "13  Tidy Data",
    "section": "13.5 tibble() and tribble()",
    "text": "13.5 tibble() and tribble()\nA tibble is a special kind of data frame in R. Tibbles are a modern re-imagining of the data frame, designed to be more friendly and consistent than traditional data frames.\n\nTibbles are similar to data frames in that they can hold tabular data, but they have some key differences:\nTibbles display only the first 10 rows by default when printed, making them easier to work with large datasets.\nTibbles use a consistent printing format, making it easier to work with multiple tibbles in the same session.\nTibbles have a consistent subsetting behavior, making it easier to select columns by name. When printed, the data type of each column is specified.\nSubsetting a tibble will always return a tibble. You don’t need to use drop = FALSE compared to traditional data.frames.\nAnd most importantly, you can have column consisting of lists.\n\nIn summary, Tibbles are a more modern and consistent version of data frames, they are less prone to errors and more readable, making them a great choice for data manipulation and exploration tasks.\nTo make a tibble we can use tibble() similar to data.frame()\n\ntibble(x = c(1,2,3),\n       y = c('one', 'two', 'three'))\n\n# A tibble: 3 × 2\n      x y    \n  <dbl> <chr>\n1     1 one  \n2     2 two  \n3     3 three\n\n\nYou can also you tribble() for creating a row-wise, readable tibble in R. This is useful for creating small tables of data. Syntax: tribble (~column1, ~column2) where, Row column — represents the data in row by row layout.\n\ntribble(~x, ~y,\n        1, \"one\",\n        2, \"two\",\n        3, \"three\")\n\n# A tibble: 3 × 2\n      x y    \n  <dbl> <chr>\n1     1 one  \n2     2 two  \n3     3 three"
  },
  {
    "objectID": "chapters/tidy_data.html#janitor-clean-your-data",
    "href": "chapters/tidy_data.html#janitor-clean-your-data",
    "title": "13  Tidy Data",
    "section": "13.6 janitor Clean Your Data",
    "text": "13.6 janitor Clean Your Data\njanitor is designed to make the process of cleaning and tidying data as simple and efficient as possible. To learn more about the function check out this vignette! If you are interested in making summary tables with janitior check this vignette!\n\n13.6.1 clean_names()\nclean_names() is used to clean variable names, particularly those that are read in from Excel files using readxl::read_excel() and readr::read_csv(). It parses letter cases, separators, and special characters to a consistent format, converts certain characters like “%” to “percent” and “#” to “number” to retain meaning, resolves duplicate names and empty names. It is recommended to call this function every time data is read.\n\n# Create a data.frame with dirty names\ntest_df <- as.data.frame(matrix(ncol = 6))\nnames(test_df) <- c(\"camelCase\", \"ábc@!*\", \"% of respondents (2009)\",\n                    \"Duplicate\", \"Duplicate\", \"\")\ntest_df %>% colnames()\n\n[1] \"camelCase\"               \"ábc@!*\"                 \n[3] \"% of respondents (2009)\" \"Duplicate\"              \n[5] \"Duplicate\"               \"\"                       \n\n\n\nlibrary(janitor)\ntest_df %>% clean_names() %>% colnames()\n\n[1] \"camel_case\"                  \"abc\"                        \n[3] \"percent_of_respondents_2009\" \"duplicate\"                  \n[5] \"duplicate_2\"                 \"x\"                          \n\n\n\n\n13.6.2 remove_empty()\nremove_empty() removes empty row and columns, especially useful after reading Excel files.\n\ntest_df2 <- data.frame(numbers = c(1, NA, 3),\n                       food = c(NA, NA, NA),\n                       letters = c(\"a\", NA, \"c\"))\ntest_df2\n\n  numbers food letters\n1       1   NA       a\n2      NA   NA    <NA>\n3       3   NA       c\n\ntest_df2 %>%\n  remove_empty(c(\"rows\", \"cols\"))\n\n  numbers letters\n1       1       a\n3       3       c\n\n\n\n\n13.6.3 remove_constant()\nremove_constant() drops columns from a data.frame that contain only a single constant value (with an na.rm option to control whether NAs should be considered as different values from the constant).\n\ntest_df3 <- data.frame(cool_numbers = 1:3, boring = \"the same\")\ntest_df3\n\n  cool_numbers   boring\n1            1 the same\n2            2 the same\n3            3 the same\n\ntest_df3 %>% remove_constant()\n\n  cool_numbers\n1            1\n2            2\n3            3\n\n\n###convert_to_date() and convert_to_datetime() Remember loading data from Excel and seeing 36922.75 instead of dates? Well, convert_to_date() and convert_to_datetime() will convert this format and other date-time formats to actual dates! If you need more customization check excel_numeric_to_date().\n\nconvert_to_date(36922.75)\n\n[1] \"2001-01-31\"\n\nconvert_to_datetime(36922.75)\n\n[1] \"2001-01-31 18:00:00 UTC\"\n\n\n###row_to_names() row_to_names() is a function that takes the names of variables stored in a row of a data frame and makes them the column names of the data frame. It can also remove the row that contained the names, and any rows above it, if desired.\n\ntest_df4 <- data.frame(X_1 = c(NA, \"Names\", 1:3),\n                   X_2 = c(NA, \"Value\", 4:6))\ntest_df4\n\n    X_1   X_2\n1  <NA>  <NA>\n2 Names Value\n3     1     4\n4     2     5\n5     3     6\n\nrow_to_names(test_df4, 2)\n\n  Names Value\n3     1     4\n4     2     5\n5     3     6"
  },
  {
    "objectID": "chapters/data_bases.html#sec-relationships",
    "href": "chapters/data_bases.html#sec-relationships",
    "title": "14  Relational Databases",
    "section": "14.1 Types of Relationships",
    "text": "14.1 Types of Relationships\nOne to One (1:1)\nA single row in the first table is related to only one row in the second table, and a single row in the second table is related to only one row in the first table.\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n  \n\ncluster_1\n\n   \n\nTable1\n\n Country China France Italy    \n\nTable2\n\n Capital Beijing Paris Rome    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n   \n\n\n\n\n\nOne to Many (1:M) A single row in the first table is related to many rows in the second table, and a single row in the second table is related to only one row in the first table.\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n  \n\ncluster_1\n\n   \n\nTable1\n\n Professor Hobbs Doe    \n\nTable2\n\n Class 690-02 692-01 405-01    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n   \n\n\n\n\n\nMany to Many (M:N) A single row in the first table is related to many rows in the second table, and a single row in the second table is related to many rows in the first table.\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n  \n\ncluster_1\n\n   \n\nTable1\n\n Student John David Celeste    \n\nTable2\n\n Class 690-02 692-01 405-01    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w\n\n    \n\nTable1:e->Table2:w"
  },
  {
    "objectID": "chapters/data_bases.html#key-unique-identifier",
    "href": "chapters/data_bases.html#key-unique-identifier",
    "title": "14  Relational Databases",
    "section": "14.2 Key – Unique Identifier",
    "text": "14.2 Key – Unique Identifier\nPrimary key (PK) is a column that uniquely identifies each row within a table. Every table should have a primary key! In order to connect (relate) tables, we put PKs of a table into another table. A PK of a table in another table is called foreign key (FK).\n\n\nKeys\n\n\n\n\n\n\nClassID (PK)\nProfessorID (FK)\nCredits\nLocation\n\n\n\n\n620-01\n1\n4\nLM-340\n\n\n623-01\n2\n2\nUM-102\n\n\n663-01\n2\n2\nLO-234\n\n\n\n\n\n\n\n\nProfessorID (PK)\nProfessor\n\n\n\n\n1\nArman\n\n\n2\nAlessandra"
  },
  {
    "objectID": "chapters/data_bases.html#types-of-joins",
    "href": "chapters/data_bases.html#types-of-joins",
    "title": "14  Relational Databases",
    "section": "14.3 Types of Joins",
    "text": "14.3 Types of Joins\nIn relational databases, a join is used to combine two or more tables based on a related column between them. There are different types of joins. To illustrate how the joins work we will be using two tables employees and projects. The first table contains employee_id (PK) and name, and the second one contains project_id (PK) and employee_id (FK).\n\n# | layout-ncol: 2\nprint(employees)\n\n  employee_id  name\n1           1  John\n2           2  Jane\n3           3   Bob\n4           4 Alice\n5           5   Tom\n\nprint(projects)\n\n  project_id employee_id\n1          1           1\n2          2           2\n3          3           3\n4          4           1\n5          5           4\n6          6           6\n\n\n\n14.3.1 Outer Joins\nOuter join are used return matched data or unmatched data from one or both tables. You can see as making a more comprehensive table.\n\n14.3.1.1 Left Join\nLeft join returns all the rows from the left table and only the matched rows from the right table. In an essence you are adding extra information to your left table.\n\n\n\nLeft Join\n\n\n\nleft_join_result <- employees %>%\n  left_join(projects, by = \"employee_id\")\n\nWarning in left_join(., projects, by = \"employee_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\nprint(left_join_result)\n\n  employee_id  name project_id\n1           1  John          1\n2           1  John          4\n3           2  Jane          2\n4           3   Bob          3\n5           4 Alice          5\n6           5   Tom         NA\n\n\n\n\n14.3.1.2 Right Join\nRight join is very similar to left join, except it returns all the rows from the right table and only the matched rows from the left table.\n\n\n\nRight Join\n\n\n\nright_join_result <- employees %>%\n  right_join(projects, by = \"employee_id\")\n\nWarning in right_join(., projects, by = \"employee_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\nprint(right_join_result)\n\n  employee_id  name project_id\n1           1  John          1\n2           1  John          4\n3           2  Jane          2\n4           3   Bob          3\n5           4 Alice          5\n6           6  <NA>          6\n\n\n\n\n14.3.1.3 Full Join\nFull join Returns all the rows from both tables and null values for non-matching rows. Here we are combining both tables\n\n\n\nFull Join\n\n\n\nfull_join_result <- employees %>%\n  full_join(projects, by = \"employee_id\")\n\nWarning in full_join(., projects, by = \"employee_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\nprint(full_join_result)\n\n  employee_id  name project_id\n1           1  John          1\n2           1  John          4\n3           2  Jane          2\n4           3   Bob          3\n5           4 Alice          5\n6           5   Tom         NA\n7           6  <NA>          6\n\n\n\n\n14.3.1.4 Inner Join\nInner join returns only the matched rows between two table. So, only the rows that found a match from both tables will be kept.\n\n\n\n\n\nInner Join\n\n\n\n\ninner_join_result <- employees %>%\n  inner_join(projects, by = \"employee_id\")\n\nWarning in inner_join(., projects, by = \"employee_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\nprint(inner_join_result)\n\n  employee_id  name project_id\n1           1  John          1\n2           1  John          4\n3           2  Jane          2\n4           3   Bob          3\n5           4 Alice          5\n\n\n\n\n\n\n\n14.3.2 Filtering Joins\n\n14.3.2.1 Anti Join\nAn anti join returns the rows from the left table that do not have a corresponding match in the right table, without adding any new columns to the output. It is useful when you want to filter rows based on the absence of matching entries in the other table.\n\n\n\nanti_join_result <- employees %>%\n  anti_join(projects, by = \"employee_id\")\n\nprint(anti_join_result)\n\n  employee_id name\n1           5  Tom\n\n\n\nTom Does not have a project, assigned! May be he can take care of the project 6?\n\n\n\n\n14.3.2.2 Semi Join\nA semi join resembles an inner join in that it identifies matching rows between two tables. However, unlike an inner join, it does not add any new columns to the output. Instead, it filters the rows from the left table that have a corresponding match in the right table. You would use a semi join when you want to filter the left table based on the presence of matching entries in the right table.\n\nsemi_join_result <- employees %>%\n  semi_join(projects, by = \"employee_id\")\n\nprint(semi_join_result)\n\n  employee_id  name\n1           1  John\n2           2  Jane\n3           3   Bob\n4           4 Alice"
  },
  {
    "objectID": "chapters/data_bases.html#drawing-databases",
    "href": "chapters/data_bases.html#drawing-databases",
    "title": "14  Relational Databases",
    "section": "14.4 Drawing Databases",
    "text": "14.4 Drawing Databases\nRemember we talked about UML? We are going to use it to draw out the Entity-Relationship Model to understand how tables in our database are related and how they are working together, which will simplify our later work. You can use UML code tools such as Mermaid and Graphviz, but in my opinion drag and drop web applications such as LucidChart and Draw.io are better for this task. To begin, let’s introduce an entity, which is an object (place,person,thing) that we want to track in our database. In our case it will be a customer, order, and product. Each entity has attributes, for example, customer has customer_id, name, email, address, etc. and similarly other entities also have a list of attributes. In your table entities will be reflected as rows and attributes as columns. These tables can be related to each other, which we show by drawing a line between the tables. To describe such relationships in numeric terms we use cardinality. You can think of cardinality as minimum and maximum number of relationships, very much like in Section 14.1 .\n\n\n\nCardinality\n\n\nLet’s for examine the relationship between customer and order. Let’s first ask a question: what relationship does customer have with order? Using min, max framework, what is the minimum number of order a customer can have and what is the maximum number of order a customer can have? They can have no orders, so min = 0, and they can have many orders (2, 3, 4…), so max = many. We get customer to order relationship is 0 or many. Now let’s look in the opposite directions: how many customer can an order have? An order can have only one customer, no more no less. So, the relationship is one and only one.\n\n\n\nCustomer-Order Relationship\n\n\nNow let’s do the same for order and product. One order can have one or many products in it, and each product can be in none or many orders. The full diagram will look something like this:\n\n\n\nEntity-Relationship Diagram\n\n\nYou should draw a diagram any time you are planning to start a project with complex design. It will make crystal clear what tables you need to build and how to relate them. You can also scetch out a diagram whenever you are confused about a data set. If you would like to learn more on this topic, check out Lucid Software YouTube guides and Neso Academy’s Database Management Systems course."
  },
  {
    "objectID": "chapters/data_viz.html#types-of-data-visualisation",
    "href": "chapters/data_viz.html#types-of-data-visualisation",
    "title": "15  Data Visualization",
    "section": "15.1 Types of Data Visualisation",
    "text": "15.1 Types of Data Visualisation\nData visualization is an essential tool for understanding and communicating complex information. There are two main types of visualization: 1. Exploratory It is common to look at summary statistics such as mean and standard deviation. But these numbers obscure the datapoints hiding the form of our datasets. Matejka and Fitzmaurice generated datasets with Identical Statistics that look distinctly different. You can access all 12 patterns with datasauRus package. It is important to see the structure to move your analyses forward.\n\n\n\n\n \n  \n    mean_x \n    mean_y \n    std_dev_x \n    std_dev_y \n    corr_x_y \n  \n \n\n  \n    54.26 \n    47.83 \n    16.77 \n    26.94 \n    -0.06 \n  \n\n\n\n\n\n\n\nShow the code\ndatasauRus::datasaurus_dozen %>% filter(dataset %in% c(\"away\",\"dino\", \"star\")) %>%\nmutate(dataset = str_to_upper(dataset)) %>%\nggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_void(base_size = 18) +\n  theme(legend.position = \"none\",  \n        strip.text = element_text(face = \"bold\")) +\n  facet_wrap(~dataset, ncol = 3) +\n  coord_fixed(ratio = 0.8)\n\n\n\n\n\n\nExplanatory So, you got your results together and now you need to not only present them, but also convince non-techical audience. They don’t care whether your model user cross-validation or how you optimized your gradient boosted forest, all they want is a convincing simple message. That is why you won’t see fency overloaded graphs in forward facing presentation it all about the message. Look at the graph Apple used to show their M1 MacBooks are better. \n\nR offers a variety of packages for creating visually appealing and informative plots. One of the most popular and versatile packages for data visualization in R is ggplot2. We will explore the basics of using ggplot2 to create different types of plots and customize them to suit your needs. We can load it separately libary(ggplot2) or with libary(tidyverse)."
  },
  {
    "objectID": "chapters/data_viz.html#grammar-of-graphics",
    "href": "chapters/data_viz.html#grammar-of-graphics",
    "title": "15  Data Visualization",
    "section": "15.2 Grammar of Graphics",
    "text": "15.2 Grammar of Graphics\nThe Grammar of Graphics is a concept in data visualization that was developed by Leland Wilkinson in his book “The Grammar of Graphics” (The Grammar of Graphics 2005) in 1999. The Grammar of Graphics is essentially a system of rules that describes how to represent data visually using a set of graphical elements and mappings between data variables and visual properties.\n\nThe Grammar of Graphics. 2005. Statistics and Computing. New York: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n“Excel Enjoyers” are familiar with the Excel plotting workflow: you select a plot you want and it just produces one for you.\n\n\n\n“Excel GUI”\n\n\nUnder this framework scatter plot and bar plots appear completely different:\n\n\nShow the code\npoint_plot <- data_raven %>% count(pr_correct, name = \"count\") %>% ggplot(aes(x = pr_correct, y = count)) + geom_point(size = 3) + theme_minimal()\ncol_plot <- data_raven %>% count(pr_correct, name = \"count\") %>% ggplot(aes(x = pr_correct, y = count)) + geom_col() + theme_minimal()\npoint_plot + col_plot\n\n\n\n\n\nHowever, under the Grammar of Graphics we see how similar these graphics are! They are exactly the same in terms everything, but geometries! The first one use “points” while the second uses “columns” to display the data.\nThe Grammar of Graphics provides a framework for creating complex visualizations by breaking down the visualization process into a set of components.\n\n\n\n“Grammar of Graphics Visual, from QCBS R Workshop 3”\n\n\n\nData: The information that is being visualized. To explore how grammar of graphics works in ggplot2 we will use iris dataset, which is a built-in dataset of measurements of different parts of iris flowers.\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe Data layer of the graph is just a blank canvas. Becase we have not specified any graphing elements yet.\n\ndata_layer <- ggplot(data = iris)\ndata_layer\n\n\n\n\n\nAesthetics: The visual properties used to represent the data, such as x, y, color or size. Once we add aethetics we see out plotting area being set up and if we check mapping we see that Sepal.Length was assigned to x and Sepal.Width was assigned to y.\n\n\naes_layer <- ggplot(data = iris, \n                    aes(x = Sepal.Length, y = Sepal.Width)) \naes_layer\n\n\n\naes_layer$mapping\n\nAesthetic mapping: \n* `x` -> `Sepal.Length`\n* `y` -> `Sepal.Width`\n\n\n\nGeometries: The visual elements used to represent the data, such as points or bars. Once we add geometry we start seeing our data!\n\n\ngeometry_layer <- aes_layer + geom_point()\ngeometry_layer\n\n\n\n\n\nScales: The mapping between the data and the aesthetics, such as how numeric values are mapped to positions on a graph. There are different scales for color, fill, size, log(x), etc. Here we added scale color. Checking the mapping we see Species is mapped to colour.\n\n\nscales_layer <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  # We case scale function to edit the scale for example we can set our own color manually\n  scale_color_manual(values = c(\"red\",\"orange\",\"pink\"))\nscales_layer\n\n\n\nscales_layer$mapping\n\nAesthetic mapping: \n* `x`      -> `Sepal.Length`\n* `y`      -> `Sepal.Width`\n* `colour` -> `Species`\n\n\n\nStatistics: Mathematical transformations applied to the data before visualization, such as summary statistics or new variables. Histogram for example splits data into bins and counts observations.\n\n\nstat_layer <- ggplot(data = iris, aes(x = Sepal.Length)) + \n  geom_histogram(bins = 20, color = \"white\")\nstat_layer\n\n\n\n\n\nFacets: Ways of dividing the data into subgroups and creating separate visualizations for each subgroup.\n\n\nfacets_layer <- geometry_layer + facet_wrap(vars(Species), ncol = 3) \nfacets_layer\n\n\n\n\n\nTheme: Adding Polishing touches to your visual and making it look exactly the way you want.\n\n\ntheme_layer <- facets_layer + theme_minimal(base_size = 18) +  \n  geom_point(size = 2, color = \"#ffb86c\") +\n  theme(plot.background = element_rect(fill = \"#282a36\", color = \"#44475A\"),\n        axis.text = element_text(color = \"#f8f8f2\"),\n        axis.title = element_text(color = \"#f8f8f2\"),\n        strip.text = element_text(color = \"#f8f8f2\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(colour = \"#44475a\")\n        ) +\n  labs(x = \"Sepal Length\", y = \"Sepal Width\")\ntheme_layer"
  },
  {
    "objectID": "chapters/data_viz.html#ggplot",
    "href": "chapters/data_viz.html#ggplot",
    "title": "15  Data Visualization",
    "section": "15.3 ggplot()",
    "text": "15.3 ggplot()\nIf you used R before then you are familiar with the default graphing function plot,hist, etc. ggplot2 has it own version of quickly making a graph qplot(). To learn about qplot() check out this vignette.\n\ndata_raven %>% pull(pr_correct) %>% hist() \n\n\n\ndata_raven %>% qplot(pr_correct, data = ., geom = 'histogram', bins = length(unique(data_raven$pr_correct)))\n\n\n\n\nThe ggplot() function sets up the basic structure of a plot, and additional layers, such as points, lines, and facets, can be added using + operator (like %>%, but for +). This makes it easy to understand, modify the code, and build complex plots by adding layers. This allows for easy creation of plots that reveal patterns in the data. In contrast, the basic R plotting functions and qplot() have a simpler and less expressive syntax, making it harder to create complex and multi-layered plots. Mastering ggplot() is well worth your time and effort as it will teach you how to think about graphs and what goes into building them. For example, let’s improve the histogram from earlier!\n\ndata_raven %>% \n  count(pr_correct) %>% # I prefer calculating statistics myself\n  ggplot(aes(x = as.factor(pr_correct), y = n)) + # We use aes to set x and y\n  geom_col(fill = \"steelblue\") +\n  theme_minimal(base_size = 15) + \n  theme(panel.grid = element_blank(), \n        panel.grid.major.y = element_line(linewidth = 0.5, linetype = 2, color = \"grey\")) +\n  labs(x = \"Number of Correct Answers\", \n       y = \"Subject Count\", \n       title = \"Distribution of Correct Answers in Piece-rate Game\")\n\n\n\n\nAh much better! We added labels, removed unnecessary grid lines, and added some color. If you want to learn more about ggplot check out ggplot2: Elegant Graphics for Data Analysis (Hadley 2016) and the cheatsheet.\n\nHadley, Wickham. 2016. Ggplot2. New York, NY: Springer Science+Business Media, LLC.\nWe can use an amazing package esquisse to build our plots with drag-and-drop!\n\n# install.packages('esquisse')\nlibrary(esquisse)\n\nYou can access esquisse by going to “Addins” in the top panel or with esquisser(your_data). Now go learn more about this package here."
  },
  {
    "objectID": "chapters/data_viz.html#tips",
    "href": "chapters/data_viz.html#tips",
    "title": "15  Data Visualization",
    "section": "15.4 Tips",
    "text": "15.4 Tips\n\n15.4.1 group\nUsually ggplot groups your data by one the aesthetics you provided such as color and fill; however, sometimes it fails to do so. When that happens it is worth specifying group argument on your own.\nNotice how labels for years 2020, 2021, 2022 are all over the place.\n\n\nShow the code\nspending_plot_data %>% \n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) + \n  geom_col(position=\"fill\", show.legend = T) +\n  scale_fill_manual( \n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")) +\n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label( size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nIf we specify group aesthetic everything goes back to its place!\n\n\nShow the code\nspending_plot_data %>% \n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) + \n  geom_col(position=\"fill\", show.legend = T) +\n  scale_fill_manual( \n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")) +\n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label( aes(group = agency),size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nThis error is very common in line charts too.\n\n\nShow the code\ngroup_line_data <- tibble(\n measure = c(rep(\"hot\",5),rep(\"cool\",5)),\n date = rep(seq(2000,2004, by = 1),2),\n value = c(89,111,100,130,159,24,37,88,69,105)\n) \n\n\n\n\nShow the code\ngroup_line_data %>% ggplot (aes(x=date, y= value)) + geom_line() + theme (legend.position = \"bottom\",legend.title = element_blank()) + theme_minimal()\n\n\n\n\n\nShow the code\ngroup_line_data %>% ggplot (aes(x=date, y= value, group = measure)) + geom_line () + theme (legend.position = \"bottom\",legend.title = element_blank()) + theme_minimal()"
  },
  {
    "objectID": "chapters/color.html#categorical-data",
    "href": "chapters/color.html#categorical-data",
    "title": "16  Color",
    "section": "16.1 Categorical Data",
    "text": "16.1 Categorical Data\n\n16.1.1 Highlight Important Point\n\n\nShow the code\ngapminder1997eu <- gapminder %>% filter((year == 1997) & (continent == \"Europe\"))\ncontinents <- gapminder %>% select(-country) %>% mutate(gdp = gdpPercap * pop) %>% group_by(year,continent) %>% summarize(total_pop_mil = sum(pop)/10^6, total_gdp = sum(gdp), total_gdppc = total_gdp/(total_pop_mil*10^6)) %>% ungroup()\nmalay_miracle <- gapminder %>% filter(country %in% c(\"Malaysia\", \"Vietnam\", \"Indonesia\", \"Thailand\"))\nasian_tigers <- gapminder %>% filter(country %in% c(\"Hong Kong, China\", \"Taiwan\", \"Singapore\", \"Korea, Rep.\")) %>%\n  mutate(country = recode(country,  \"Hong Kong, China\" = \"Hong Kong\",  \"Korea, Rep.\" = \"South Korea\")) \n\n\nWe use color to emphasize certain data and give context. Below are graphs comparing GDP of European Countries in 1997. First graph gives each country its own color, creating a “explosion at a candy factory”. Second graph is an improvement as it removes the distraction, emphasizes the country of the interest by highlighting “Greece” and greying out the rest. The “Red” color signals that Greece might be doing not so well.\n\n\nShow the code\np1 <- gapminder1997eu %>% \n  mutate(is.Greece = country == \"Greece\") %>%\n  ggplot(aes(y = fct_reorder(country, gdpPercap), x = gdpPercap)) + \n  geom_segment(aes(yend = country, xend=0, color = country), size = 1, show.legend = FALSE) +\n  geom_point(aes(color = country), show.legend = FALSE, size = 3) + \n  theme_minimal(base_size = 12) + \n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank()) +\n # ggplot2::scale_color_manual(values = c(\"#7286D3\",\"#D37286\")) +\n  labs(x = \"GDP per Capita\", y = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = 'off')\n\np2 <- gapminder1997eu %>% \n  mutate(is.Greece = country == \"Greece\") %>%\n  ggplot(aes(y = fct_reorder(country, gdpPercap), x = gdpPercap)) + \n  geom_segment(aes(yend = country, xend=0, color = is.Greece), size = 1, show.legend = FALSE) +\n  geom_point(aes(color = is.Greece), show.legend = FALSE, size = 3) + \n  theme_minimal(base_size = 12) + \n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank()) +\n  ggplot2::scale_color_manual(values = c(\"grey\",\"#D37286\")) +\n  labs(x = \"GDP per Capita\", y = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = 'off')\n\np1 + p2 + plot_annotation(title = \"Countries in Europe by GDP per Capita\",theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.1.2 Comparing Two Things\n\n16.1.2.1 Complementary Harmony with a Positive/Negative Connotation\nComplementary Harmony refers to the use of colors that are opposite each other on the color wheel to create a strong contrast. This creates a positive/negative connotation that is good for showcasing differences. Colors near each other on the wheel can also work well together, but opposite colors provide the strongest support for a key color. The example below shows the comparison between population of Asia and Europe. The use of bright purple emphasizes the outstanding population growth of Asia, while the green color highlights the slower population growth in Europe.\n\n\nShow the code\np3 <- continents %>% \n  filter(continent %in% c(\"Asia\",\"Europe\")) %>% \n  ggplot(aes(x=year, y = total_pop_mil, color = continent)) + \n  geom_line(linewidth = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_y_log10() + \n  scale_color_manual(values = c(\"#DA70D6\", \"#70DA74\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n  \n\ncomplementary + p3 + plot_annotation(title = \"Complementary Harmony with a Positive/Negative Connotation\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.1.2.2 Near Complementary Harmony for Highlighting Two Series Where One Is the Primary Focus\nNear Complementary Harmony is a color scheme that creates good contrast without using polar opposite colors. It involves selecting a color that is 33% around the color wheel from the key color instead of the full 50%. This works well for highlighting two series where one is the primary focus. It is best to use warm colors for the key color and cool colors for the complementary colors. If necessary, the complementary colors can be muted by decreasing their saturation or altering their lightness to reduce the contrast with the background. The example below highlights the importance of population growth in Asia, with Europe being presented neutrally as a point of comparison rather than as a slow-growing region.\n\n\nShow the code\np4 <- continents %>% \n  filter(continent %in% c(\"Asia\",\"Europe\")) %>% \n  ggplot(aes(x=year, y = total_pop_mil, color = continent)) + \n  geom_line(size = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#70D6DA\"))  +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n\ntriadic + p4 + plot_annotation(title = \"Near Complementary Harmony for Highlighting \\nTwo Series Where One Is the Primary Focus\",theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n\n16.1.3 Color Palettes for Comparing Three Things\n\n16.1.3.1 Analogous/Triadic Harmony for Highlighting Three Series\nAnalogous harmony involves using neighboring colors to the key color for simple distinctions among categories. In contrast, triadic harmony uses the key color and two complementary colors evenly spaced around the color wheel for greater contrast, but may lose the emphasis on the key color. The example below displays the population of three countries without any specific emphasis.\n\n\nShow the code\np5 <- continents %>% \n  filter(continent %in% c(\"Asia\",\"Europe\",\"Americas\")) %>% \n  ggplot(aes(x=year, y = total_pop_mil, color = continent)) + \n  geom_line(size = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#DA70A1\", \"#A970DA\"))  +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n\nanalogous + p5 + plot_annotation(title = \"Analogous/Triadic Harmony for Highlighting Three Series\",theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.1.3.2 Highlighting One Series Against Two Related Series\nNear Complementary Harmony is a color scheme that creates good contrast without using polar opposite colors. It involves selecting a color that is 33% around the color wheel from the key color instead of the full 50%. This works well for highlighting two series where one is the primary focus. It is best to use warm colors for the key color and cool colors for the complementary colors. If necessary, the complementary colors can be muted by decreasing their saturation or altering their lightness to reduce the contrast with the background. The example below highlights the GDPs of 3 countries with emphasis on Asia through the use of the purple color. Europe and the Americas are depicted with similar shades of green, indicating their lesser significance for the narrative.\n\n\nShow the code\np6 <- continents %>% \n  filter(continent %in% c(\"Asia\",\"Europe\",\"Americas\")) %>% \n  ggplot(aes(x=year, y = total_gdppc, color = continent)) + \n  geom_line(size = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c( \"#A1DA70\", \"#DA70D6\", \"#70DAA9\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\ncomplementary_3 + p6 + plot_annotation(title = \"Highlighting One Series Against Two Related Series\",theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n\n16.1.4 Color Palettes for Comparing Four Things\n\n16.1.4.1 Analogous Complementary for One Main Series and Its Three Secondary\nAnalogous Complementary is a color scheme that uses four colors, where the key color and its complementary color are combined with two colors that are one step away from the complementary color. This scheme still allows for analogous harmony while creating a quartet of colors that can be used for one main series and its three components. The similarities between the three complementary colors make the key color stand out. The example below shows Malaysian Economic Miracle in comparison to Malaysia’s three neighbors.\n\n\nShow the code\np7 <- malay_miracle %>% ggplot(aes(x = year, y = gdpPercap, color = country)) + \n  geom_line(linewidth = 1.5) + \n  theme_minimal() + \n  #scale_y_log10() +\n  theme(legend.position = \"none\") +\n  scale_color_manual(values = c(\"#A1DA70\", \"#DA70D6\", \"#70DA74\", \"#70DAA9\")) +\n  geom_dl(aes(label = country), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\ncomplementary_4 + p7 + plot_annotation(title = \"Analogous Complementary for One Main Series and \\nIts Three Components\",theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.1.4.2 Double Complementary for Two Pairs Where One Pair Is Dominant\nDouble Complementary Harmony is a color scheme suitable for four different data series that are divided into two groups of two series. The scheme involves selecting the key color and one of its two adjacent colors on the color wheel. Then, choose the complements of both the key color and its adjacent color to serve as their respective partners. It is recommended that the key color and its adjacent color be warmer colors, while the complementary colors should be cooler colors. This scheme works well for highlighting two pairs where one pair is dominant. Below is an example that compares the GDP of the two top and two bottom countries in 1952. Switzerland and Norway are assigned purple-ish colors, putting them in one group, while Bosnia and Albania are assigned green-blue colors to differentiate them.\n\n\nShow the code\np8 <- gapminder %>% filter((continent == \"Europe\")) %>% mutate(bot_top = case_when(\n  country %in% c(\"Albania\") ~ \"Albania\",\n  country %in% c(\"Bosnia and Herzegovina\") ~ \"Bosnia\",\n  country %in% c(\"Switzerland\") ~ \"Switzerland\",\n  country %in% c(\"Norway\") ~ \"Norway\",\n  T ~ \"other\"\n)) %>% ggplot(aes(x = year, y = gdpPercap, color = bot_top, group = country, alpha = bot_top))+ \n  geom_line(linewidth = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\", legend.title = element_blank()) +\n  scale_y_log10() +\n  geom_dl(aes(label = bot_top), method = \"smart.grid\") +\n  scale_color_manual(values = c(\"#70DA74\", \"#70D6DA\", \"#DA70D6\", \"grey\", \"#DA7470\")) +\n  scale_alpha_manual(values = c(1,1,1,0.3,1)) +\n  labs(#title = \"Growth of the bottom 2 and \\ntop 2 countries by GDP in 1952\",\n       x = element_blank(), y = \"GDP per Capita (log10)\")\n\n\ntetradic + p8 + plot_annotation(title = \"Double Complementary for Two Pairs Where One Pair Is Dominant\",theme = theme(plot.title = element_text(size = 16))) \n\n\n\n\n\n\n\n16.1.4.3 Rectangular or Square Complementary for Four Series of Equal Emphasis\nRectangular or Square Complementary is a color scheme suitable for four series of equal emphasis where the objective is to use colors to make categorical distinctions. This scheme involves selecting the key color and its complementary color, but unlike the double complementary scheme, two additional colors are added to create a rectangle or square on the color wheel. The resulting colors create a clear distinction between the four series. This scheme is similar to double complementary but works better when all four series are of equal importance. The example below displays the Four Asian Tigers, a group of four fast-developing economies in East Asia that experienced high growth rates and rapid industrialization from the 1960s to 1990s. Hong Kong, Singapore, South Korea, and Taiwan are all equally important with the emphasis on the dynamic of their economies.\n\n\nShow the code\np9 <- asian_tigers %>% ggplot(aes(x = year, y = gdpPercap, color = country)) + \n  geom_line(linewidth = 1.5) + \n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#DAA970\", \"#70DA74\", \"#70A1DA\")) +\n  geom_dl(aes(label = country), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\nsquare + p9 + plot_annotation(title = \"Rectangular or Square Complementary for \\nFour Series of Equal Emphasis\",theme = theme(plot.title = element_text(size = 16)))"
  },
  {
    "objectID": "chapters/color.html#sequential-and-divergent",
    "href": "chapters/color.html#sequential-and-divergent",
    "title": "16  Color",
    "section": "16.2 Sequential and Divergent",
    "text": "16.2 Sequential and Divergent\nSequential colors are a gradient of colors from light to dark, assigned to numeric values, based on hue or lightness. The colors depend on the background, with lower values assigned lighter colors, and higher values assigned darker colors. A single hue or a sequence of hues can be used.\nLet’s use our beloved purple to GDP of different countries.\n\n16.2.1 Sequential\n\n\nShow the code\nlibrary(\"sf\")\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\") %>% mutate(gdppc = gdp_md_est) \n\n\n\n\nShow the code\n(sequential | p10) + \n  patchwork::plot_layout(widths = c(1,3)) + \n  plot_annotation(title = \"Sequential\",\n                  theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.2.2 Divergent\nDiverging color schemes are used when the numeric variables have a meaningful central value such as zero. They combine two sequential palettes with a shared endpoint that rests on the central value, with positive values assigned colors on one side and negative values on the other side. The central value should have a light color so that darker colors can indicate more distance from the center. It is important to keep the color scheme simple to avoid diluting the meaning and confusing the audience. Proper use of colors can reduce the cognitive load and help people understand complex information more easily.\n\n\nShow the code\n(divergent | p11) + \n  patchwork::plot_layout(widths = c(1,3)) + \n  plot_annotation(title = \"Divergent\",\n                  theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n\n\n16.2.3 Prebuilt\nPrebuilt color scales like “Viridis” are designed with perceptual uniformity in mind, which makes them visually appealing and easy to interpret. They provide a consistent and standardized color scheme, eliminating the need for custom design and testing. In addition, prebuilt color scales can help people with color blindness to better interpret data visualizations, as they use colors with consistent visual contrast. Using prebuilt color scales can help ensure that data visualizations are accessible to the widest possible audience.\n\n\nShow the code\n(viridis | p12) + \n  patchwork::plot_layout(widths = c(1,3)) + \n  plot_annotation(title = \"Virdis\",\n                  theme = theme(plot.title = element_text(size = 16)))"
  },
  {
    "objectID": "chapters/color.html#color-systems",
    "href": "chapters/color.html#color-systems",
    "title": "16  Color",
    "section": "16.3 Color Systems",
    "text": "16.3 Color Systems\nThere are several popular color systems that are commonly used in digital design and data visualization. The sRGB color system is the standard color space used for displaying images and graphics on digital displays. It is a device-dependent color space that is designed to provide consistent color reproduction across a wide range of devices. The HCV color system is based on hue, chroma, and value, and is used to create visually distinct color palettes for use in data visualization. The HSL color system is based on hue, saturation, and lightness, and is often used to create color palettes for web design and user interfaces. The LAB color system is a device-independent color space that is designed to accurately represent colors across different devices and environments. It is often used in professional printing and color management applications. Each of these color systems has its own strengths and weaknesses, and the choice of which one to use depends on the specific needs of the project.\nTo see how these spaces look, check out this amazing video!\n\n\n16.3.1 HSL\nThe HSL color system describes colors using three parameters: hue, saturation, and lightness. Hue is represented by a value from 0 to 360 degrees on the color wheel, and determines the basic color of the pixel. Saturation represents the purity of the hue, or how much gray is mixed into the color. Saturation ranges from 0% (gray) to 100% (pure hue). Lightness, on the other hand, represents the amount of white or black mixed with the color, with 0% being black, 50% being the original color, and 100% being white. HSL is often used in graphic design and web development, as it allows for the easy selection of colors based on hue, saturation, and lightness. However, it has some limitations, such as not being perceptually uniform, meaning that changes in the numeric values of the parameters may not correspond to equal changes in the perceived color.\n\n\n16.3.2 HSV\nThe HSV color system describes colors using three parameters: hue, saturation, and value. Hue and saturation are the same as in HSL. Value represents the brightness of the pixel, with 0% being black and 100% being the brightest possible color. The HSV color system is often used in graphics and image editing software, as it allows for easy selection of colors. However, it is also not perceptually uniform.\n\n\n16.3.3 HCL (my favorite)\nHCL color is a perceptually uniform color space used in data visualization and scientific applications. It consists of three values: hue, chroma, and lightness, which represent the color, saturation, and brightness of a color, respectively. Due to its ability to mimic human perception of color, HCL color space is becoming increasingly popular in design and user interface applications.\n\n\n16.3.4 LAB\nThe LAB color system is a device-independent color space that is designed to accurately represent colors across different devices and environments. It consists of three parameters: L (lightness), a (the position between red/magenta and green), and b (the position between yellow and blue). The L parameter represents the brightness of the color, ranging from 0 (black) to 100 (white). The a and b parameters represent the color channels, with positive values representing colors in the red, and yellow directions, respectively, and negative values representing colors in green and blue direction. The LAB color space is used in professional printing and color management applications, as it allows for accurate color matching across different devices and environments. Additionally, more recent LAB color spaces (ex. OKLAB) are perceptually uniform, meaning that equal distances in LAB color space correspond to equal steps in perceived color difference.\n\n\n16.3.5 OKLAB\nOKLAB is a color space designed to be more perceptually uniform than other color spaces like sRGB or LAB. It uses an opponent color model, where the color information is encoded L – perceived lightness a – how green/red the color is b – how blue/yellow the color is. This means that the OKLAB color space can accurately represent colors while maintaining perceptual uniformity. OKLAB was developed to address the limitations of other color spaces. The use of OKLAB is becoming increasingly popular in digital design and data visualization, as it can provide more accurate and consistent color representation. You can learn about OKLAB from the video folder.\n\n\n16.3.6 Perceptual Uniformity\nHumans are not machines, we wee see things with our eyes and process them with our brain. What might appear like a similar color to a machine for humans will not. As an example, below are two color wheels one is RGB (perceptually non-uniform) and the other is HCL (uniform). When the color spectra are viewed in Gray scale the uneven nature of RGB becomes apparent. A technical definition is that a perceptual uniform color space ensures that the difference between two colors (as perceived by the human eye) is proportional to the Euclidian distance within the given color space.\n\n\n\nuniform perception\n\n\n\n16.3.6.1 Warning Colormaps might Increase Risk of Death!\nIn 1990s data visualization specialists adopted Rainbow Color Map with the most famous variation being Jet default palette. Many researchers expressed concerns as the non-uniform nature of the palette introduced transitions that could be perceived incorrectly.\nRogowitz and Treinish raised concerns about the Rainbow Color Map in 1998 “Data Visualization: The End of the Rainbow”(Rogowitz and Treinish 1998), and Borland and Taylor highlighted additional concerns in a 2007 paper “Rainbow Color Map (Still) Considered Harmful” (Borland and Taylor Ii 2007). In 2011, Borkin and her team conducted user studies on the application of various color maps, including the Rainbow Color map, to medical visualization problems. Their findings in “Evaluation of Artery Visualizations of Heart Disease Diagnosis” (Borkin et al. 2011) showed that a perceptually uniform color map resulted in fewer diagnostic errors than the Rainbow Color map. So, diagnostic errors could be reduced by simply switching to a proper color palette. The problem of misuse of color still persists as outlined by Crameri, Shephard and Heron in “The misuse of colour in science communication” (2020) (Crameri, Shephard, and Heron 2020) – a paper that I believe must be read by any scientist.\n\nRogowitz, B. E., and L. A. Treinish. 1998. “Data Visualization: The End of the Rainbow.” IEEE Spectrum 35 (12): 52–59. https://doi.org/10.1109/6.736450.\n\nBorland, David, and Russell M. Taylor Ii. 2007. “Rainbow Color Map (Still) Considered Harmful.” IEEE Computer Graphics and Applications 27 (2): 14–17. https://doi.org/10.1109/MCG.2007.323435.\n\nBorkin, M., K. Gajos, A. Peters, D. Mitsouras, S. Melchionna, F. Rybicki, C. Feldman, and H. Pfister. 2011. “Evaluation of Artery Visualizations for Heart Disease Diagnosis.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2479–88. https://doi.org/10.1109/TVCG.2011.192.\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The Misuse of Colour in Science Communication.” Nature Communications 11 (1): 5444. https://doi.org/10.1038/s41467-020-19160-7.\n\n\n\nImage from the “Rainbow Color Map (Still Considered Harmful)\n\n\nAll of these issues are amplified once we consider that roughly 8% of all men and 0.5% of all women are colorblind. There are three main forms of red(protan), gren(deutan), and blue (tritan) disorders, corresponding to color sensitive cones in our eyes. To check whether your visualization is colorblind friendly use Coblis (“Coblis  Color Blindness Simulator  Colblindor,” n.d.).\n\n“Coblis  Color Blindness Simulator  Colblindor.” n.d. https://www.color-blindness.com/coblis-color-blindness-simulator/.\nTo improve the readability of your colors vary their value and hue, but try not to include both red and green into your graphics as red-green colorblindness is the most common.\n\n\n\nColor Blind Rainbow Flower\n\n\n\n\n16.3.6.2 So what should you use?\nA simple and correct answer would be to use a scientific color map that you find pretty and use it as your default. If you need some help use this graph from “The misuse of colour in science communication”.\n\n\n\nchoosing color map\n\n\nIf you want to pick colors yourself use HSL, because it is the most intuitive one and the easiest to make color palettes in. I would also recommend tinkering with OKHSL, which is a child of OKLAB and HSL producing a perceptually uniform HSL space. Try out both of them and see the difference here1.1 https://bottosson.github.io/misc/colorpicker/\n\n\n\n16.3.7 Where do I find color waves?\nAdobe Color - Adobe Color let’s you create color palettes using different color harmony rules and color modes. You can also pick colors from your image, make gradients from images, and test for accessibility.\nPaletton - Amazing tool for creating color palettes\nColor Brewer - Color Brewer is a web-based tool that provides color schemes perceptual uniform for maps and data visualizations.\nColor Thief - Color Thief let’s you extract colors from your image to create nature-inspired palettes.\nViz Palette - Viz Palette can be used to check your color palettes before creating visualizations. It allows you to view color sets in example plots, simulate color deficiencies, and modify the colors of your palette.\nScientific colour maps - A collection of uniform and readable color maps for scientific use."
  },
  {
    "objectID": "chapters/color.html#whats-next",
    "href": "chapters/color.html#whats-next",
    "title": "16  Color",
    "section": "16.4 What’s next?",
    "text": "16.4 What’s next?"
  },
  {
    "objectID": "chapters/which_graph.html#category-comparison",
    "href": "chapters/which_graph.html#category-comparison",
    "title": "17  A Graph for The Job",
    "section": "17.1 Category Comparison",
    "text": "17.1 Category Comparison\nGraphs for category comparison are a type of data visualization that are used to compare and contrast different categories or groups. The most common of them is bar chart! The one below shows the top 5 countries by GDP per Capita in 1997. You can easily see that Norway is first and Switzerland is 5th!\n\n\nShow the code\ngapminder %>% filter(year == 1997) %>% slice_max(gdpPercap, n = 5) %>% ggplot(aes(x = fct_reorder(country,gdpPercap,.desc = T), y = gdpPercap)) + geom_col(fill = \"steelblue\") + theme_minimal(base_size = 18) + labs(x = NULL, y = NULL, title = \"Top 5 countries by GDP per Capita in 1997\") + \ngeom_text(aes(label = round(gdpPercap,0)), vjust = 10, color = \"white\", size = 5) + \ntheme(panel.grid = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\nVertical bar charts are great to provide a quick comparison for a small number of categories (less than 7). But if need to show ranking of more things, flip the axis of the bar chart! Additional bonus, horizontal bar charts are great if you have long names to display. Below are the results of the 2021 London election. British YouTuber Niko Omilana finished 5th for the memes! Max Fosh, another YouTuber, also passed the cut off!\n\n\nShow the code\nbarh_data <- tibble(Candidate = c(\"Sadiq Khan\", \"Shaun Bailey\", \"Siân Berry\", \n\"Luisa Porritt\", \"Niko Omilana\", \"Laurence Fox\", \"Brian Rose\", \n\"Richard Hewison\", \"Count Binface\", \"Mandu Reid\", \"Piers Corbyn\", \n\"Vanessa Hudson\", \"Peter Gammons\", \"Farah London\", \"David Kurten\", \n\"Nims Obunge\", \"Steve Kelleher\", \"Kam Balayev\", \"Max Fosh\", \"Valerie Brown\"\n), \nPercentage = c(40.0, 35.3, 7.8, 4.4, 2.0, 1.9, \n1.2, 1.1, 1.0, 0.8, 0.8, 0.7, 0.6, 0.5, 0.4, \n0.4, 0.3, 0.3, 0.2, 0.2)) %>% mutate(is.youtuber = case_when(\n  Candidate == \"Niko Omilana\" ~ 1,\n  Candidate == \"Max Fosh\" ~ 2,\n  T ~ 0))\n\n\n\n\nShow the code\nbarh_data %>%\nggplot(aes(x = fct_reorder(Candidate, Percentage), y = Percentage, fill = as.factor(is.youtuber))) +\ngeom_col() + \ntheme_minimal(base_size = 16) + \ncoord_flip() + \nlabs(x = NULL, y = NULL, title = \"London Mayor Elections (2021) by % of Votes\") + \ntheme(panel.grid = element_blank(), legend.position = \"none\", plot.caption.position = \"plot\") + \nscale_y_discrete(expand = c(0,0,0,3)) + \ngeom_text(aes(label = Percentage), nudge_y = 0.3, hjust = \"left\")  + \nscale_fill_manual(values = c(\"#c0bfff\",\"#f3e408\",\"#D96161\"))"
  },
  {
    "objectID": "chapters/which_graph.html#distribution",
    "href": "chapters/which_graph.html#distribution",
    "title": "17  A Graph for The Job",
    "section": "17.2 Distribution",
    "text": "17.2 Distribution\n\n17.2.1 Histogram\nWhat if you want to show the distribution of the data? We can use a variation of a bar chart – histogram! Histograms show the distribution of continuous data by grouping it into bins and displaying the frequency or proportion of observations that fall into each bin. They are great if you want to show the shape of the distribution, but they are very sensitive to the bins you choose. Notice how the shape of the distribution changes for each number of bins. It is important to strike a balance between too few and too many. 6 bins makes our distribution look pretty normal while 30 bins make it all over the place. 15 bins seems about right it preserves the bimodal feature of the distribution, while keeping the picture legible.\n\n\nShow the code\nbase <- iris %>% ggplot(aes(x = Sepal.Length))  + theme_minimal() + theme(panel.grid = element_blank(), axis.text.y = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = \"off\") + labs(y = NULL, x = NULL)\nhist_1 <- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 6) + labs(subtitle = \"6 bins\")\nhist_2 <- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 15) + labs(subtitle = \"15 bins\")\nhist_3 <- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 30) + labs(subtitle = \"30 bins\")\n\n(hist_1 + hist_2 + hist_3) + plot_annotation(title = \"iris Sepal Length Distribution Histograms with Varying Bins\")\n\n\n\n\n\n\n\n17.2.2 Density Plot\nUnlike histograms, density plots use a continuous line to represent the data instead of bars. This smooth curve provides a more detailed and nuanced representation of the distribution of the data, allowing for easier detection of patterns and trends. The density plot constructs this line by placing many small normal distributions at each point in the data, which are then used to weigh all points within their respective range and draw a curve connecting them. The width of these curves is controlled by the bandwidth of the density plot, which determines how wide the curves span. A larger bandwidth will consider more points, resulting in a smoother curve, while a smaller bandwidth will lead to a jagged line.\n\n\nShow the code\nbase <- iris %>% ggplot(aes(x = Sepal.Length))  + theme_minimal() + theme(panel.grid = element_blank(), axis.text.y = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = \"off\") + labs(y = NULL, x = NULL)\ndens_1 <- base + geom_density(color = \"steelblue\", linewidth = 2, bw = 0.3) + labs(subtitle = \"Band Width 0.3\")\ndens_2 <- base + geom_density(color = \"steelblue\", linewidth = 2, bw = 0.1) + labs(subtitle = \"Band Width 0.1\")\ndens_3 <- base + geom_density(color = \"steelblue\", linewidth = 1, bw = 0.03) + labs(subtitle = \"Band Width 0.03\")\n\n(dens_1 + dens_2 + dens_3) + plot_annotation(title = \"iris Sepal Length Distribution Density Plots with Varying Band Widths\")\n\n\n\n\n\n\n\n17.2.3 Frequency Polygon\nIt is similar to a histogram, but instead of bars, it uses a continuous line to connect the points representing the frequencies. Frequency polygons are particularly useful when comparing two or more data sets on the same plot. Just like histogram it relies on the selection of bins.\n\n\nShow the code\nfreq_1 <- base + geom_histogram(fill = \"grey\", color = \"white\", bins = 15, alpha = 0.5) + geom_freqpoly(color = \"steelblue\", bins = 15, linewidth = 1.5) + labs(subtitle = \"15 bins\")\n\nfreq_2 <- iris %>% ggplot(aes(x = Sepal.Length))   + \n  geom_histogram(aes(fill = Species), position = \"dodge\", color = \"white\", bins = 15, alpha = 0.3) + \n  geom_freqpoly(aes(color = Species), bins = 15, linewidth = 1.5) + labs(subtitle = \"15 bins\") + \n  theme_minimal() + \n  theme(panel.grid = element_blank(), axis.text.y = element_blank(),\n         legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.box.just = \"right\",\n    legend.margin = margin(6, 6, 6, 6)) + \n  coord_cartesian(expand = FALSE, clip = \"off\") + \n  labs(y = NULL, x = NULL)  \n\nfreq_1 + freq_2\n\n\n\n\n\n\n\n17.2.4 Box Plot\nThis section was inspired by https://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\nBoxplots provide a summary of the distribution of a dataset, show the median, the lower and upper quartiles, and the minimum and maximum values of a dataset. The box in the middle represents the interquartile range (IQR), which is the range of the middle 50% of the data. The line in the box represents the median, which is the midpoint of the data. The whiskers on the top and bottom extend to the minimum and maximum values, excluding outliers. It is incredible how much information boxplots contain! With just one plot, you can quickly identify outliers and gain a visual understanding of the distribution of the data.\nIn the context of the iris dataset, the boxplot of Sepal Length across different species provides a clear picture of the distribution of this variable. However, like real boxes, boxplots can also hide important information. To illustrate this point, we can use a dataset with the same summary statistics but different distributions. In the second graph, three identical boxplots are displayed. However, once we add data points to the plot, it becomes evident that the distributions are quite different.\n\n\nShow the code\nset.seed(1337)\n\ndata_dist <- tibble(\n  group = factor(c(rep(\"Group 1\", 100), rep(\"Group 2\", 250), rep(\"Group 3\", 25))),\n  value = c(seq(0, 20, length.out = 100),\n            c(rep(0, 5), rnorm(30, 2, .1), rnorm(90, 5.4, .1), rnorm(90, 14.6, .1), rnorm(30, 18, .1), rep(20, 5)),\n            rep(seq(0, 20, length.out = 5), 5))\n  ) %>% \n  rowwise() %>%\n  mutate(value = if_else(group == \"Group 2\", value + rnorm(1, 0, .4), value))\n\n\n\n\nShow the code\nbase2 <- iris %>% ggplot(aes(y = Sepal.Length, x = Species))  + theme_minimal() + theme(panel.grid.major.x  = element_blank(),panel.grid.minor.x  = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = \"off\") + labs(y = NULL, x = NULL)\nbox_1 <- base2 + geom_boxplot()\n\nbase_dist <- data_dist %>% ggplot(aes(y = value, x = group))  + theme_minimal() + theme(panel.grid.major.x  = element_blank(),panel.grid.minor.x  = element_blank()) + \n  coord_cartesian(expand = FALSE, clip = \"off\") + labs(y = NULL, x = NULL)\nbox_2 <- base_dist + geom_boxplot()\nbox_3 <- base_dist + geom_boxplot() + geom_point(color = \"orange\", size = 1.5, alpha = 0.25, position = position_jitter(width = 0.1))\n\nbox_1 + box_2 + box_3\n\n\n\n\n\n\n\n17.2.5 Violin Plot\nOne solution is to use violin plots. In its essence it is a vertical density plot. Look how much more we know about out data distribution of iris species! We can see the density distribution, points and quantiles!\n\n\nShow the code\nbase2 + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), bw = 0.15) + geom_jitter(alpha = 0.2, position = position_jitter(width = 0.1))\n\n\n\n\n\nGoing back to our new data set, notice how different the datasets look, clearly there are some patterns. Group 1, which has four times more observations, appears to be nearly identical to Group 3. This is because the default setting “scale =”area”” is a little misleading. We can fix that by changing it to “scale =”count””\n\n\nShow the code\ndist_1 <- base_dist + geom_violin(scale = \"area\") + labs(subtitle = \"scale = 'area'\")\ndist_2 <- base_dist + geom_violin(scale = \"count\") + labs(subtitle = \"scale = 'count'\")\ndist_1 + dist_2\n\n\n\n\n\nDo you remember how band width is extremely important when making density plots? Setting an apprpriate band width reveals the true distribution!\n\n\nShow the code\nbase_dist + geom_violin(scale = \"count\", bw = .3, color = NA, fill = \"steelblue\")\n\n\n\n\n\n\n\n17.2.6 Bee Hive Plot\nThe bee hive plot is a scatter plot that arranges data points as dots to minimize overlap. It’s ideal for visualizing small datasets because it creates patterns like a density plot without hiding individual data points.\n\n\nShow the code\nbee_1 <- base2 + geom_beeswarm() \nbee_2 <- base_dist + geom_beeswarm()\nbee_1 + bee_2\n\n\n\n\n\nAll of the plots we have covered so far have their advantages: 1. Box plot shows important statistics 2. Density plot provides high-level view of data shape 3. Bee hive plot “shows” the actual datapoints While combining these plots might make for a crowded visual, with some modifications, it’s possible to create a hybrid plot that captures the strengths of each.\n\n\nShow the code\nbase_dist + geom_violin(fill = \"steelblue\", alpha = .4, scale = \"count\", bw = .4) + geom_boxplot(fill = NA, width = .1) + geom_beeswarm(color = \"orange\", alpha = .8, cex = 1) \n\n\n\n\n\n\n\n17.2.7 Rain Cloud Plot\nRain Cloud Plot combines elements of box plots, violin plots, and density plots. It uses a density plot to show the distribution of the data, a box plot to display the statistics, and individual data points are represented as rain drops. The result is a visually appealing and informative way to visualize a large number of distributions side-by-side, allowing for easy comparisons and identification of patterns.\nIsn’t this beautiful? We have a box plot, density plot, and jittered points all in the same graph without looking cluttered.\n\n\nShow the code\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3, #bw\n    width = .6, \n    .width = 0, \n    justification = -.2, \n    point_colour = NA\n  ) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  ## add justified jitter from the {gghalves} package\n  gghalves::geom_half_point(\n    ## draw jitter on the left\n    side = \"l\", \n    ## control range of jitter\n    range_scale = .4, \n    ## add some transparency\n    alpha = .3\n  ) +\n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") \n\n\n\n\n\nOne alternative option to the boxplot is to stack the data points and use a minimal boxplot representation. While this alternative can be visually appealing, it is important to ensure that your audience understands the visualization and the meaning behind the stacked data points. It may be necessary to provide additional context or include a note explaining the meaning of the stacked slabs to avoid confusion.\n\n\nShow the code\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3,\n    width = .6, \n    ## set slab interval to show IQR and 95% data range\n    .width = c(.5, .95)\n  ) + \n  ggdist::stat_dots(\n    side = \"left\", \n    dotsize = .8, \n    justification = 1.05, \n    binwidth = .3\n  ) +\n  coord_cartesian(xlim = c(1.2, NA)) \n\n\n\n\n\nMy personal favorite is the rain cloud plot, which combines vertical lines and a bar plot that is rotated horizontally to resemble actual rain clouds.\n\n\nShow the code\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3, \n    width = .6, \n    .width = 0, \n    justification = -.2, \n    point_colour = NA\n  ) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  geom_half_point(\n    ## draw horizontal lines instead of points\n    shape = \"|\",\n    side = \"l\",\n    size = 5,\n    alpha = .2,\n    transformation = position_identity()\n  ) +\n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\")+ coord_flip()"
  },
  {
    "objectID": "chapters/which_graph.html#proportions",
    "href": "chapters/which_graph.html#proportions",
    "title": "17  A Graph for The Job",
    "section": "17.3 Proportions",
    "text": "17.3 Proportions\nAnother big collection of graphs is concerned with communicating proportions and composition.\n\n17.3.1 Stacked Bar Charts\n\n\nShow the code\nus_spending <- read_csv(\"data/USFR_StmtNetCost_2017_2022.csv\") %>% janitor::clean_names() %>% filter((restatement_flag == \"N\") & (agency_name != \"Total\")) %>% select(year = statement_fiscal_year, agency_name, net_cost_in_billions) %>% mutate(net_cost_in_billions = as.numeric(net_cost_in_billions)) %>% group_by(year) %>% mutate(proportion = round(net_cost_in_billions/sum(net_cost_in_billions),2)) %>% ungroup()\n\nspending_plot_data <- us_spending %>% group_by(year) %>% mutate(rank = rank(-1*net_cost_in_billions), agency = ifelse(rank>=5, \"Other\", agency_name)) %>% count(year,agency,wt = net_cost_in_billions) %>%\n  mutate(other = agency == \"Other\") %>%\n  group_by(other) %>%\n  arrange(desc(n),.by_group = T) %>%\n  ungroup() %>%\n  mutate(order = -1*row_number()) %>%\n  mutate(agency = recode( agency,\n\"Department of Veterans Affairs\" = \"Veterans Affairs\",\n\"Department of Health and Human Services\" = \"HHS\",\n\"Department of Defense\" = \"Defense\",\n\"Social Security Administration\" = \"SSA\",\n\"Department of the Treasury\" = \"Treasury\",\n\"Interest on Treasury Securities Held by the Public\" = \"i on Treasuries\"\n)) %>% mutate(agency = factor(agency,c(\"Other\", \"i on Treasuries\", \"Veterans Affairs\", \"Defense\", \"Treasury\", \"SSA\", \"HHS\")))\n\n\nA stacked bar chart is a type of graph used to visualize the distribution of a categorical variable. It is similar to a regular bar chart, but in a stacked bar chart, each bar is divided into sections, with each section representing a different category within the variable. The height of each section corresponds to the proportion or frequency of the category within that bar. Stacked bar charts are particularly useful when comparing the distribution of a variable across different subgroups or time periods, as they allow for easy visualization of both the overall distribution as well as the relative proportions of each subgroup or category within the variable.\nAs an example we will use US Expernditures across departments. Only top four departments are shown, the rest are collected into “other”. The graph below shows absolute values and its components across years.\n\n\nShow the code\nspending_plot_data %>% \n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) + \n  geom_col(position=\"stack\", show.legend = F) +\n  scale_fill_manual(values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")) + theme_minimal() + theme(legend.position = \"right\") + labs(y = \"Millions Spent\") + \n  geom_label(size = 3, aes(group = agency), position = position_stack(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nWhat if we are not concerned with absolute values, but relative proportions? We can use percentage stacked chart.\n\n\nShow the code\nspending_plot_data %>% \n  ggplot(aes(x = year, y = n, fill = agency)) + \n  geom_col(position=\"fill\", show.legend = T) +\n  scale_fill_manual( \n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")) +\n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(aes(x = year, y = n, label = agency, group = agency), size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nStacked charts are useful for visualizing the distribution of categorical variables, but they can be challenging to compare categories in the middle. Typically, the easiest categories to compare are the ones at the top and bottom of the stack. For example, suppose we want to compare the trend of the Department of Defense and the Social Security Administration (SSA) over time. In this case, we can move these categories to the top and bottom positions of the stacked chart to make it easier to compare their relative sizes and trends.\n\n\nShow the code\nspending_plot_data %>% mutate(agency = factor(agency,c(\"Defense\", \"Other\", \"i on Treasuries\", \"Veterans Affairs\", \"Treasury\", \"HHS\", \"SSA\"))) %>% \n  ggplot(aes(x = year, y = n, fill = agency)) + \n  geom_col(position=\"fill\", show.legend = T) +\n  scale_fill_manual( \n    values = c(\"#006837\", \"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\",  \"#F7DC6F\", \"#FFC0CB\", \"#00FFFF\")) +\n  theme_minimal() + \n  theme(legend.position = \"none\") + \n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(aes(x = year, y = n, label = agency, group = agency), size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\n\n\n17.3.2 Pie Chart\ndata used from: The Growth Lab at Harvard University. The Atlas of Economic Complexity. http://www.atlas.cid.harvard.edu.\nAs an example dataset we will be used Japan’s export basket from 2020.\n\n\nShow the code\njapan_export <- read_csv(\"data/japan_export_2020.csv\") %>% rename(\"export\" = `Gross Export`) %>% janitor::clean_names()\n\njapan_sectors <- japan_export %>% count(sector, wt = share) \n\n\nPie charts are a variation of bar charts where each category is represented as a slice of a circle. While pie charts can effectively communicate when one category is significantly larger or smaller than the others, they become difficult to read and compare accurately when there are many categories or the differences between them are small. Comparing angles and areas of the slices can be confusing, leading to misinterpretation of the data.\n\n\nShow the code\npie_1 <- japan_sectors %>%\n  ggplot(aes(x=\"\", y=n, fill = sector)) +\n  geom_bar(stat=\"identity\", width=1) +\n    theme_void()  +\n  theme(legend.position = \"none\")\n\npie_2 <- japan_sectors %>%\n  ggplot(aes(x=\"\", y=n, fill = sector)) +\n  geom_bar(stat=\"identity\", width=1) +\n  coord_polar(\"y\", start=0) +\n  theme(panel.background = element_rect(fill = \"white\"))\n\npie_1 + pie_2 \n\n\n\n\n\nBut if absolutely must use a pie chart here are some rules to keep in mind: 1. Limit the number of categories to 5-7 at most. 2. Consider grouping small categories into an “Other” category to avoid clutter. 3. Arrange the slices in decreasing order of size, starting at 12 o’clock to aid in comparing them. 4. Include the category labels directly on the chart instead of relying solely on a legend. 5. Add separators between slices to help with distinguishing between them. However, keep in mind that this can also add visual clutter, so use with discretion.\n\n\nShow the code\njapan_sectors %>% \n  mutate(sector = ifelse(n<11,\"Other\",sector)) %>% \n  count(sector, wt = n) %>%\n  mutate(other = sector == \"Other\") %>%\n  group_by(other) %>%\n  arrange(desc(n),.by_group = T) %>%\n  ungroup() %>%\n  mutate(prop = n / sum(japan_sectors$n) * 100) %>%\n  mutate(ypos = cumsum(prop)- 0.5*prop ) %>%\n  mutate(order = -1*row_number()) %>%\nggplot(aes(x=\"\", y=n, fill= fct_reorder(sector, order))) +\n  geom_bar(stat=\"identity\", width=1, color = \n             \"white\") +\n  coord_polar(\"y\", start=0) +\n    theme_void() + \n  theme(legend.position=\"none\") +\n  geom_text(aes(y = ypos, label = sector), color = c(\"white\",\"#333333\", rep(\"white\",4)), size=6) +\n  scale_fill_manual(values = c(\"grey\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\"))\n\n\n\n\n\n\n\n17.3.3 Waffle Chart\nOne alternative to a pie chart could be a waffle chart (these food names make me hungry). It is a grid-like visualization that resembles a waffle or a checkerboard. Each square in the grid represents a proportion of the total data, making it a useful way to visualize proportions or percentages in a visually appealing way. However, they are also vulnerable to large numbers of categories. But what they are truly great at is giving the sense of proportions and sizes. Waffle chart will significantly benefit from interactivity.\n\n\nShow the code\n# waffle_data <- waffle_iron(iris, aes_d(group = Species))\n# \n# ggplot(waffle_data, aes(x, y, fill = group)) + \n#   geom_waffle() + \n#   coord_equal() + \n#   scale_fill_viridis_d() + \n#   theme_waffle() +\n#   theme(legend.position = \"top\", legend.title = element_blank()) +\n#   labs(x = element_blank(), y = element_blank())\n\njapan_sectors %>% \n    # mutate(sector = ifelse(n<11,\"Other\",sector)) %>% \n    # count(sector, wt = n) %>%\n    # mutate(other = sector == \"Other\") %>%\n    # group_by(other) %>%\n    # arrange(desc(n),.by_group = T) %>%\n    # ungroup() %>%\n    mutate(other = sector == \"Other\") %>%\n    uncount(weights = round(n,0)) %>% \n    group_by(other) %>%\n    arrange(desc(n),.by_group = T) %>%\n    ungroup() %>%\n    waffle_iron(aes_d(group = sector)) %>%\n    ggplot( aes(x, y, fill = group))+ geom_waffle() + coord_equal() + \n    # scale_fill_viridis_d() + \n    theme_waffle() +\n    theme(legend.position = \"top\", legend.title = element_blank()) +\n    labs(x = element_blank(), y = element_blank()) +\n    scale_fill_manual(values = c(\"#CF8F00\", \"#E52B50\", \"#003366\", \"#228B22\", \"#1E90FF\", \"#FFD700\", \"#666666\", \"#800000\", \"#9932CC\", \"#8B0000\", \"#FFA07A\"))\n\n\n\n\n\n\n\n17.3.4 Tree Maps\nWhat if we have a lot of data hierarchical data? Treemaps!\nTreemaps are a type of visualization that allows you to display hierarchical data in a way that is easy to understand. Each node in the hierarchy is represented by a rectangle, and the size of the rectangle corresponds to the proportion of the total data. The nodes are organized in a way that preserves the hierarchy, with parent nodes containing smaller child nodes. This allows you to quickly identify which nodes are the largest and which are the smallest, as well as the relationships between them. Tree maps are especially useful for displaying large amounts of data in a compact and intuitive way. Tree maps can become very cluttered and interactivity is almost always necessary for such detailed plots. Check out the same plot from the source website.\n\n\nShow the code\nlibrary(treemap)\ntreemap(japan_export,\n        index = c(\"sector\",\"name\"),\n        vSize = \"export\",\n        type = \"index\",\n        fontsize.labels = c(14,10),\n        fontcolor.labels = c(\"black\",\"white\"),\n        fontface.labels = c(2,1),\n        bg.labels = 0,\n        align.labels = list(\n          c(\"center\",\"center\"),\n          c(\"left\",\"top\")\n        ),\n        border.col = c(\"black\",\"white\"),\n        border.lwds = c(3,1),\n        title = \"Japans Export in 2020\",\n        fontsize.title = 14)"
  },
  {
    "objectID": "chapters/which_graph.html#correlation",
    "href": "chapters/which_graph.html#correlation",
    "title": "17  A Graph for The Job",
    "section": "17.4 Correlation",
    "text": "17.4 Correlation\nIn addition to understanding the distribution of individual variables, it is important to examine the relationship between pairs of variables. Correlation plots are a useful tool for visualizing many aspects of data: relationships between variables (or lack there of), clustering, outliers, etc.\n\n17.4.1 Scatter Plot\nThe most common visualization is scatter plot! It is not a secret for anyone that scatter plots are amazing and perhaps the most persuasive types of plot. We can add a fitted lines to the plot to better show the relationships between the variables.\n\n\nShow the code\nscatter_1 <- iris %>% drop_na() %>% ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n  geom_point(show.legend = F) + \n  geom_dl(aes(label = Species), method = \"smart.grid\") + \n  theme_minimal() + \n  labs(x = \"Sepal Length\", y = \"Sepal Width\")\n\nscatter_2 <- scatter_1 + geom_smooth(se = F, fullrange = F, show.legend = F, method = \"lm\", linewidth = 2) + theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nscatter_1 + scatter_2"
  },
  {
    "objectID": "chapters/which_graph.html#change-over-time",
    "href": "chapters/which_graph.html#change-over-time",
    "title": "17  A Graph for The Job",
    "section": "17.5 Change over Time",
    "text": "17.5 Change over Time\nWe have already seen plots that incorporate time change. Time series plots typically have time on the x-axis and the variable being measured on the y-axis. They can show trends, patterns, and seasonal fluctuations in the data.\n\n17.5.1 Line Chart\nMost common\nS&P 500 stock market index since 1927. Historical data is inflation-adjusted using the headline CPI and each data point represents the month-end closing value.\n\n\nShow the code\nsp500 <- tribble(\n  ~Year, ~Average_Closing_Price, ~Year_Open, ~Year_High, ~Year_Low, ~Year_Close, ~Annual_Percent_Change,\n  2023, 4020.94, 3824.14, 4179.76, 3808.10, 3970.04, 3.40,\n  2022, 4097.49, 4796.56, 4796.56, 3577.03, 3839.50, -19.44,\n  2021, 4273.41, 3700.65, 4793.06, 3700.65, 4766.18, 26.89,\n  2020, 3217.86, 3257.85, 3756.07, 2237.40, 3756.07, 16.26,\n  2019, 2913.36, 2510.03, 3240.02, 2447.89, 3230.78, 28.88,\n  2018, 2746.21, 2695.81, 2930.75, 2351.10, 2506.85, -6.24,\n  2017, 2449.08, 2257.83, 2690.16, 2257.83, 2673.61, 19.42,\n  2016, 2094.65, 2012.66, 2271.72, 1829.08, 2238.83, 9.54,\n  2015, 2061.07, 2058.20, 2130.82, 1867.61, 2043.94, -0.73,\n  2014, 1931.38, 1831.98, 2090.57, 1741.89, 2058.90, 11.39,\n  2013, 1643.80, 1462.42, 1848.36, 1457.15, 1848.36, 29.60,\n  2012, 1379.61, 1277.06, 1465.77, 1277.06, 1426.19, 13.41,\n  2011, 1267.64, 1271.87, 1363.61, 1099.23, 1257.60, 0.00,\n  2010, 1139.97, 1132.99, 1259.78, 1022.58, 1257.64, 12.78,\n  2009, 948.05, 931.80, 1127.78, 676.53, 1115.10, 23.45,\n  2008, 1220.04, 1447.16, 1447.16, 752.44, 903.25, -38.49,\n  2007, 1477.18, 1416.60, 1565.15, 1374.12, 1468.36, 3.53,\n  2006, 1310.46, 1268.80, 1427.09, 1223.69, 1418.30, 13.62)\n\n\n\n\nShow the code\nsp500_scatter <- sp500 %>%\nselect(-c(Average_Closing_Price,Year_Open,Annual_Percent_Change)) %>% pivot_longer(-Year) %>% mutate(year_close = name != \"Year_Close\") %>%\nggplot(aes(x = Year, y = value, color = name)) + \n  geom_point(size = 2) + \n  geom_dl(aes(label = name), method = \"smart.grid\") +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  labs(x = element_blank(), y = \"S&P 500\")\n\nsp500_line <- sp500 %>%\nselect(-c(Average_Closing_Price,Year_Open,Annual_Percent_Change)) %>% pivot_longer(-Year) %>% mutate(year_close = name != \"Year_Close\") %>%\nggplot(aes(x = Year, y = value, color = name)) + \n  geom_line(aes(linetype = year_close),linewidth = 1.5) + \n  scale_color_manual(values = c(\"steelblue\", \"grey\", \"grey\")) +\n  geom_dl(aes(label = name), method = \"smart.grid\") +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  labs(x = element_blank(), y = \"S&P 500\")\n\nsp500_scatter + sp500_line"
  },
  {
    "objectID": "chapters/which_graph.html#waterfall-graph",
    "href": "chapters/which_graph.html#waterfall-graph",
    "title": "17  A Graph for The Job",
    "section": "17.6 Waterfall Graph",
    "text": "17.6 Waterfall Graph\nWaterfall charts, also known as bridge charts, are a type of bar chart used to visualize the cumulative effect of sequentially introduced positive or negative values. The graph is named “waterfall” because it resembles a series of falling water droplets. Each bar in the chart represents a value and is color-coded to indicate whether it contributes to an increase or decrease in the cumulative total. They are useful for visualizing the relative contributions of positive and negative factors that affect the net change in the value being analyzed.\n\n\nShow the code\nwaterfall_data <- tribble(\n  ~year, ~bank, ~change,\n  2017, 2000, 2000,\n  2018, 1745, -255,\n  2019, 1930, 185,\n  2020, 2197, 267,\n  2021, 2453, 256,  \n  2022, 2300, -153,\n) %>% transmute(as.character(year), change)\nlibrary(waterfalls)\n\n\nwaterfall(waterfall_data,  calc_total = TRUE,\n          total_rect_color = \"orange\",\n          total_rect_text_color = \"white\") + \n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  labs(y = \"Money in Bank\", x = NULL)"
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Abadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge.\n2017. “When Should You Adjust Standard Errors for\nClustering?” Cambridge, MA. https://doi.org/10.3386/w24003.\n\n\nBai, Jushan. 2009. “Panel Data Models with Interactive Fixed\nEffects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ecta6135.\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nBorkin, M., K. Gajos, A. Peters, D. Mitsouras, S. Melchionna, F.\nRybicki, C. Feldman, and H. Pfister. 2011. “Evaluation of Artery\nVisualizations for Heart Disease Diagnosis.” IEEE\nTransactions on Visualization and Computer Graphics 17 (12):\n2479–88. https://doi.org/10.1109/TVCG.2011.192.\n\n\nBorland, David, and Russell M. Taylor Ii. 2007. “Rainbow Color Map\n(Still) Considered Harmful.” IEEE Computer Graphics and\nApplications 27 (2): 14–17. https://doi.org/10.1109/MCG.2007.323435.\n\n\n“Coblis  Color Blindness Simulator \nColblindor.” n.d. https://www.color-blindness.com/coblis-color-blindness-simulator/.\n\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The\nMisuse of Colour in Science Communication.” Nature\nCommunications 11 (1): 5444. https://doi.org/10.1038/s41467-020-19160-7.\n\n\nHadley, Wickham. 2016. Ggplot2. New York, NY: Springer\nScience+Business Media, LLC.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nPopper, K. R. 2002. The Logic of Scientific Discovery. ISSR\nLibrary. Routledge. https://books.google.com/books?id=Yq6xeupNStMC.\n\n\nRogowitz, B. E., and L. A. Treinish. 1998. “Data Visualization:\nThe End of the Rainbow.” IEEE Spectrum 35 (12): 52–59.\nhttps://doi.org/10.1109/6.736450.\n\n\nThe Grammar of Graphics. 2005. Statistics and Computing. New\nYork: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n\n\nWickham, Hadley. 2014. “Tidy Data.”\nJournal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWilson, Seth. 2011. “Inkfumes: Poster Designs: Color, Design,\nTypography Theory.” http://inkfumes.blogspot.com/2011/10/poster-designs-color-design-typography.html."
  }
]