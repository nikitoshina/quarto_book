---
title: Imputation
message: false
warning: false
---

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
  options(qurto.max_rows = 5)
```

Imputation is a statistical technique that fills in missing or incomplete data by estimating values from existing information, but its controversial nature arises from the introduction of artificial data, potentially impacting analysis outcomes as completed data is assigned more weight.

## Types of Missing Data

Understanding the type of missing data is crucial as it can guide the appropriate choice of imputation methods or handling strategies to minimize potential biases in the analysis.

1.  **Missing Completely at Random (MCAR)**: The missingness in the data is entirely random, and the probability of missing data is the same for all observations, regardless of observed or unobserved values. MCAR is an ideal scenario, as missing data does not depend on any other variables in the dataset.

2.  **Missing at Random (MAR)**: As mentioned earlier, MAR occurs when the missingness is related to observed variables in the dataset, but not to the unobserved values themselves. The probability of missing data depends on other observed variables but not on the unobserved values.

3.  **Missing Not at Random (MNAR)**: In MNAR, the missingness is related to the unobserved values themselves. The probability of missing data depends on the values that are missing, leading to potential bias in the analysis.

4.  **Missing by Design**: Missing by design occurs when specific data points are intentionally missing, often as part of the study design or data collection process. In such cases, the missing data is systematic and has a purpose.

Before addressing missing data, it is often essential to distinguish between different types of missingness. Explicitly marking missing data as NA in R helps in identifying the patterns and dealing with the missing values appropriately.

## Dealing with Missing Data

There are several ways to handle missing data:

1.  **Keep it**: In some cases, it might be appropriate to retain the missing data if the missingness does not significantly affect the analysis.

2.  **Drop it (Listwise Deletion or Complete Case Analysis)**: This approach involves removing rows with missing data. It's simple and can be suitable if missing data is minimal and random. However, it can lead to information loss, potential bias, and reduced sample size. Despite these drawbacks, it's a common method in quantitative research due to its simplicity.

3.  **Impute it**: Imputation involves estimating missing values based on observed data, which allows for a complete dataset and ensures that all cases are retained for analysis. However, if not done correctly will introduce bias.

4.  **Set as Dummy or a Factor**: Sometimes, missing values can be treated as a separate category. This can be done by creating a dummy variable or converting the variable into a factor. This approach acknowledges the missing data and incorporates it into the analysis as a distinct group.

Imputation, while a powerful tool, must be applied judiciously as it modifies the original dataset and can substantially influence analysis outcomes. It's essential to comprehend the reasons behind the missing data, the assumptions involved, and the justification for using imputation in the specific context of your analysis. Imputation should never be a default choice, but a well-considered strategy.

### Explicitly Handling Missing Data with `complete()`

When working with datasets, it's crucial to understand that missing values aren't always explicitly identifiable. These implicit absences can misleadingly suggest data completeness, emphasizing the necessity for appropriate identification and handling. The `tidyr` package's `complete()` function offers a robust solution.

The `complete()` function generates a new dataframe featuring all potential combinations of specified columns, thereby assuring data comprehensiveness. This process, complemented by filling absent combinations with default values, facilitates precise analysis and mitigates the risk of implicit missing data.

Take, for instance, a dataset tracking four students attending various classes over three days. Initially, this dataset might seem comprehensive. However, only the attending students were recorded, leaving a data gap for absentees.

In this case, we employ `complete()` to create `complete_df`, a new dataframe that encapsulates all conceivable combinations of `student_id`, `day`, and `class.` This method ensures accurate recording of each student's attendance for every class on each day, irrespective of the initial data's shortcomings. The "present" column's missing values default to `FALSE`, clearly denoting unrecorded attendance.

```{r}
#| code-fold: true
df <- tribble( # <1> 
  ~day, ~class, ~student_id, ~present,
  1, "English", 1, T,
  1, "English", 2, T,
  1, "English", 4, T,
  1, "Science", 2, T,
  2, "Math", 1, T,
  2, "Math", 2, T,
  2, "Math", 4, T,
  2, "English", 4, T,
  2, "English", 1, T,
  3, "Math", 1, T,
  3, "Math", 2, T,
  3, "Math", 1, T
)
```

1.  `tribble()` is a function to create a data frame in a readable format

```{r}
#| classes: output
#| echo: false
df
```

```{r}
complete_df <- df %>% # <1> 
  complete(
    student_id = full_seq(student_id, 1), # <2> 
    nesting(day, class), # <3> 
    fill = list(present = FALSE), # <4> 
    explicit = FALSE # <5> 
  ) %>%
  arrange(day, class, student_id, present) # <6> 
```

1.  Creating a complete dataframe with all possible combinations of `student_id` and nested combinations of `day` and `class`
2.  `full_seq()` generates a sequence of student IDs, using the number of unique student IDs as the maximum value
3.  `nesting()` creates only combinations that already exist in the data
4.  Fills missing attendance data with `FALSE`
5.  Limit the `fill` to only the newly created (i.e. previously implicit)
6.  Orders the resulting data frame by `day`, `class`, `student_id`, and `present`

```{r}
#| classes: output
#| echo: false
print(complete_df, n =100)
```

By using `nesting()` within `complete()`, we explicitly handle missing data, creating a comprehensive nested dataset suitable for further analysis. This approach guarantees that our analysis is based on a more complete and reliable dataset, providing accurate insights into student attendance across different classes and days.

### Simple Imputations

To explore different imputations we will use "airquality" dataset that contains daily measurements of air pollutants and weather conditions in New York City during a five-month period in 1973. It includes data on ozone concentration, solar radiation, temperature, wind speed, and relative humidity.

```{r}
head(airquality)
airquality <- drop_na(airquality, Solar.R)
invisible(mice::md.pattern(airquality)) # <1>
head_na <- function(data, n = 5) {
  data %>%
    filter(is.na(Ozone)) %>%
    select(-Ozone) %>%
    head(n)
}
```

1.  `md.pattern` makes a matrix of missing values. `invisible` to only show plot, without matrix output.

#### Fixed Value Imputation

Missing values are replaced with a predetermined constant value. This method is simple and useful when you believe the fixed value reasonably represents the missing data.

#### Mean and Median Imputation

Missing values are replaced with the mean or median of the non-missing data in the same column. Using the mean is effective when data follow a normal distribution, but it can be affected by outliers, leading to biased imputations. In contrast, using the median is more robust to outliers and suitable for skewed or extreme data.

#### Fill

In some scenarios, it's rational to replace missing data with either preceding or succeeding values. This approach is particularly effective with datasets where the next available value logically substitutes the missing ones, such as in time-series or ordered data.

```{r}
airquality %>%
  arrange(Month, Day) %>%
  mutate(
    imp_fixed = replace(Ozone, is.na(Ozone), 0),  # <1> 
    imp_mean = replace(Ozone, is.na(Ozone), round(mean(Ozone, na.rm = TRUE), 2)), # <2> 
    imp_median = replace(Ozone, is.na(Ozone), median(Ozone, na.rm = TRUE)), # <3> 
    imp_fill = Ozone,
    .keep = "used"  # <4> 
  ) %>%
  fill(imp_fill, .direction = "down") %>% # <5>
  head_na()
```

1.  Impute with a fixed value (0 in this case).
2.  Impute with the mean.
3.  Impute with the median.
4.  Keep only "used" columns.
5.  Fills missing values with the most recent non-missing value above.

Remember that the choice of imputation method can significantly impact the results of your analysis. Always consider the nature of your data, the distribution of missingness, and the potential implications of each method before making a decision.

#### K-Nearest Neighbors (KNN)

```{r}
# KNN imputation using the 'VIM' package
library(VIM)
airquality_knn <- kNN(airquality)
head_na(airquality_knn)
```

KNN imputation can be a good choice when the data have a complex structure that simple methods can't capture. It uses the relationships between variables to estimate missing values. However, it can be computationally intensive for large datasets.

#### Maximum Likelihood

```{r}
# Maximum Likelihood using the 'norm' package
library(norm)
s <- prelim.norm(as.matrix(airquality))   #do preliminary manipulations
thetahat <- em.norm(s, showits = FALSE)   #find the mle
rngseed(1337)   #set random number generator seed
airquality_mlh <- imp.norm(s,thetahat,airquality)
head_na(airquality_mlh)
```

Maximum likelihood estimation can be a powerful method for imputing missing data, especially when the data are normally distributed. However, it makes strong assumptions about the data and can be complex to implement.

#### Regression

```{r}
# Fit a linear regression model
model <- lm(Ozone ~ ., data = airquality)

# Predict missing values
predicted_values <- predict(model, newdata = airquality)

# Replace missing values with predicted values
airquality %>% 
  mutate(
    Ozone = Ozone,
    imp_lm = ifelse(is.na(Ozone), predicted_values, Ozone),
    .keep = "used"
  ) %>% 
  head_na()
```

Regression imputation can be useful when there are relationships between variables that can be captured by a regression model. However, it can lead to underestimated variance and overestimated model fit.

#### Forest

```{r}
# Install and load the missForest package
library(missForest)

airquality %>%
  mutate(
    Ozone = Ozone,
    imp_forest = missForest(.)$ximp$Ozone,
    .keep = "used"
  ) %>%
  head_na()
```

Tree-based methods like Random Forests can handle complex data structures and can be a good choice when relationships between variables are non-linear or involve interactions. However, they can be computationally intensive and may not perform well with small sample sizes or sparse data.

Remember, each of these methods has its own strengths and weaknesses, and the choice of method should be guided by the nature of your data and the specific requirements of your analysis. Always check the assumptions of the imputation method you're using and consider the potential impact on your results.

### Multiple Imputations

Now, listen closely, imputing can be better then dropping the data! Wait what? Yeh, because instead of dropping the data you preserve it! However, there is a method to this.

[@jakobsenWhenHowShould2017]

Multiple imputation is a statistical technique that has been increasingly utilized since its inception in the early 1970s. It is a simulation-based method designed to address the issue of missing data in research studies. The process of multiple imputation is carried out in three main steps:

1.  **Imputation Step**: In this initial stage, missing values in the dataset are identified and replaced with a set of plausible values, creating multiple completed datasets. These plausible values, or 'imputations', are generated based on a chosen imputation model. To reduce sampling variability from the imputation process, it is often preferable to generate 50 datasets or more.

2.  **Completed-Data Analysis (Estimation) Step**: Once the imputed datasets are created, the desired analysis is performed separately on each dataset. For instance, if 50 datasets were generated during the imputation step, 50 separate analyses would be conducted.

3.  **Pooling Step**: The results obtained from each completed-data analysis are then combined into a single multiple-imputation result. Each analysis result is considered to have the same statistical weight, so there is no need for a weighted meta-analysis.

It is crucial to ensure compatibility between the imputation model and the analysis model, or that the imputation model is more general than the analysis model. This means that the imputation model should include more independent covariates than the analysis model. For instance, if the analysis model includes significant interactions, then the imputation model should include them as well. Similarly, if the analysis model uses a transformed version of a variable, then the imputation model should use the same transformation.

![](images/imputation_flow.webp)

#### MICE (Multivariate Imputation by Chained Equations)

The `mice` package in R is a powerful tool for handling missing data through multiple imputation. It uses the Multivariate Imputation by Chained Equations (MICE) algorithm, which creates multiple imputations (replacement values) for multivariate missing data. The package creates multiple imputations by applying specified imputation methods to each missing value in an iterative process, also known as 'chained equations'.

Here's a basic example of how to use the `mice` package:

First, install and load the `mice` package:

```{r}
library(mice)
```

:::

TODO: write about doing simple imputations

MICE stands for *Multivariate Imputation via Chained Equations*, and it's one of the most common packages for R users. It assumes the missing values are missing at random (MAR).

The basic idea behind the algorithm is to treat each variable that has missing values as a dependent variable in regression and treat the others as independent (predictors). You can learn more about MICE in [this paper](https://www.researchgate.net/publication/44203418_MICE_Multivariate_Imputation_by_Chained_Equations_in_R).

The R `mice` packages provide many [univariate imputation methods](https://www.rdocumentation.org/packages/mice/versions/3.14.0/topics/mice), but we'll use only a handful. First, let's import the package and subset only the numerical columns to keep things simple. Only the `Age` attribute contains missing values:

Onto the imputation now. We'll use the following MICE imputation methods:

-   **pmm**: Predictive mean matching.

-   **cart**: Classification and regression trees.

-   **laso.norm**: Lasso linear regression.

:::

```{r}
airquality %>% 
  mutate(
  Ozone = Ozone,
  imp_pmm = complete(mice(., method = "pmm", printFlag = FALSE))$Ozone,
  imp_cart = complete(mice(., method = "cart", printFlag = FALSE))$Ozone,
  imp_lasso = complete(mice(., method = "lasso.norm", printFlag = FALSE))$Ozone,
  .keep = "used"
) %>%
  head_na()
```

Assuming you have a dataset `data` with missing values, you can use `mice()` function to perform multiple imputation:

```{r}
# Perform multiple imputation
imp <- mice(airquality, m = 50, method = "pmm", seed = 1337, printFlag = FALSE)

# m=5 specifies the number of multiple imputations (i.e., the number of complete datasets to generate)
# method='pmm' specifies predictive mean matching method for imputation
# seed=500 for reproducibility
```

The `mice` function supports several imputation methods. The choice of method depends on the nature of variables. Here are a few commonly used methods:

-   `'pmm'`: Predictive mean matching. Useful for numeric data.
-   `'logreg'`: Logistic regression. Useful for binary data.
-   `'polyreg'`: Polytomous logistic regression. Useful for ordered categorical data.
-   `'polr'`: Proportional odds model. Useful for ordered categorical data.

After imputation, you can analyze each dataset separately. For example, if you want to fit a linear regression model and summarize the pooled results:

```{r}
#| echo: false

# Fit a linear model on each imputed dataset
fit <- with(imp, lm(Ozone ~ Solar.R + Wind + Temp + Month)) # replace y, x1, x2 with your dependent and independent variables

# Pool the results of these models
pool <- pool(fit)
summary(pool)
```

You can also use `gtsummary` to create a summary table, here `tbl_regression` automatically pools results.

```{r}
library(gtsummary)
tbl_regression(fit)
```

Remember, the imputation model should be compatible with the analysis model. If the analysis model includes interactions or transformations, they should be included in the imputation model as well.

Also, it's important to note that the `mice` package assumes that the missing data are Missing At Random (MAR), which means that the probability of a value being missing depends only on observed data and not on unobserved data. If this assumption is violated, the results of the imputation might be biased.

Imputation is a valuable tool for handling missing data, but it should be used judiciously and based on a thorough understanding of the data and the analysis objectives. Responsible imputation ensures that any assumptions made during the process align with the data generation process and results in more reliable and meaningful data analysis.

#### Table of Imputations

```{r}
#| echo: false
#| eval: true
airquality %>%
  mutate(
    imp_fixed = replace(Ozone, is.na(Ozone), 0), # Impute with a fixed value (0 in this case)
    imp_mean = replace(Ozone, is.na(Ozone), round(mean(Ozone, na.rm = TRUE), 2)), # Impute with the mean
    imp_fill = Ozone,
    imp_median = replace(Ozone, is.na(Ozone), median(Ozone, na.rm = TRUE)), # Impute with the median
    imp_forest = missForest(.)$ximp$Ozone,
    imp_lm = ifelse(is.na(Ozone), predicted_values, Ozone),
    imp_knn = airquality_knn$Ozone,
    imp_mlh = airquality_mlh$Ozone,
    imp_pmm = complete(mice(., method = "pmm", printFlag = FALSE))$Ozone,
    imp_cart = complete(mice(., method = "cart", printFlag = FALSE))$Ozone,
    imp_lasso = complete(mice(., method = "lasso.norm", printFlag = FALSE))$Ozone,
    .keep = "used" 
  ) %>%
  fill(imp_fill, .direction = "down") %>% 
  filter(is.na(Ozone)) %>%
  select(-Ozone) %>%
  mutate(across(everything(), ~round(.,1))) %>%
  DT::datatable(
  options = list(pageLength = 10, scrollX = TRUE), 
  rownames = FALSE, 
  class = 'cell-border stripe' 
)


```
