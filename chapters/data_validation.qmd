---
title: "Data Validation"
code-annotations: hover
error: false
message: false
warning: false
---

```{r}
#| include: false
source("../scripts/knitr-minted-listed-hook.r")
```

Data validation is a crucial part of data analysis, encompassing the maintenance of data integrity, accuracy, and cleanliness for computational tasks. Since the results of analysis are heavily dependent on the quality of the input data, having a robust validation process is essential. A lack of such a process can lead to the effect of "Garbage in, garbage out".

Imagine dedicating hours to an analysis only to discover a duplicate row or sporadic NAs. The key to avoid such scenarios lies in regular data checks. With dynamic data, automating these checks becomes the solution. You can write a function to perform these checks or set up data validation pipelines for multiple checks. If the requirement includes sharing validation results, generate reports accordingly.

While this might seem daunting, there are numerous R packages, along with native functions, specifically designed to significantly simplify this task.

## Manual Inspection

Despite the convenience of automation, remember that you can't address what you're not aware of. Sometimes, data may not be ready for immediate analysis and may require cleaning before validation. 
Therefore, it's essential to manually open the files, inspect the tables and their values, and conduct preliminary exploratory analysis. 
From my experience, the most crucial step is to meticulously examine each variable individually, building an understanding of their meanings and implications. This thorough process may consume a few hours, but the payoff is significant: you'll gain an enhanced familiarity with the dataset, coupled with a comprehensive list of aspects requiring attention and rectification.
Lastly, always verify your results by reviewing the output table, an important yet simple step to remain fully engaged with the raw data.


## Handling Data Issues

### Base R

In the event of data issues, it's crucial to stop the script execution and alert the user. Base R offers several functions to facilitate this. For instance, the `is_numeric()` function, along with its `is_` counterparts, are traditional examples. Control flow functions like `if`, `else`, `stop()`, and logical operators prove to be quite useful. Lastly, don't forget the `duplicated()`, `unique()`, and `dplyr`'s `distinct()` functions.

```{r}
x <- c(1, 2, 3)
df <- data.frame(x = c(x, 1), y = c(x * 2, 2))
# Stops execution if `x` isn't numeric.
if (!is.numeric(x)) stop("x is not numeric!")
stopifnot(is.numeric(x)) # Halts if `x` isn't numeric.
# Stops the process if `x` contains non-positive values.
if (!all(x > 0)) stop("x contains non-positive values!")
# Halts execution if `x` has duplicates.
if (any(duplicated(x))) stop("x contains duplicated values!")
# Issues a warning if `df` has duplicate rows.
if (nrow(dplyr::distinct(df)) != nrow(df)) warning("df has duplicated rows!")
```
### Assert Your Conditions

`assertr` is an excellent package for tidyverse-compatible data validation. Rather than manually running checks, you can add an assert statement to verify your assumptions about the data. If the assumption holds, the code continues running; however, if it fails, an error is thrown and execution is halted.

`assertr` provides functions such as `verify()`, `assert()`, `insist()`, and `chain()`, all of which set conditions your data must meet:

+ `verify()`: This function checks whether a given logical condition holds true for the entire dataset. If not, it halts the execution and throws an error.
+ `assert()`: This function applies a specific predicate function to selected columns in your data frame. The data passes validation only if all values in those columns satisfy the predicate function's condition.
+ `insist()`: Similar to `assert()`, this function allows for specifying a proportion or number of values that must meet the predicate function condition.
+ `chain()`: This function is used when you want to specify more than one predicate function in the same `assert()` or `insist()` call. The data passes validation only if all predicates are met.

The syntax of the package integrates smoothly into a typical `dplyr` pipeline. Here's a brief example:

```{r}
#| error: true
library(assertr)
library(dplyr)

data(mtcars)

mtcars %>%
tibble::rownames_to_column() %>%
verify(nrow(.) > 0) %>% # Verify dataset isn't empty.
# Assert that specified Engine Types are present in row names.
assert(in_set(0, 1), vs) %>%
# Ensure that at least one 'mpg' value is within two standard
# deviations.
insist(within_n_sds(2), mpg)
```

In this example, `verify()` ensures `mtcars` has more than one row. `assert()` checks for the presence of certain row names, and `insist()` ensures at least one `mpg` value lies within two standard deviations. If any check fails, the pipeline stops and throws an error.

"Helper" functions in `assertr` are predicate functions that return a logical true or false. These are used in conjunction with `assert()`, `insist()`, or `verify()`. Examples of helper functions include `within_bounds()`, `not_na()`, `is.numeric()`, `in_set()`, etc. You can also create and use your own predicate functions if needed.

Replace your in-console data check with assertions, and if you're not already conducting data checks, start now.

### Precise Validation with Pointblank

The `pointblank` package in R is specifically tailored for data validation. It's designed with features to make data validation more reliable, better documented, interactive, and adaptable to various scenarios. Its key features include:

- **Assertion functions**: These functions allow setting quality standards for your data and halting execution if these standards are not met. They can check data types, value ranges, set memberships, distribution properties, etc.
- **Informative interruptions**: Pointblank provides detailed error messages when data issues arise, causing a halt in the R process.
- **Report generation**: A unique feature of Pointblank is that it creates comprehensive reports about the data validation process and its results.
- **Integration with `dplyr` and `tidyverse`**: Pointblank complements the `tidyverse` suite of packages, especially `dplyr`, making it easy to apply data validation alongside data manipulation and visualization tools.
- **Agent objects for ongoing validation**: The concept of 'agent' objects is introduced for continuous data validation. An agent can hold various types of validation checks and be reused across different datasets.

Pointblank has a plethora of functionality built into it and is a great fit for important projects as it lets you create validation pipelines, HTML reports, and even distribute the reports to stakeholders. Below, we'll create an agent and investigate the `mtcars` dataset. It's worth noting that you're not required to create an agent - you can use the functions similarly to `assertr`.

![[Point Blank Workflow](https://github.com/rich-iannone/pointblank)](images/pointblank_pipeline.png){height=2in}

#### Step 1: Create a Validation Plan (an Agent)

```{r}
library(pointblank)

agent <- create_agent(
  tbl_name = "a simple mtcars data validation",
  label = "an example of using pointblank for data validation",
  tbl = mtcars # attach the data frame to validate
)
```

#### Step 2: Specify Checks

```{r}
agent <- agent %>%
# Check if 'mpg' values are greater than 10.
col_vals_gt(vars(mpg), value = 10) %>%
# Check if 'hp' values are less than or equal to 300.
col_vals_lte(vars(hp), value = 300) %>%
# Check if 'vs', 'am', and absent columns exist.
col_exists(vars(vs, am, absent)) %>%
# Check if 'cyl' and 'gear' columns have no missing values.
col_vals_not_null(vars(cyl, gear))
```

#### Step 3: Execute Checks

::: {.content-visible when-format="html"}
```{r}
(report <- interrogate(agent, extract_failed = T))
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| eval: false
(report <- interrogate(agent, extract_failed = T))
```
```{r}
#| echo: false
#| include: false
report <- interrogate(agent, extract_failed = T)
```

![](./images/point_blank_screenshot.png){height=3.5in, width=6in}
:::

The interrogation revealed that all checks were correctly evaluated (EVAL), with almost all checks passing. The exception was a single row that failed due to the horsepower being over 300. Additionally, there is no column named `absent`. You can view the proportion of PASS/FAIL in the respective columns. The columns with failed checks are highlighted in color on the far left side of the table. The results can be downloaded as a CSV, which is particularly helpful for sharing the report with others, and there's a built-in functionality for sharing via email. To examine the failed rows, use `extract_failed = TRUE` and access them with `$extracts`. Below, we observe the failed row: 335 horsepower, indicating a very fast car (Maserati Bora)!

```{r}
report$extracts$`2` # `2` refers to the check number
```


## Summary

We learned that data validation is crucial in data analysis, playing a key role in maintaining the integrity, accuracy, and cleanliness of data. This process requires regular checks, which can be automated using control flow functions, as well as `assertr` and `pointblank` packages in R. Despite the efficiency of automation, manual inspection still holds significant importance and should be the initial step in the validation process. Ensuring the validation of your data is essential for providing high-quality inputs in your reports. In the next chapter, we will explore strategies for dealing with missing values.

