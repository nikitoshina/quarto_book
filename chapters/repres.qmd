---
title: "Reproducible Research"
editor: visual
---

When authoring a research paper you are expanding the knowledge frontier. Good research doesn't simply discover something new, but does so in a systematic and reproducible way. You wouldn't believe a study that is impossible to confirm, right?

> Non-reproducible single occurrences are of no significance to science. [@popper2002logic]

A survey of Scientists conducted by Nature in 2016 [@baker500ScientistsLift2016] discovered that 70% of researchers were unable to replicate the experiments of other scientists, and more than 50% were unable to reproduce their own experiments.

So, how do we provide all the information, so others can confirm the results of your study? We use the principles of reproducible research.

![[**https://xkcd.com/242/**](https://xkcd.com/242)](images/scientist_comic.png)

**Replicability**, the ability of other scientists to confirm your findings in different settings, is considered the gold standard in scientific research. However, achieving replicability can be challenging due to factors such as time, effort, money, and opportunity constraints. As an alternative, researchers can aim for **reproducibility** -- making analytical data and code available so others may reproduce their findings. Reproducibility is a standard and should be employed, especially in hard-to-replicate studies.

The minimum requirement for reproducibility involves providing the analytical data used in the analysis. While having access to the raw data and processing code is beneficial, it is not always necessary for reproducing the analysis. Researchers should also supply the analytic code used to produce results, such as model specifications. Additionally, clear documentation of the data and code is crucial to help readers understand the dataset and the code's functionality. Finally, this information should be easily accessible through standard distribution methods, such as GitHub.

## Literate Programming

<!-- Measured Data, Processing Code, Analytic Data, Analytic Code-->
```{mermaid}
%%| label: fig-literate-programming
%%| fig-cap: "Literate Programming Flow"

flowchart TD
    A[Collected Data] -->|Processing Code| B(Analytic Data)
    B --> |Analysis Code| C(Computational Results)
    C --> D{Presentation Code}
    D --> G[Figures]
    D --> E[Tables]
    D --> F[Summaries]

    G --> H[Article]
    E --> H
    F --> H

    I[Text] --> H
```

Literate programming integrates text and code chunks, creating a seamless blend of human-readable explanations and machine-executable code. The code loads data, generates graphs, and runs models, while the text provides context and interprets the results. This approach enables researchers to produce documents that are both human- and machine-readable.

One of the earliest implementations of literate programming was Sweave, which combined LaTeX and R for documentation and programming. Since then, the field has evolved with the introduction of RMarkdown, Jupyter Notebooks, Python Markdown, etc. The latest development in this area is Quarto, which will be covered in this book. This tool continues to advance the concept of literate programming, offering researchers a comprehensive solution for creating transparent, reproducible, and well-documented research outputs.


### Example: Customer Satisfaction Survey Analysis

Let's explore how @fig-literate-programming can be applied to a hypothetical customer satisfaction survey analysis project. This project aims to assess customer satisfaction through a survey and effectively communicate the findings. Here's an overview of the workflow with detailed steps:

1. **Collected Data:** Gather survey responses on customer satisfaction, preferences, and feedback. Ensure to retain the raw data for reference.

2. **Processing Code:** Develop a separate code to clean the collected data, addressing inconsistencies, missing values, and irrelevant entries. Document assumptions and processes meticulously, avoiding opinionated methods that may influence results.

3. **Analytic Data:** The cleaned data serves as the basis for analysis, providing accurate insights. Share this cleaned data with stakeholders.

4. **Analysis Code:** Utilize various analytical techniques to derive meaningful insights from the clean data, unveiling trends, patterns, and correlations. Document this analysis code comprehensively for publication.

5. **Computational Results:** Generate computational results highlighting key findings such as average satisfaction scores, common complaints, and customer segments.

6. **Presentation Code:** Develop code to create visualizations, charts, and graphs that effectively communicate survey results to stakeholders and other analysts:

    + **Figures:** Visual representations illustrate trends and distributions, aiding in the understanding of Net Promoter Scores over time and customer segment distribution.
    
    + **Tables:** Use tables to showcase snippets of data, including summaries and visual information.
    
    + **Summaries:** Summarize significant findings from tables, such as regression summaries and five-number summaries^[Minimum, First Quartile (Q1), Median (Q2), Third Quartile (Q3), Maximum].

7. **Text:** Incorporate descriptive text to provide context and explanations for visual and tabular components.

8. **Article:** Leverage the insights extracted to compose a comprehensive article or report on customer satisfaction, encompassing trends, analysis, and actionable recommendations.

Remember that while this is a simplified representation, real projects may involve more intricate steps and considerations.
