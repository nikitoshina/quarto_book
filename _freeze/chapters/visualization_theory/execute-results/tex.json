{
  "hash": "068a0be80e5e677eb8dc394c8ebf2fd8",
  "result": {
    "markdown": "---\ntitle: \"Data Visualization Fundamentals\"\neditor: visual\nformat:\n  html:\n    echo: false\n    warning: false\n    # code-fold: true\n    # code-summary: \"Show the code\"\ntidy: true\n---\n\n\n\n\n\nWe are about to embark on a wonderful journey into the world of data visualization. This chapter was not originally planned, but the more I thought about the importance of basic psychophysics concepts, their relationship to design, and data visualization, the more I felt compelled to include them (besides, having two chapters dedicated to color seemed excessive, so why not fully commit?).\n\nLet's start with a simple question: What is data visualization? Data visualization is the graphical representation of information and data. It sounds pretty simple and self-explanatory, right?\n\nNext question: What are the goals of data visualization?\n\n1.  Effective communication of information and insights\n2.  Analysis and exploration\n3.  Decision-making\n\nIt may sound obvious, but the complexity lies in how we achieve these goals. The main power of data visualization lies in its ability to present large, multidimensional datasets in a succinct, easy-to-consume form. Our memory has limits, as we can typically hold about 5Â±2 things in our heads at one time. Visuals can help us reduce and simplify information. For instance, if we have three numbers to present, it might be best to use a table (which, in my opinion, are underrated).\n\nSo, how do we reach the point where our visuals are aiding us and our audience in better understanding the data?\n\n## Perceptual Processing\n\nTo answer that question, we'll start with how humans perceive the world, or more accurately, how our brains process visual information.\n\nWhy do we care about perceptual processing?\n\n-   It helps explain why some techniques work well and others do not.\n-   It informs us about the limits people have in terms of visual perception.\n-   It can support design decisions.\n\nVisual perception occurs in two phases (similar to Daniel Kahneman's systems 1 and 2):\n\n1.  Early, parallel detection of color, texture, shape, and spatial attributes:\n\n    -   Involves the detection of orientation, color, texture, movement, etc.\n    -   Engages arrays of neurons working in parallel.\n    -   Occurs \"automatically\" and rapidly.\n    -   Information is briefly held in iconic storage.\n    -   Follows a bottom-up, data-driven model of processing.\n    -   Often referred to as \"pre-attentive\" processing because it occurs without the direction of our conscious mind.\n\n2.  Serial processing of object identification (using memory) and spatial layout and action:\n\n    -   Involves sequential processing.\n    -   Splits into subsystems for object recognition and interaction with the environment.\n    -   Evidence supports the independence of systems for symbolic object manipulation and for locomotion & action.\n    -   The first subsystem interfaces with the verbal linguistic portion of the brain, while the second interfaces with motor systems that control muscle movements.\n    -   Involves slow serial processing.\n    -   Engages working and long-term memory.\n    -   Follows a top-down processing model.\n\n### Pre-attentive Processing\n\nOur main focus for now is system 1, as it is the system that \"sees\" and directs our attention. By catering to it, we can make our visuals more intuitive.\n\nHow does the human visual system analyze images? - Some things seem to be processed pre-attentively, without the need for focused attention. - This process generally takes less than 200-250 milliseconds (for reference, eye movements take about 200 milliseconds). - It seems to be handled in parallel by the low-level vision system.\n\nLet's consider an example with a number wall:\n\n|     |     |     |     |     |     |\n|-----|-----|-----|-----|-----|-----|\n| 1   | 2   | 7   | 9   | 8   | 3   |\n| 5   | 6   | 4   | 1   | 7   | 9   |\n| 6   | 7   | 4   | 2   | 3   | 6   |\n| 2   | 7   | 5   | 7   | 9   | 0   |\n| 1   | 3   | 8   | 5   | 8   | 3   |\n| 2   | 0   | 0   | 3   | 7   | 4   |\n\nNow, with highlights, finding the number 2 becomes much faster, doesn't it?\n\n|       |       |     |       |     |     |\n|-------|-------|-----|-------|-----|-----|\n| 1     | **2** | 7   | 9     | 8   | 3   |\n| 5     | 6     | 4   | 1     | 7   | 9   |\n| 6     | 7     | 4   | **2** | 3   | 6   |\n| **2** | 7     | 5   | 7     | 9   | 0   |\n| 1     | 3     | 8   | 5     | 8   | 3   |\n| **2** | 0     | 0   | 3     | 7   | 4   |\n\nWhat kinds of tasks can the pre-attentive system perform? - Target detection: Users rapidly and accurately detect the presence or absence of a \"target\" element with a unique visual feature within a field of distractor elements. - Boundary detection: Users rapidly and accurately detect a texture boundary between two groups of elements, where all the elements in each group have a common visual property. - Region tracking: Users track one or more elements with a unique visual feature as they move in time and space. - Counting and estimation: Users count or estimate the number of elements with a unique visual feature.\n\nExample: Can you rapidly detect the presence of a red circle?\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nset.seed(42)\npoints = 40\ncoords <- data.frame(x = runif(n = points, min = 0, max = 1),\n                     y = runif(n = points, min = 0, max = 1),\n                     color = c(rep(F,times = points - 1),T))\n\nno_red <- coords |> ggplot(aes(x, y)) +\n    geom_point(size = 8, show.legend = F, color = \"steelblue\") +\n    theme_void() +\n    coord_cartesian(clip = \"off\") \n\nwith_red <- coords |> ggplot(aes(x, y, color = color)) +\n    geom_point(size = 8, show.legend = F) +\n    theme_void() +\n    coord_cartesian(clip = \"off\") +\n    scale_color_manual(values = c(\"steelblue\",\"red\"))\n\nno_red\n```\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-2-1.pdf)\n:::\n\n```{.r .cell-code}\nwith_red\n```\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-2-2.pdf)\n:::\n:::\n\n\n\n<!-- https://www.csc2.ncsu.edu/faculty/healey/PP/#:\\~:text=A%20simple%20example%20of%20a,target%20is%20present%20or%20absent. -->\n\n<!-- ```          -->\n\n<!-- We do not attend to everything we see -->\n\n<!-- We do not have good working memeory for what we see -->\n\n<!-- ``` -->\n\nPre-attentive processing allows our visual system to rapidly and accurately detect limited visual properties before we are consciously aware of them:\n\n-   We can easily identify the presence or absence of a target within a visual field.\n\n-   We can effortlessly detect a texture boundary between two groups of elements.\n\n-   We can smoothly track an element with a unique visual feature as it moves through space and time.\n\nHere are some types of preattentive processing we can perform:\n\n::: {layout-ncol=\"4\"}\n![Line (blob) orientation](images/data_viz_fundamentals/tg_orient.gif)\n\n![Length, width](images/data_viz_fundamentals/tg_len.gif)\n\n![Closure](images/data_viz_fundamentals/tg_closure.gif)\n\n![Size](images/data_viz_fundamentals/tg_size.gif)\n\n![Curvature](images/data_viz_fundamentals/tg_curve.gif)\n\n![Density, contrast](images/data_viz_fundamentals/tg_den.gif)\n\n![Number, estimation](images/data_viz_fundamentals/tg_num.gif)\n\n![Color (hue)](images/data_viz_fundamentals/tg_hue.gif)\n\n![Intensity, luminosity, binocular lustre](images/data_viz_fundamentals/tg_lum.gif)\n\n![Intersection](images/data_viz_fundamentals/tg_isect.gif)\n\n![Terminators](images/data_viz_fundamentals/tg_term.gif)\n\n![2D depth cues](images/data_viz_fundamentals/tg_3d_depth.gif)\n\n![Flicker](images/data_viz_fundamentals/tg_flick.gif)\n\n![Direction of motion](images/data_viz_fundamentals/tg_dir.gif)\n\n![Velocity of motion](images/data_viz_fundamentals/tg_vel.gif)\n\n![Lighting direction](images/data_viz_fundamentals/tg_3d_light.gif)\n:::\n\n```         \nWe struggle to have multiple categories \n```\n\n#### Feature Hierarchy\n\nMultiple features such as color and shape can represent multiple types of data in a single image. But, it's important to make sure that these visual features don't mix up and hide the data we want to show. Think of it like trying to find a red apple in a bowl full of green apples -- it's easy because the color stands out.\n\nSometimes, our eyes like one visual feature more than another. For example, when we're looking at shapes, colors can be distracting and make it harder to see the shape patterns. But, if the colors are all the same, the shapes stand out clearly. We struggle to perceive more then two categories. Try to find the groups of points below, which groups do you spot first?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolor_points <- complete(tibble(x = 1:6, y = 1:6), x, y) %>%\n  mutate(color = ifelse(x >= 1 & x <=2, T,F)) %>%\n  ggplot(aes(x,y, color = color)) + \n  geom_point(size = 4, show.legend = F) +\n  xlim(c(1,6)) +\n  ylim(c(1,6)) +\n  coord_fixed(ratio = 1, expand = T, clip = \"off\") +\n  theme_void()\n\ncolor_shape_points <- complete(tibble(x = 1:6, y = 1:6), x, y) %>%\n  mutate(color = ifelse(y > 2 & y < 5, T,F)) %>%\n  mutate(shape = ifelse(x > 2 & x < 5, T,F)) %>%\n  ggplot(aes(x,y, color = color, shape = shape)) + \n  geom_point(size = 4, show.legend = F) +\n  xlim(c(1,6)) +\n  ylim(c(1,6)) +\n  coord_fixed(ratio = 1, expand = T, clip = \"off\") +\n  theme_void()\n\ncolor_mix_shape_points <-complete(tibble(x = 1:6, y = 1:6), x, y) %>%\n  mutate(color = ifelse(y <= 2, T,F)) %>%\n  mutate(shape = ifelse(sample(0:1,size = nrow(.),replace = TRUE), T,F)) %>%\n  ggplot(aes(x,y, color = color, shape = shape)) + \n  geom_point(size = 4, show.legend = F) +\n  xlim(c(1,6)) +\n  ylim(c(1,6)) +\n  coord_fixed(ratio = 1, expand = T, clip = \"off\") +\n  theme_void()\n\nmix_color_shape_points <- complete(tibble(x = 1:6, y = 1:6), x, y) %>%\n  mutate(shape = ifelse(x >= 5, T,F)) %>%\n  mutate(color = ifelse(sample(0:1,size = nrow(.),replace = TRUE), T,F)) %>%\n  ggplot(aes(x,y, color = color, shape = shape)) + \n  geom_point(size = 4, show.legend = F) +\n  xlim(c(1,6)) +\n  ylim(c(1,6)) +\n  coord_fixed(ratio = 1, expand = T, clip = \"off\") +\n  theme_void()\n```\n:::\n\n::: {#fig-feature-hierarchy .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ncolor_points\ncolor_mix_shape_points\ncolor_shape_points\nmix_color_shape_points\n```\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/fig-feature-hierarchy-1.pdf){#fig-feature-hierarchy-1}\n:::\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/fig-feature-hierarchy-2.pdf){#fig-feature-hierarchy-2}\n:::\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/fig-feature-hierarchy-3.pdf){#fig-feature-hierarchy-3}\n:::\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/fig-feature-hierarchy-4.pdf){#fig-feature-hierarchy-4}\n:::\n\nExamples for Feature Hierarchy\n:::\n\n\n\nCheck out more and play the game here: https://www.csc2.ncsu.edu/faculty/healey/PP/#jscript_search\n\nSo, when we're deciding how to visually represent our data, we should pick the features that make the most important information stand out. This way, we avoid hiding the data we want to show.\n\nCommon ways to visually encode numbers in order from most easily perceived to least:\n\n-   position along a common scale, axis, and baseline\n-   position along non-aligned axes\n-   Length, direction, angles of relative lines/ slope\n-   Area\n-   Volume, curvature, arcs / angles within a shape\n-   Color or shading\n\n#### Integral vs Separable\n\nCan you develp a set of unique symbols that can be placed on a display and be rapidly recieved and differentiated? If we talked privously about picking a number off a visual depiction and match the intended encoded number, here we talk whether they can be rapidly percieved and differentiated from each other.\n\nSuppose that we use two different visual properties to encode two different variables in a discrete dataset (one is easy, three is hard): - color, size, share, lightness\n\nWill the two different properies interact so that they are more/less difficult to untangle? - Integral - two properies are viewed holistically - Separable - judge each dimension independently\n\n![@wareInformationVisualizationPerception2021](images/integral_vs_separable.png)\n\n<!-- ware 2000 picture, munzer 2015 -->\n\n#### Nonlinear Perception\n\nPerception is not uniformly linear. There are some things we perceive accurately, such as length, while there are others that we tend to underestimate, such as the true difference between two values due to our ability to sense ratios.\n\nFor example, you are pretty good at estimating lengths and temperatures after a little practice. However, in some domains, we tend to underestimate differences and miss the ratios. Our perception adjusts to the strength of the signal; for instance, our eyes adjust to bright daylight and to darkness in a room.\n\nVisualization is about turning numbers into pictures. However, the goal is for the user to be able to translate these pictures back into numbers accurately.\n\nOne tricky task in visualization is translating numbers into areas. This is not only difficult to decipher but also to encode. What are we comparing when we look at areas---radius, area, or sensation?\n\nLet's consider an example. If you have three red circles and one green circle, which red circle represents a number that is twice as big as the green one?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_a_circle <- function(radius, color, label){\n  ggplot() + \n  xlim(c(0,5)) +\n  ylim(c(0,5)) +\n  coord_fixed(ratio = 1, expand = T)+\n  theme_void() +\n  ggforce::geom_circle(aes(x0 = 2.5, y0 = 2.5 ,r= radius), fill=color) +\n  geom_text(aes(x = 2.5, y = 2.5, label = label), fontface = \"bold\", color = \"black\", size = 12)\n}  \n\n# Original\ncircle_1 <- make_a_circle(radius=1, color = \"green\", label = 1)\n\n# Area\ncircle_2 <- make_a_circle(radius=1*sqrt(2), color = \"red\", label = 2)\n\n# Steven's Law\ncircle_3 <- make_a_circle(radius=1*sqrt(2)^(1/0.7), color = \"red\", label = 3)\n\n# Radius\ncircle_4 <- make_a_circle(radius=1*2, color = \"red\", label = 4)\n\ncircle_1 + circle_2 + circle_3 + circle_4\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` in the `default_aes` field and elsewhere instead.\n```\n:::\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nFunny enough, all three are correct! The second circle has twice the area of the original, the third circle appears to have twice the area according to Stevens' Law (which we will discuss shortly), and the fourth circle has twice the radius. Yes, it is indeed confusing! The way we perceive proportional differences in sensation is not a one-to-one relationship with the measurement.\n\nOur goal in data visualization is to transform visuals into numbers in a way that makes it easy for the reader to understand.\n\nNow, let's talk about Stevens' power law. Stevens was interested in this exact question and formulated Stevens' Law in 1960.\n\n$$\ns(x) = ax^{b}\n$$ {#eq-stevens}\n\n$s$ is sensation $x$ is intensity of the attribute $a$ is a multiplicative constant $b$ is the power\n\n$b > 1$: overestimate;\n\n$b < 1$: underestimate\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstevens_function_factory <- function(b) {\n    function(x) {\n        1 * x ^ b\n    }\n}\n\n# stevens_data <- tribble(\n#   ~category, ~b, ~x, ~y,\n#   \"Saturation (Red)\", 1.7, 2,4,\n#   \"Sweetness\", 1.3, 2,3,\n#   \"Length\", 1, 4.5,4,\n#   \"Area\", 0.7, 1,2,\n#   \"Brightness\", 0.5, 2,4,\n#   \"Heaviness\", 1.45, 4,5,\n#   \"Duration\", 1.1, 5,5\n# )\n# \n# \n# stevens_functions[[3]](5)\n# stevens_function_factory(2)(2)\n# \n# stevens_data[\"function\"] <- list(stevens_functions)\n\nplot_functions <- function(x,y,b,category) {\n    list(\n    list(geom_function(fun = stevens_function_factory(b), linewidth = 1.2)),\n    list(geom_text(aes(x=x,y=y, label = category)))\n    )\n}\n\nstevens_data <- tribble(\n  ~category, ~b, ~x, ~y,\n  \"Saturation (Red)\", 1.7, 1.2, 4,\n  \"Length\", 1, 4.5, 4,\n  \"Area\", 0.7, 4, 3,\n  \"Brightness\", 0.5, 4.2, 1.5,\n  \"Duration\", 1.1, 3.2, 4.5\n)\nggplot() +\n    ylim(0,5) +\n    pmap(stevens_data,plot_functions)+\n    xlim(0,5) +\n    coord_fixed(ratio = 1, expand = F) + \n    theme_minimal() +\n    labs(x=\"Intensity\", y = \"Sensation\") + \n    theme(panel.grid.minor = element_blank())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 49 rows containing missing values (`geom_function()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 14 rows containing missing values (`geom_function()`).\n```\n:::\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nExperimental results for (b), the exponent in Stevens' Law, range from 0.9 to 1.1 for lengths, 0.6 to 0.9 for area, and 0.5 to 0.8 for volume. As a rule of thumb, (b \\approx \\frac{1}{\\sqrt{\\text{dimensionality}}}).\n\nSo, how would we apply this apparent scaling in practice? Let's consider an example where we want to draw circles of different areas. Imagine the largest circle has an area twelve times bigger than the smallest one. To counteract our tendency to underestimate, we could increase the area by approximately (\\sqrt{0.7}). However, it's important to consider the context and whether these adjustments will truly benefit your visualization. Nonetheless, if you were to make these adjustments, this is how you would do it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncircle_comparison <- tribble(\n  ~number, ~area, \n  1, 100,\n  2, 400,\n  3, 900,\n  4, 1600,\n) %>%\n  mutate(x = 1, \n         r = sqrt(area/pi),\n         adj_area = .98365*100*(area/100)^(1/0.87),\n         adj_r = sqrt(adj_area/pi),\n         y = r,\n         adj_y = adj_r,\n         ) %>%\n  arrange(desc(area))\n\ncustom_colors <- c(\"Piggy Pink\" = \"#F7DAE5\",\n                   \"Old Lace\" = \"#FFF2E1\",\n                   \"Columbia Blue\" = \"#C3E2E6\",\n                   \"Languid Lavender\" = \"#D0CCE0\")\n\nactual_circles <- circle_comparison %>% \n  ggplot() + \n  ylim(c(0,56)) +\n  xlim(c(-28,28)) +\n  coord_fixed(ratio = 1, expand = T)+\n  theme_void()+\n  ggforce::geom_circle(aes(x0 = 0, y0 = y, r= r, fill = number), show.legend = F) +\n  geom_label(aes(x=0, y = 2*y-4, label = area)) +\n  scale_fill_continuous(type = \"viridis\")\n\nadjusted_circles <- circle_comparison %>% \n  ggplot() + \n  ylim(c(0,56)) +\n  xlim(c(-28,28)) +\n  coord_fixed(ratio = 1, expand = T)+\n  theme_void()+\n  ggforce::geom_circle(aes(x0 = 0, y0 = adj_y, r= adj_r, fill = number), show.legend = F) +\n  geom_label(aes(x=0, y = 2*adj_y-4, label = area)) +\n  scale_fill_continuous(type = \"viridis\")\n\nactual_circles + adjusted_circles\n```\n\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# source https://files.eric.ed.gov/fulltext/ED045469.pdf\n```\n:::\n\n\n\nTurning color into numbers is complicated, as it is affected by a myriad of factors, from lighting to individual perception. For example, consider the chess pieces in the image below. Do they appear to be the same color?\n\n![by Barton L. Anderson and Jonathan Winawer](images/chess-pieces.jpg)\n\nDespite appearances, they are actually the exact same color!\n\nPurposes of Using Color:\n\n-   Call attention to specific data points\n-   Enhance appeal and memorability\n-   Represent discrete categories\n\nWhen using color:\n\n-   Opt for pastel shades.\n-   Avoid high saturation.\n-   Be mindful of spectral colors as they can cause afterimages.\n-   Utilize color for grouping and searching.\n\n#### Gestalt Principles\n\nGestalt Principles explain how the human brain perceives visual patterns from grouped elements. These principles encompass concepts like proximity, similarity, continuity, closure, connection, and enclosure.\n\n::: columns\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n:::\n\n::: column\n**Proximity**: When objects are close together, we often perceive them as a group\n:::\n:::\n\n::: columns\n::: column\n**Similarity**: When objects share similar attributes (color, shape, etc.), we often perceive them as a group\n:::\n\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n:::\n:::\n\n::: columns\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n:::\n\n::: column\n**Enclosure**: When objects are surrounded by a boundary, we often perceive them as a group\n:::\n:::\n\n::: columns\n::: column\n**Closure**: Sometimes partially open structures can still be perceived as a grouping metaphor (e.g., \"\\[...\\]\")\n:::\n\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n:::\n:::\n\n::: columns\n::: column\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](visualization_theory_files/figure-pdf/unnamed-chunk-12-1.pdf)\n:::\n:::\n\n\n:::\n\n::: column\n**Connectivity**: When you draw curves or lines through data elements, this is often perceived as creating a connection between them\n:::\n:::\n\n<!-- ### Visual Encoding -->\n\n<!-- Now we have covered the preattentive process, let's talk about how we can use this knowledge to visually encode the data. -->\n\n<!-- Rules: Humans have different kinds of memory, stored differently and in different parts of the brain Long-term vs. working memory Verbal memory vs. visual memory Working memory for visual information is very limited Humans can retain roughly three chunks of information at a time Visualizations can help \"chunk\" information together -->\n\n<!-- Keeping It Together We should avoid \"fragmentation\" (separating things that should be remembered together) So place the things most related closest together -- things that you most want the reader to remember together Highlight and annotate things explicitly, if you want the reader to notice them -->\n\n<!-- > Data visualization is not about putting numbers into shapes, but translating shapes into numbers -->\n\n<!-- > This how design principles work! Good design is the one that allows us use preattentive processing to the fullest to scan thing. -->\n\n<!-- > Good desing that allows for post attentive detailed exploration. -->\n\n<!-- > Do not make users search for things in your visualization but draw attention to things explicitly -->\n\n<!-- ????????????????????? -->\n\n<!-- Visual Encoding -->\n\n<!-- - Tasks to be done when visualizing information: -->\n\n<!--     - Encode numeric data visually -->\n\n<!--     - Encode categorical data visually -->\n\n<!--     - Encode distinctions between different pieces of information -->\n\n<!--     - Encode methods to associate data / distinctions to some context -->\n\n<!-- > Make reader's decoding process as easy and error-free as possible -->\n\n<!-- Relative magnitude estimation from most accurate to least accurate: -->\n\n<!-- -   Position (common) scale -->\n\n<!-- -   Position (non-aligned) scale -->\n\n<!-- -   Length -->\n\n<!-- -   Slope -->\n\n<!-- -   Angle -->\n\n<!-- -   Area -->\n\n<!-- -   Volume -->\n\n<!-- -   Color (hue/saturation/value) -->\n\n<!-- Visual attributes like length and 2D position encode quantitative data very precisely Visual attributes like width, size, color intensity, and blur do not not Data visualizations should align more precise attributes with variables that have the greatest need of precision -->\n\n<!-- ```          -->\n\n<!-- Use Familiar Chart types -->\n\n<!-- Don't make people remember views -->\n\n<!-- Avoid large legends -->\n\n<!-- Use intuities colors and shapes -->\n\n<!-- ``` -->\n\n<!-- What is our target audience? -->\n\n### Visual Encoding\n\nAfter learning about the basics of how our eyes and brain quickly process visuals, it's time to delve into visual encoding, which is about turning data into visuals that are easy to understand.\n\nWe need to tackle when visually encoding information we need to tackle the following taks:\n\n-   Turning numeric data into visuals\n-   Turning categorical data into visuals\n-   Showing the differences between pieces of information\n-   Showing how data or information relates to some context\n\nHumans have different types of memory like long-term, working, verbal, and visual memory, each stored in various parts of the brain. Our working memory, which temporarily holds information, can only keep around three chunks of information at a time. Visualizations can help group or \"chunk\" information together, making it easier for us to process and remember.\n\nIt's essential to keep related information close together in a visualization to avoid fragmentation, which is when we separate things that should be remembered together. By doing this, we help people remember and understand the information better. We can highlight or annotate important points to draw attention to them.\n\nGood design in visualizations helps people quickly understand what they're looking at. It's not about just putting numbers into shapes, but making those shapes tell a story. A well-designed visualization will help people easily scan through the information and also delve deeper if they want to.\n\n> The goal is to make it easy for the reader to decode the visual information without making errors.\n\nDifferent visual attributes like position, length, angle, or color help represent data. Some attributes, like position and length, are better for showing precise data, while others like color or size are less precise. It's crucial to match the right attribute with the type of data we're showing.\n\nUsing familiar chart types, intuitive colors, and shapes help make the visualization easy to understand. Avoid making people remember too many new symbols or having large legends, as it can be overwhelming.\n\nLastly, knowing who will be looking at the visualization will inform your decisions resulting in visuals that are easy to understand, remember, and interpret.\n\n## Evaluating your Graphs\n\nHow do we evaluate our graphics to enlighten and engage our audience, rather than deceive them? Several practical frameworks have been proposed for this purpose.\n\n### Data Ink Ratio [@tufteVisualDisplayQuantitative2001]:\n\nOne of the popular ideas in data visualization is the Data-ink ratio, introduced by Edward Tufte. This idea is all about keeping things simple and getting rid of any extras that don't help convey the main message. As Tufte suggests, it's good to \"erase non-data-ink, within reason\" and \"erase redundant data-ink, within reason.\" It might be tempting to remove too much, but it's better to take it slow. Trust your gut feeling on whether the chart still makes sense. The suggestions we'll discuss next are based on having clean and clear graphics.\n\n![](images/data_ink.gif)\n\n### Levers of Chart-Making [@wareInformationVisualizationPerception2021]:\n\n-   Speed to primary insight: How fast the audience can extract insight from a graph.\n-   Granularity: The level of detail in the data shown in a chart.\n-   Explore or explain: Whether the visualization allows users to explore the data themselves or is accompanied by an explanation.\n-   Dry or emotional: The seriousness or informality of the data presentation. We can make presentation more emotional to attract less data savvy audience.\n-   Ambiguity vs. accuracy: The balance between clear accuracy and intended ambiguity in the chart.\n\n### Cognitive Load [@sibingaCognitiveLoadGuide2021]:\n\nThis framework is divided into three categories:\n\n1.  Intrinsic load: Concerned with the complexity of the data itself.\n\n-   Measurement: The type of data (quantitative vs. qualitative).\n-   Knowability: The certainty of the data (certain vs. uncertain).\n-   Specificity: The clarity of data categories (precise vs. ambiguous).\n-   Relatability: How relatable the data is to everyday life (concrete vs. abstract).\n\n2.  Germane load: Concerned with the audience's readiness to process the information.\n\n-   Connection: How the audience first encounters the visualization (intentional vs. coincidental).\n-   Pace: The time the audience has to view the visualization (slow vs. fast).\n-   Knowledge: The audience's familiarity with the subject (expert vs. novice).\n-   Confidence: The audience's familiarity with the data reporting format (confident vs. anxious).\n\n3.  Extraneous load: Concerned with how new information is presented.\n\n-   Chart type: The commonality of the chart type (common vs. rare).\n-   Interpretation: The precision of the chart's values (accurate vs. approximate).\n-   Composition: The density of information on the page (concise vs. detailed).\n-   Delivery: Whether the data report is self-explanatory or requires exploration (explanatory vs. exploratory).\n\nNo framework is likely to replace the others; instead, they complement each other to cover the vast territory of the data visualization domain. Data-ink ratio principles remain a good starting point for most business contexts, but considering emerging frameworks can make the practice more nuanced for tackling different needs, messages, and audiences. The final determinant of how to incorporate the three frameworks will depend on the context of the visualization, with a clear understanding of the audience, the message, and the medium being key.\n\n> Finally, the most tried and true method of testing graphics is asking others to have a look at it!\n",
    "supporting": [
      "visualization_theory_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": true
  }
}