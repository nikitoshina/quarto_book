[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical tools for modern research",
    "section": "",
    "text": "Preface\n\nOptimizing Research Toolkit: A Guide for What School Forgot to Teach.\n\nThis book was born from my frustrations and experiences in higher education and professional work. Young data professionals typically learn about models, experiments, and theories during their classes and frequently return to that knowledge. However, executing high-quality research and analysis necessitates a deeper understanding of the tools and the “How” rather than just the “What” and “Why”. The knowledge required often transcends what is taught within the constraints of a standard curriculum. In this book, I aim to bridge that gap between knowing what you want to do and understanding how to do it. I’ve distilled hundreds of hours of frustrations into these chapters, so you won’t have to traverse that path yourself."
  },
  {
    "objectID": "chapters/intro.html#about-the-book",
    "href": "chapters/intro.html#about-the-book",
    "title": "1  Introduction",
    "section": "1.1 About the book",
    "text": "1.1 About the book\nThis book’s genesis was a collection of notes and reading materials for a class I taught on Survey Design in Spring 2023. The positive reception and plethora of insightful questions from the students spurred me to systematize and weave these resources into this book.\nThis book is not a comprehensive guide; if that’s what you’re seeking, you may want to look elsewhere. Instead, this book serves as a map that outlines the necessary tools and topics for your research journey. We’ll delve into the basics of data collection, cleaning, validation, and presentation, all while underpinning these concepts with relevant theory. The goal is to build your intuition and provide pointers for where to find more detailed information. The chapters are deliberately concise and to the point, as my objective isn’t to bore you, but to enlighten."
  },
  {
    "objectID": "chapters/intro.html#who-should-read-this-book",
    "href": "chapters/intro.html#who-should-read-this-book",
    "title": "1  Introduction",
    "section": "1.2 Who Should Read This Book",
    "text": "1.2 Who Should Read This Book\nWhile my experiences in Economics graduate school served as the primary motivation for writing this book, I’ve aimed to make it universally applicable for anyone working with data. That’s why you’ll find dedicated chapters on topics like thesis writing and literature reviews. If your work involves humanities, data analytics, or anything in-between, I’m confident that you’ll find this book valuable. Whether you’re a seasoned professional brushing up on some skills or a beginner hoping to learn something new, I promise you’ll discover something useful here.\nAlthough the examples in this book are written in R, which I believe to be one of the most powerful statistical languages, you don’t need prior knowledge of it to benefit from the content. The book focuses on the underlying concepts behind the tools, making it framework-agnostic. Some of the more theoretical chapters do not even require any programming knowledge. Over time, individuals and teams from diverse domains, such as web development, mathematics, data analytics, and economics, have found the content valuable for their work. So regardless of your coding skills or professional background, there’s something in here for you.\nThe book does not need to be read sequentially, but the chapters are ordered and build upon each other for those who prefer a more structured approach. Initially, the data manipulation part was positioned in the middle, but it was moved to the beginning to aid those who need to understand R code earlier."
  },
  {
    "objectID": "chapters/acknowledgements.html",
    "href": "chapters/acknowledgements.html",
    "title": "2  Acknowledgements",
    "section": "",
    "text": "I want to dedicate this book to my late grandfather, Vladimir Zamashikov.\nMy immeasurable gratitude goes to Alessandra Cassar, whose invaluable support and patience allowed me to explore my passion for research. Michael Jonas inspired me with the intriguing world of econometrics. Jesse Antila Hughes, Shiva Shukla, Peter Lorentzon, Armin, and others at the University of San Francisco deserve a heartfelt thank you. I am particularly grateful for the library and writing center at USF.\nGordon Getty has been an inspirational figure, enabling me to work on fantastic projects. I’m grateful to Ker Gibbs and CBSI for their support and mentorship and to Tom Wiley for his review and support.\nThanks to the authors of “Causal Inference” and “Causal Mixtape” for sharing their superb materials and inspiring stories online.\nMario, Steve Trettel, Parsa Rahimi, John Chetwynd, and Jake Consgrove, your support, curiosity, and confidence in my work are deeply appreciated.\nI must acknowledge my parents, Anton Tkachenko and Ekaterina Tkachenko, for providing me access to these incredible opportunities. A special note of thanks to Anastasia Ternovskaya for her unwavering belief in me.\nI’m thankful to Nelson Kin, Renee Read, Dibyojoty Chowdhury, Daria Tkachenko, and others, including the Posit team, for their invaluable contributions and support.\nFinally, my heartfelt appreciation goes to the open-source community and everyone who has generously shared their knowledge online. You made this possible, and I’m thrilled to contribute to the conversation. I hope that you find this book useful in your journey towards mastering research."
  },
  {
    "objectID": "chapters/summary.html",
    "href": "chapters/summary.html",
    "title": "\n3  Summary\n",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chapters/setup.html#r-and-rstudio-set-up",
    "href": "chapters/setup.html#r-and-rstudio-set-up",
    "title": "4  Set up",
    "section": "4.1 R and RStudio Set Up",
    "text": "4.1 R and RStudio Set Up\nTo begin our data analysis journey, we will utilize the R language, which was specifically designed by statisticians for statisticians. R boasts an intuitive syntax and workflow, enabling us to create readable and reproducible code, making it ideal for data analysis tasks.\nBefore we dive into the exciting world of data exploration and modeling, it’s essential to understand the distinction between R and RStudio IDE. R is the actual programming language itself, while RStudio is an Integrated Development Environment (IDE) that provides a user-friendly interface to work with R efficiently. In other words, RStudio serves as a helpful tool to interact with R and enhances the overall coding experience.\nTo get started, you’ll need to download both R and RStudio. R acts as the backbone, and RStudio complements it, making the coding process smoother and more accessible, especially for beginners. So, make sure to install both R and RStudio on your computer to embark on your data analysis journey seamlessly. Let’s dive in and explore the incredible possibilities that R and RStudio have to offer! ## Download R\nR is maintained through The Comprehensive R Archive Network.\n\n4.1.1 For macOS\n\nGo to https://cran.r-project.org/\nSelect “Download R for macOS”\nIf you have Apple Silicon mac (M1,M2…) download the latest version that has -arm64 in its name (R-4.2.2-arm64.pkg)\nIf you have Intel Max Download one without -arm64 (R-4.2.2.pkg)\nFollow the steps from the installation wizzard\nThe installer lets you customize your installation, but the defaults will be suitable for most users\nYour computer might ask for your password before installing new programs\n\n\n\n4.1.2 For Windows\n\nGo to https://cran.r-project.org/\nSelect “Download R for Windows”\nClick on “base”\nClick the first link at the top of the new page (Download R-4.2.2 for Windows)\nink downloads an installer program, which installs the most up-to-date version of R for Windows.\nFollow the steps from the installation wizzard\nThe installer lets you customize your installation, but the defaults will be suitable for most users\nYou might need administration privileges to install new software on your machine"
  },
  {
    "objectID": "chapters/setup.html#download-rstudio",
    "href": "chapters/setup.html#download-rstudio",
    "title": "4  Set up",
    "section": "4.2 Download RStudio",
    "text": "4.2 Download RStudio\nRstudio is like a Microsoft Word for writing text, but instead of text RStudio helps you write in R. RStudio is also free and easy to install! Go to RStudio Website, click on DOWNLOAD RSTUDIO or select your operation system below, and follow the isntallation instructions."
  },
  {
    "objectID": "chapters/setup.html#configure-rstudio",
    "href": "chapters/setup.html#configure-rstudio",
    "title": "4  Set up",
    "section": "4.3 Configure RStudio",
    "text": "4.3 Configure RStudio\nFirst watch an introduction to RStudio: https://www.youtube.com/watch?v=FIrsOBy5k58 and introduction to projects: https://www.youtube.com/watch?v=MdTtTN8PUqU\nNow the most crucial part!!! Change the default theme of RStudio from light to dark! To do this\n\nGo to the top panel\nSelect the Tools tab\nNavigate to Global Options\nSelect Appearance\nIn Editor Theme, select “Dracula”\nClick Apply\n\nNow let’s install a cooler font with ligatures! Install fira-code: https://github.com/tonsky/FiraCode/wiki/Installing. Restart RStudio for the font to load. Then go back to Appearance, choose FiraCode, and hit apply."
  },
  {
    "objectID": "chapters/setup.html#install-packages",
    "href": "chapters/setup.html#install-packages",
    "title": "4  Set up",
    "section": "4.4 Install Packages",
    "text": "4.4 Install Packages\nAn R package is a collection of useful functions, documentation, and data sets that can be used in your own R code after it is loaded. These packages typically focus on a specific task and make use of pre-written routines for various data science tasks.\nYou can install packages with a single line of code:\ninstall.packages(\"tidyverse\")\nYou can also install multiple packages at the same time using c():\ninstall.packages(c(\"tidyverse\",\"gapminder\"))\nYou can load packages using the library() function:\nlibrary(\"tidyverse\")\nRun the following command to install packages we will use in class\ninstall.packages(c(\"tidyverse\", \"janitor\", \"esquisse\", \"modelsummary\", \"styler\"))"
  },
  {
    "objectID": "chapters/data_manipulation.html#basics",
    "href": "chapters/data_manipulation.html#basics",
    "title": "5  Data Manipulation",
    "section": "5.1 Basics",
    "text": "5.1 Basics\nLet’s kick off with some fundamental concepts! R can be employed as a simple calculator.\n\n# A \"#\" is used to annotate comments!\n2 + 2\n\n[1] 4\n\n2 * 4\n\n[1] 8\n\n2^8\n\n[1] 256\n\n(1 + 3) / (3 + 5)\n\n[1] 0.5\n\nlog(10) # Calculates the natural log of 10!\n\n[1] 2.302585\n\n\nR allows for defining variables and performing operations on them. Both = or &lt;- can be used for assigning values to a variable name, though &lt;- is preferred to evade confusion and certain errors.\n\nx &lt;- 2 # Equivalent to x = 2\nx * 4\n\n[1] 8\n\n\nThe command x &lt;- 2 assigns the value 2 to x. Thus, when we subsequently type x * 4, R substitutes x with 2 to evaluate 2 * 4 and obtain 8. The value of x can be updated as needed using = or &lt;-. Bear in mind that R is case sensitive, so X and x are recognized as different variables.\n\nx\n\n[1] 2\n\nx &lt;- x * 5\n\n\n5.1.1 Data Types\nR possesses a multitude of data types and classes, including data.frames which are akin to Excel spreadsheets with columns and rows. Initially, we’ll examine vectors. Vectors can store multiple values of the same type, with the most basic ones being numeric, character, and logical.\n\nx\n\n[1] 10\n\nclass(x)\n\n[1] \"numeric\"\n\n\n\n1(name &lt;- \"Parsa Rahimi\")\nclass(name)\n\n\n1\n\nWrapping with (…) prints the variable\n\n\n\n\n[1] \"Parsa Rahimi\"\n[1] \"character\"\n\n\n\n(true_or_false &lt;- TRUE)\n\n[1] TRUE\n\nclass(true_or_false)\n\n[1] \"logical\"\n\n\nNote that name is stored as a single character string. If we want to store the name and surname separately in the same object, we can employ c() to concatenate objects of similar class into a vector.\n\n(name_surname &lt;- c(\"Parsa\", \"Rahimi\"))\n\n[1] \"Parsa\"  \"Rahimi\"\n\nlength(name)\n\n[1] 1\n\nlength(name_surname)\n\n[1] 2\n\n\nObserve that the length of name is 1 and that of name_surname is 2! Let’s create a numeric vector and perform some operations on it!\n\n(i &lt;- c(1, 2, 3, 4))\n1i + 10\n2i * 10\n3i + c(2, 4, 6, 8)\n\n\n1\n\nAdds 10 to each element\n\n2\n\nMultiplies each element by 10\n\n3\n\nAdds together elements in corresponding positions\n\n\n\n\n[1] 1 2 3 4\n[1] 11 12 13 14\n[1] 10 20 30 40\n[1]  3  6  9 12\n\n\nThe operations performed above don’t modify i. The results are merely printed, not stored. If we wish to save the results, we must assign them to a variable.\n\nname\n\n[1] \"Parsa Rahimi\"\n\nname &lt;- i + c(5, 4, 2, 1)\nname\n\n[1] 6 6 5 5\n\n\nNotice that name is no longer “Parsa Rahimi”. It was overwritten by a numeric vector rather than a character string. Make sure to perform numeric operations only on numeric objects to avoid errors. The str() function can be used to obtain the structure of the object, such as type, length, etc.\n\nname_surname + 2\n\nError in name_surname + 2: non-numeric argument to binary operator\n\nstr(name_surname)\n\n chr [1:2] \"Parsa\" \"Rahimi\""
  },
  {
    "objectID": "chapters/data_manipulation.html#downloading-data",
    "href": "chapters/data_manipulation.html#downloading-data",
    "title": "5  Data Manipulation",
    "section": "5.2 Downloading Data",
    "text": "5.2 Downloading Data\nIf you’re accustomed to Base R (i.e., functions that come with R upon installation), you may be aware of read.csv(). However, readr, a part of the Tidyverse package, offers functions that address common issues associated with Base R’s reading functions. read_csv not only loads data 10 times faster than read.csv, but it also produces a tibble instead of a data frame and evades inconsistencies of read.csv. You might be asking, “What exactly is a tibble?” A tibble is a special type of data frame with several advantages, such as faster loading times, maintaining input types, permitting columns as lists, allowing non-standard variable names, and never creating row names.\nTo load your data, you first need to know the path to your data. You can find your file, check its location, and then copy and paste it. If you are a Windows user, your path might contain “\\”, which is an escape character. To rectify this, replace “\\” with “/”. Copying the path gives you the absolute path (e.g., \"/Users/User/Documents/your_project/data/file.csv\"), but you can also use a local path from the folder of the project (e.g., \"/data/file.csv\"). Let’s read the data! Using readr:: specifies which package to use.\n\n5.2.1 Example Data\nIn this tutorial, I’ll be using a real sample from the Climate and Cooperation Experiment conducted in Mexico. During the experiment, subjects were asked to complete three series of Raven’s matrices, four dictator games, and a single lottery game in rooms with varying temperatures. We will primarily be using results from the Raven’s matrices games, which consist of 3 sets of 12 matrices. The first set, pr_, is the piece-rate round where participants received points for each correctly solved matrix. The second set, tr_, is the tournament round where participants competed against a random opponent. The winner received double points, and the loser received nothing. The third set, ch_, is the choice round where participants chose whether they wanted to play the piece-rate or tournament round against a different opponent’s score from the tournament round.\nThe data is located in the data directory of the GitHub repository. To access and manipulate it, we will require the tidyverse package. This package includes several sub-packages, including reader.\n\nlibrary(tidyverse)\n\n\n1data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nikitoshina/quarto_book/main/chapters/data/experiment_data.csv\")\n\n\n1\n\nDownload data from GitHub\n\n\n\n\nTo get a glimpse of the data, we can use the glimpse() function, which will provide us with a sample and the type of the column. Another common method is to use head() to get a slice of the top rows or tail() to get a slice of the bottom rows. To view the entire data set, use View().\n\ndata %&gt;% glimpse()\n\nRows: 114\nColumns: 7\n$ id                &lt;chr&gt; \"001018001\", \"001018002\", \"001018005\", \"001018009\", …\n$ mean_temp_celsius &lt;dbl&gt; 28.58772, 28.58772, 28.58772, 28.58772, 28.58772, 28…\n$ gender            &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", …\n$ pr_correct        &lt;dbl&gt; 7, 5, 6, 1, 7, 2, 6, 7, 5, 6, 8, 2, 2, 5, 6, 5, 2, 6…\n$ tr_correct        &lt;dbl&gt; 3, 6, 7, 5, 9, 7, 6, 7, 4, 7, 6, 3, 6, 7, 8, 6, 6, 8…\n$ ch_tournament     &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0…\n$ ch_correct        &lt;dbl&gt; 9, 4, 7, 7, 10, 5, 7, 6, 4, 4, 8, 6, 5, 6, 6, 6, 5, …"
  },
  {
    "objectID": "chapters/data_manipulation.html#basic-data-management-with-dplyr",
    "href": "chapters/data_manipulation.html#basic-data-management-with-dplyr",
    "title": "5  Data Manipulation",
    "section": "5.3 Basic Data Management With dplyr",
    "text": "5.3 Basic Data Management With dplyr\ndplyr uses a collection of verbs to manipulate data that are piped (chained) into each other with a piping operator %&gt;% from magrittr package. The way you use functions in base R is you wrap new function over the previous one, such as k(g(f(x))) this will become impossible to read very quickly as you stack up functions and their arguments. To solve this we will use pipes x %&gt;% f() %&gt;% g() %&gt;% k()! Now you can clearly see that we take x and apply f(), then g(), then k().\n\nNote: base R now also has its own pipe |&gt;, but we will stick to %&gt;% for compatibility across packages.\n\n\n5.3.1 select()\nselect() selects only the columns that you want, removing all other columns. You can use column position (with numbers) or name. The columns will be displayed in the order you list them. We will select subject_id, temperature, gender and results of raven’s matrices games.\n\nid is a unique subject identification number, where site_id.session_n.subject_n (001.001.001).\nmean_temp_celsius is mean temperature through the session\ngender is gender of the subject.\npr_correct is number of correct answers in piece-rate round.\ntr_correct is number of correct answers in tournament round.\nch_correct is number of correct answers in choice round.\nch_tournament is 1 if participant decided to play tournament and 0 if choice.\n\n\ndata_raven &lt;- data %&gt;% select(id, mean_temp_celsius, gender, pr_correct, tr_correct, ch_tournament, ch_correct)\ntail(data_raven)\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00102…              30.5 Female          6          8             1          6\n2 00102…              30.5 Male            6          5             0          3\n3 00102…              30.5 Male            6          6             1          4\n4 00102…              30.5 Female          6          7             0          6\n5 00102…              30.5 Female          6          7             0          5\n6 00102…              30.5 Male            4          7             0          7\n\n\nYou can also exclude columns or select everything else with select using -.\n\ndata_raven %&gt;%\n  select(-gender) %&gt;%\n  head()\n\n# A tibble: 6 × 6\n  id        mean_temp_celsius pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 001018001              28.6          7          3             1          9\n2 001018002              28.6          5          6             0          4\n3 001018005              28.6          6          7             0          7\n4 001018009              28.6          1          5             1          7\n5 001018010              28.6          7          9             1         10\n6 001018013              28.6          2          7             0          5\n\n\n\n\n5.3.2 filter()\nThe filter() function helps us keep only the rows we need, based on certain rules. For example, we use it here to make two separate groups of data: one for Males and another for Females. We use == to check one-to-one equality. Also, you can use symbols like &lt;, &lt;=, &gt;, &gt;=, and %in%. The %in% symbol is special – it checks if a value is part of a list.\n\ndata_male &lt;- data_raven %&gt;% filter(gender == \"Male\")\ndata_female &lt;- data_raven %&gt;% filter(gender == \"Female\")\nhead(data_male)\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              28.6 Male            5          6             0          4\n2 00101…              28.6 Male            6          7             0          7\n3 00101…              28.6 Male            1          5             1          7\n4 00101…              28.6 Male            7          9             1         10\n5 00101…              30.7 Male            6          7             1          4\n6 00101…              30.7 Male            5          6             1          6\n\nhead(data_female)\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              28.6 Female          7          3             1          9\n2 00101…              28.6 Female          2          7             0          5\n3 00101…              28.6 Female          6          6             0          7\n4 00101…              28.6 Female          7          7             0          6\n5 00101…              30.7 Female          5          4             0          4\n6 00101…              30.7 Female          8          6             1          8\n\n\nYou can chain multiple criteria. In the example below, we filter for Males with temperatures above 30 degrees Celsius and Females with temperatures below 30 degrees Celsius. We use & for “and” and | for “or”, and enclose the conditions in parentheses to avoid confusion.\n\ndata_raven %&gt;%\n  filter(\n    (gender == \"Male\" & mean_temp_celsius &gt; 30) | (gender == \"Female\" & mean_temp_celsius &lt; 30)\n  ) %&gt;%\n  head()\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              28.6 Female          7          3             1          9\n2 00101…              28.6 Female          2          7             0          5\n3 00101…              28.6 Female          6          6             0          7\n4 00101…              28.6 Female          7          7             0          6\n5 00101…              30.7 Male            6          7             1          4\n6 00101…              30.7 Male            5          6             1          6\n\n\n\n\n5.3.3 arrange()\nThe arrange() function orders the table using a variable. For example, to see the subject with the lowest score in pr_correct, we can use:\n\ndata_raven %&gt;%\n  arrange(pr_correct) %&gt;%\n  head()\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              28.6 Male            1          5             1          7\n2 00101…              28.6 Female          2          7             0          5\n3 00101…              30.7 Female          2          3             0          6\n4 00101…              30.7 Female          2          6             0          5\n5 00101…              30.7 Male            2          6             1          5\n6 00102…              30.4 Male            2          3             1          2\n\n\nTo sort in descending order, use the desc() modifier. For example, to find the subject with the highest score:\n\ndata_raven %&gt;%\n  arrange(desc(pr_correct)) %&gt;%\n  head()\n\n# A tibble: 6 × 7\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              30.7 Female          8          6             1          8\n2 00102…              31.6 Male            8          6             1          5\n3 00102…              31.6 Male            8          7             0          7\n4 00102…              30.4 Male            8          4             0          7\n5 00102…              26.8 Male            8          9             1          9\n6 00102…              32.2 Female          8          7             0          8\n\n\n\n\n5.3.4 mutate()\nThe mutate() function adds new columns or modifies existing ones in the dataset. For instance, you can create a dataset with 4 rows and add three new variable columns:\n\ntibble(rows = 1:4) %&gt;% mutate(\n  One = 1,\n  Comment = \"Something\",\n  Approved = TRUE\n)\n\n# A tibble: 4 × 4\n   rows   One Comment   Approved\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;   \n1     1     1 Something TRUE    \n2     2     1 Something TRUE    \n3     3     1 Something TRUE    \n4     4     1 Something TRUE    \n\n\nYou can use mutate() to create new variables using existing ones. For instance, you can convert Celsius to Fahrenheit, calculate the improvement in tournament scores over piece-rate round scores, and check the deviation from the mean score in the piece-rate round:\n\ndata_raven %&gt;% mutate(\n  mean_temp_fahrenheit = (mean_temp_celsius * 9 / 5) + 32,\n  impovement = tr_correct - pr_correct,\n  pr_deviation = pr_correct - mean(pr_correct)\n) %&gt;% \n  head()\n\n# A tibble: 6 × 10\n  id     mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 00101…              28.6 Female          7          3             1          9\n2 00101…              28.6 Male            5          6             0          4\n3 00101…              28.6 Male            6          7             0          7\n4 00101…              28.6 Male            1          5             1          7\n5 00101…              28.6 Male            7          9             1         10\n6 00101…              28.6 Female          2          7             0          5\n# ℹ 3 more variables: mean_temp_fahrenheit &lt;dbl&gt;, impovement &lt;dbl&gt;,\n#   pr_deviation &lt;dbl&gt;\n\n\nNotice how we can nest functions within mutate() to first calculate the mean of an entire column and then subtract it from pr_correct.\n\n\n5.3.5 case_match()\nThe case_match() function modifies values within a variable. For example, we can use case_match() to change “Male” to “M” and “Female” to “F”:\n\ndata_raven &lt;- data_raven %&gt;% mutate(gender = case_match(gender, \"Male\" ~ \"M\", \"Female\" ~ \"F\"))\n\n\n\n5.3.6 summarize()\nThe summarize() function reduces all rows into a one-row summary. It can be used to calculate the percentage of participants who were male, the median score in the piece-rate round, the maximum score in the tournament, the percentage of people choosing the tournament in choice round, and the mean score in the choice round.\nIn dplyr 1.0.0, reframe() was introduced. Unlike summarize(), reframe() can produce multiple row outputs. Use summarize() when expecting one row per group and reframe() for multiple rows.\n\ndata_raven %&gt;%\n  summarize(\n    prop_male = sum(gender == \"M\", na.rm = T) / n(),\n    pr_median = median(pr_correct),\n    tr_max = max(tr_correct),\n    ch_ratio = sum(ch_tournament) / n(),\n    ch_mean = mean(ch_correct)\n  )\n\n# A tibble: 1 × 5\n  prop_male pr_median tr_max ch_ratio ch_mean\n      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     0.482         5      9    0.456    6.01\n\n\n\n\n5.3.7 group_by()\nThe group_by() function groups data by specific variables for subsequent operations. By combining group_by() and summarize(), you can calculate different summary statistics for genders!\n\ndata_raven %&gt;%\n  drop_na(gender) %&gt;%\n  group_by(gender) %&gt;%\n  summarize(\n    pr_mean = mean(pr_correct),\n    tr_mean = mean(tr_correct),\n    ch_mean = mean(ch_correct),\n    pr_sd = sd(pr_correct),\n    n = n()\n  ) \n\n# A tibble: 2 × 6\n  gender pr_mean tr_mean ch_mean pr_sd     n\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 F         5.22    6.17    5.93  1.58    58\n2 M         5.47    6.4     6.09  1.59    55\n\n\nNow, let’s group by gender and choice in the choice round to look at points in the choice round!\n\ndata_raven %&gt;%\n  drop_na(gender) %&gt;%\n  group_by(gender, ch_tournament) %&gt;%\n  summarize(\n    ch_mean = mean(ch_correct),\n    pr_sd = sd(ch_correct),\n    n = n()\n  )\n\n# A tibble: 4 × 5\n# Groups:   gender [2]\n  gender ch_tournament ch_mean pr_sd     n\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 F                  0    5.73  1.70    33\n2 F                  1    6.2   1.73    25\n3 M                  0    6.07  1.69    29\n4 M                  1    6.12  2.25    26\n\n\n\n\n5.3.8 ungroup()\nThe ungroup() function removes grouping. Always ungroup your data after performing operations that required grouping to avoid confusion.\n\ndata_raven %&gt;%\n  drop_na(gender) %&gt;%\n  group_by(gender) %&gt;%\n  mutate(n = n()) %&gt;%\n  summarize(mean_male = mean(gender == \"M\")) %&gt;%\n  ungroup() \n\n# A tibble: 2 × 2\n  gender mean_male\n  &lt;chr&gt;      &lt;dbl&gt;\n1 F              0\n2 M              1\n\n\nNotice how mean_male (the ratio of males to the total) is 0 for Female and 1 for Male. That’s because the data was grouped, and we performed operations on Males and Females separately.\n\ndata_raven %&gt;%\n  drop_na(gender) %&gt;%\n  group_by(gender) %&gt;%\n  mutate(n = n()) %&gt;%\n  ungroup() %&gt;%\n  summarize(mean_male = mean(gender == \"M\")) \n\n# A tibble: 1 × 1\n  mean_male\n      &lt;dbl&gt;\n1     0.487\n\n\nThis time, we ungrouped the data before calculating the ratio, which gives us the correct result!\n\n\n5.3.9 .by\nGrouping is a commonly performed operation. Having to repeatedly group and ungroup for individual operations can lead to verbosity. To address this, dplyr introduced a convenient feature with version 1.0.0 — the .by argument within dplyr functions. This enhancement streamlines the process and reduces the need for excessive grouping and ungrouping.\n\n\n# A tibble: 113 × 8\n   id    mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 0010…              28.6 F               7          3             1          9\n 2 0010…              28.6 M               5          6             0          4\n 3 0010…              28.6 M               6          7             0          7\n 4 0010…              28.6 M               1          5             1          7\n 5 0010…              28.6 M               7          9             1         10\n 6 0010…              28.6 F               2          7             0          5\n 7 0010…              28.6 F               6          6             0          7\n 8 0010…              28.6 F               7          7             0          6\n 9 0010…              30.7 F               5          4             0          4\n10 0010…              30.7 M               6          7             1          4\n# ℹ 103 more rows\n# ℹ 1 more variable: n &lt;int&gt;\n\n\n\nsame as group_by %&gt;% mutate %&gt;% ungroup\n\n\n\n5.3.10 rowwise()\nThe rowwise() function allows for row-wise grouping. There may be situations where you want to perform a calculation row-wise instead of column-wise. However, when you try to perform the operation, you get an aggregate result. rowwise() comes to your rescue in such situations. Let’s create a dataframe with a column of lists and try to find the length of each list:\n\ndf &lt;- tibble(\n  x = list(1, 2:3, 4:6, 7:11)\n)\n\ndf %&gt;% mutate(length = length(x))\n\n# A tibble: 4 × 2\n  x         length\n  &lt;list&gt;     &lt;int&gt;\n1 &lt;dbl [1]&gt;      4\n2 &lt;int [2]&gt;      4\n3 &lt;int [3]&gt;      4\n4 &lt;int [5]&gt;      4\n\n\nIn the example above, instead of obtaining the lengths of the lists, we got the total number of rows in the dataset (the length of column x). Now, let’s use rowwise():\n\ndf %&gt;%\n  rowwise() %&gt;%\n  mutate(length = length(x))\n\n# A tibble: 4 × 2\n# Rowwise: \n  x         length\n  &lt;list&gt;     &lt;int&gt;\n1 &lt;dbl [1]&gt;      1\n2 &lt;int [2]&gt;      2\n3 &lt;int [3]&gt;      3\n4 &lt;int [5]&gt;      5\n\n\nWith rowwise(), R runs the length() function on each list separately, providing the correct lengths. Alternatively, you can use lengths(), which applies length() to each list.\n\nOr you can use lengths().\n\n\n\n5.3.11 count()\ncount() function in R is used for counting the number of rows within each group of values, similar to a combination of group_by() and summarize() functions. It can be used with a single column:\n\ndata_raven %&gt;% count(gender)\n\n# A tibble: 3 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 F         58\n2 M         55\n3 &lt;NA&gt;       1\n\n\nOr with multiple columns:\n\ndata_raven %&gt;% count(gender, ch_tournament)\n\n# A tibble: 5 × 3\n  gender ch_tournament     n\n  &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;\n1 F                  0    33\n2 F                  1    25\n3 M                  0    29\n4 M                  1    26\n5 &lt;NA&gt;               1     1\n\n\n\n\n5.3.12 rename()\nThe rename() function allows you to change column names, with the new name on the left and the old name on the right:\n\ndata_raven %&gt;%\n  rename(subject_id = id, sex = gender) %&gt;%\n  head(5)\n\n# A tibble: 5 × 7\n  subject_id mean_temp_celsius sex   pr_correct tr_correct ch_tournament\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 001018001               28.6 F              7          3             1\n2 001018002               28.6 M              5          6             0\n3 001018005               28.6 M              6          7             0\n4 001018009               28.6 M              1          5             1\n5 001018010               28.6 M              7          9             1\n# ℹ 1 more variable: ch_correct &lt;dbl&gt;\n\n\n\n\n5.3.13 row_number()\nrow_number() generates a column with consecutive numbers, which can be useful for creating a new id column. The following example first removes the id column, then creates a new one using row_number():\n\ndata_raven %&gt;%\n  select(-id) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  relocate(id) %&gt;%\n  head(5)\n\n# A tibble: 5 × 7\n     id mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n  &lt;int&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1     1              28.6 F               7          3             1          9\n2     2              28.6 M               5          6             0          4\n3     3              28.6 M               6          7             0          7\n4     4              28.6 M               1          5             1          7\n5     5              28.6 M               7          9             1         10\n\n\n\n\n5.3.14 skim()\nskim() from the skimr package provides an extensive summary of a data frame. It offers more than summary(), detailing quartiles, missing values, and histograms. Use it during exploratory data analysis to understand your data.\n\nlibrary(skimr)\nskim(data_raven)\n\n\nData summary\n\n\nName\ndata_raven\n\n\nNumber of rows\n114\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1.00\n9\n9\n0\n114\n0\n\n\ngender\n1\n0.99\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmean_temp_celsius\n0\n1\n30.63\n1.43\n26.84\n30.4\n30.69\n31.59\n32.3\n▁▁▂▇▆\n\n\npr_correct\n0\n1\n5.34\n1.57\n1.00\n5.0\n5.00\n6.00\n8.0\n▁▂▇▅▅\n\n\ntr_correct\n0\n1\n6.28\n1.35\n3.00\n6.0\n6.00\n7.00\n9.0\n▃▃▇▇▃\n\n\nch_tournament\n0\n1\n0.46\n0.50\n0.00\n0.0\n0.00\n1.00\n1.0\n▇▁▁▁▇\n\n\nch_correct\n0\n1\n6.01\n1.82\n2.00\n5.0\n6.00\n7.00\n10.0\n▃▃▅▇▁\n\n\n\n\n\nIt provides a neat, comprehensive view of each variable, useful for further analysis."
  },
  {
    "objectID": "chapters/data_manipulation.html#exploring-date-and-time-with-lubridate",
    "href": "chapters/data_manipulation.html#exploring-date-and-time-with-lubridate",
    "title": "5  Data Manipulation",
    "section": "5.4 Exploring Date and Time with lubridate",
    "text": "5.4 Exploring Date and Time with lubridate\nNavigating the complexities of dates, times, and timezones can be a daunting task, but fear not! The lubridate package, a recent addition to the tidyverse, comes to the rescue with its remarkable capabilities for simplifying date and time manipulations. With lubridate, you can effortlessly convert strings into dates, dates into strings, alter formats, check for overlaps, and perform date arithmetic.\n\n5.4.1 ymd(), md(), hms(), ymd_hms()\nWhen working with dates, various notations exist, each represented by a combination of letters. To extract dates from text, all you need is to discern the order of year, month, day, hours, minutes, and seconds, and then employ the corresponding function to work your magic.\n\nlibrary(lubridate)\nday_year_month &lt;- \"31/2001/01\"\n(date &lt;- dym(day_year_month))\n\nmonth_day_year_hour_minute &lt;- \"Jan 31st 2001 6:05PM\"\n(date_time &lt;- mdy_hm(month_day_year_hour_minute))\n\nsentence_with_date &lt;- \"Elizabeth II was Queen of the United Kingdom and other Commonwealth realms from 6 February 1952.\"\n1dmy(sentence_with_date)\n\n\n1\n\nImpressive, isn’t it? lubridate can identify the date within text and parse it for you!\n\n\n\n\n[1] \"2001-01-31\"\n[1] \"2001-01-31 18:05:00 UTC\"\n[1] \"1952-02-06\"\n\n\n\n\n5.4.2 year(), month(), day()\nSimilarly, you can extract specific date and time components using functions aptly named after the elements you wish to retrieve.\n\nday(date_time)\nhour(date_time)\n1month(date_time, label = TRUE, abbr = FALSE)\n\n\n1\n\nSetting label = TRUE provides the month’s name, while abbr = FALSE yields the full name.\n\n\n\n\n[1] 31\n[1] 18\n[1] January\n12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December"
  },
  {
    "objectID": "chapters/data_manipulation.html#summary",
    "href": "chapters/data_manipulation.html#summary",
    "title": "5  Data Manipulation",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nWe explored basic operations in R and delved into the key features of Tidyverse for data manipulation. This included learning how to load, manage, and handle basic date and time data. This chapter lays a solid foundation for beginners. Next, we will delve into what constitutes tidy data and how to organize data effectively."
  },
  {
    "objectID": "chapters/tidy_data.html#example",
    "href": "chapters/tidy_data.html#example",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.1 Example",
    "text": "6.1 Example\nTidy data is more than a theoretical concept; it has practical implications for data structuring. Consider this example of storing data on experiment payments:\n\nWhen a computer reads this data, it can’t understand our intent, so all columns are read as-is. Adding multiple days necessitates creating similar tables, increasing the chances of errors. Want a total for the entire experiment? We’d have to manually sum all cells. Now, compare this to:\n\nIn this format, data input is straightforward, and generating summary tables is as simple as creating a pivot table. A tidy data approach from the outset aids in creating robust tables and saves time during analysis."
  },
  {
    "objectID": "chapters/tidy_data.html#pivot_longer",
    "href": "chapters/tidy_data.html#pivot_longer",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.2 pivot_longer()\n",
    "text": "6.2 pivot_longer()\n\nA common problem arises in datasets where column names are not variable names, but values of a variable. This is the case for pr_correct, tr_correct, ch_correct, where the column names represent the game variable’s name. Meanwhile, the values in the columns represent the number of correct answers, and each row denotes two observations, not one.\n\n(data_raven &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nikitoshina/ECON-623-Lab-2023/main/data/mexico_sample_data.csv?token=GHSAT0AAAAAAB5WTPULI26TZP545VNUFQE6Y6O4XVA\") %&gt;% select(id, mean_temp_celsius, gender, pr_correct, tr_correct, ch_tournament, ch_correct))\n\n# A tibble: 114 × 7\n   id    mean_temp_celsius gender pr_correct tr_correct ch_tournament ch_correct\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 0010…              28.6 Female          7          3             1          9\n 2 0010…              28.6 Male            5          6             0          4\n 3 0010…              28.6 Male            6          7             0          7\n 4 0010…              28.6 Male            1          5             1          7\n 5 0010…              28.6 Male            7          9             1         10\n 6 0010…              28.6 Female          2          7             0          5\n 7 0010…              28.6 Female          6          6             0          7\n 8 0010…              28.6 Female          7          7             0          6\n 9 0010…              30.7 Female          5          4             0          4\n10 0010…              30.7 Male            6          7             1          4\n# ℹ 104 more rows\n\n\nTo tidy such a dataset, we need to pivot the problematic columns into a new pair of variables. This operation requires:\n\nThe columns whose names are values, not variables—columns we want to pivot. In this case, pr_correct, tr_correct, ch_correct.\nThe name of the variable where we’ll move the column names. Here, it’s game. The default is name.\nThe name of the variable where we’ll move the column values. Here, it’s n_correct. The default is value.\n\n\ndata_raven %&gt;%\n  pivot_longer(c(pr_correct, tr_correct, ch_correct), names_to = \"game\", values_to = \"n_correct\") %&gt;%\n  select(id, game, n_correct) %&gt;%\n  head(n = 5)\n\n# A tibble: 5 × 3\n  id        game       n_correct\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 001018001 pr_correct         7\n2 001018001 tr_correct         3\n3 001018001 ch_correct         9\n4 001018002 pr_correct         5\n5 001018002 tr_correct         6"
  },
  {
    "objectID": "chapters/tidy_data.html#pivot_wider",
    "href": "chapters/tidy_data.html#pivot_wider",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.3 pivot_wider()\n",
    "text": "6.3 pivot_wider()\n\npivot_wider() is the opposite of pivot_longer(). It is used when an observation is scattered across multiple rows. For instance, consider the data_raven_accident dataset, where mean_temp_celsius and ch_tournament are stacked. In this case, an observation is spread across two rows.\n\ndata_raven_accident %&gt;% head(n = 5)\n\n# A tibble: 5 × 3\n  id        name              value\n  &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;\n1 001018001 mean_temp_celsius  28.6\n2 001018001 ch_tournament       1  \n3 001018002 mean_temp_celsius  28.6\n4 001018002 ch_tournament       0  \n5 001018005 mean_temp_celsius  28.6\n\n\nTo tidy this up, we need two parameters:\n\nThe column to take variable names from. Here, it’s name.\nThe column to take values from. Here it’s value.\n\n\ndata_raven_accident %&gt;%\n  pivot_wider(names_from = name, values_from = value) %&gt;%\n  head(n = 5)\n\n# A tibble: 5 × 3\n  id        mean_temp_celsius ch_tournament\n  &lt;chr&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n1 001018001              28.6             1\n2 001018002              28.6             0\n3 001018005              28.6             0\n4 001018009              28.6             1\n5 001018010              28.6             1\n\n\nIt is evident from their names that pivot_wider() and pivot_longer() are inverse functions. pivot_longer() converts wide tables to a longer and narrower format, while pivot_wider() converts long tables to a shorter and wider format."
  },
  {
    "objectID": "chapters/tidy_data.html#separate-and-unite",
    "href": "chapters/tidy_data.html#separate-and-unite",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.4 separate() and unite()\n",
    "text": "6.4 separate() and unite()\n\nSometimes, data may come with columns united, necessitating us to separate() them to maintain tidy data. The original function was superseded in favor of separate_wider_position() and separate_wider_delim(), there is also separate_longer_*() version.\n\ndata_raven_sep %&gt;% head(n = 5)\n\n# A tibble: 5 × 2\n  id        `gender/pr_correct`\n  &lt;chr&gt;     &lt;chr&gt;              \n1 001018001 Female/7           \n2 001018002 Male/5             \n3 001018005 Male/6             \n4 001018009 Male/1             \n5 001018010 Male/7             \n\n\n\ndata_raven_sep %&gt;%\n  separate_wider_delim(col = \"gender/pr_correct\", delim = \"/\", names = c(\"gender\", \"pr_correct\")) %&gt;%\n  head(n = 5)\n\n# A tibble: 5 × 3\n  id        gender pr_correct\n  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;     \n1 001018001 Female 7         \n2 001018002 Male   5         \n3 001018005 Male   6         \n4 001018009 Male   1         \n5 001018010 Male   7         \n\n\nWhat if we have one column that has been split across multiple columns? Consider a situation where our subject ID code, composed of site_id, session_n, and subject_n, has been broken down into three separate columns. In such a scenario, we would need to unite() these columns back into one.\n\ndata_raven_uni %&gt;% head(n = 5)\n\n# A tibble: 5 × 4\n  site_id session_n subject_n gender\n  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt; \n1 001     018       001       Female\n2 001     018       002       Male  \n3 001     018       005       Male  \n4 001     018       009       Male  \n5 001     018       010       Male  \n\n\n\ndata_raven_uni %&gt;%\n  unite(c(site_id, session_n, subject_n), col = \"id\", sep = \"\") %&gt;%\n  head(n = 5)\n\n# A tibble: 5 × 2\n  id        gender\n  &lt;chr&gt;     &lt;chr&gt; \n1 001018001 Female\n2 001018002 Male  \n3 001018005 Male  \n4 001018009 Male  \n5 001018010 Male"
  },
  {
    "objectID": "chapters/tidy_data.html#tibble-and-tribble",
    "href": "chapters/tidy_data.html#tibble-and-tribble",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.5 tibble() and tribble()\n",
    "text": "6.5 tibble() and tribble()\n\nA tibble is a special kind of data frame in R. Tibbles are a modern re-imagining of the data frame, designed to be more friendly and consistent than traditional data frames. To create a tibble, we can use tibble(), similar to data.frame(). Here are some features that make tibbles unique:\n\nBy default, tibbles display only the first 10 rows when printed, making them easier to work with large datasets.\nThey use a consistent printing format, making it easier to work with multiple tibbles in the same session.\nTibbles have a consistent subsetting behavior, making it easier to select columns by name. When printed, the data type of each column is specified.\nSubsetting a tibble will always return a tibble, so you don’t need to use drop = FALSE, as you would with traditional data frames.\nMost importantly, tibbles can have columns that consist of lists.\n\nIn summary, Tibbles are a more modern and consistent version of data frames. They are less prone to errors and more readable, making them an excellent choice for data manipulation and exploration tasks.\n\ntibble(\n  x = c(1, 2, 3),\n  y = c(\"one\", \"two\", \"three\")\n)\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 one  \n2     2 two  \n3     3 three\n\n\nYou can also use tribble() to create a row-wise, readable tibble in R. This is especially useful when creating small tables of data. The syntax is as follows: tribble(~column1, ~column2), where the Row column — represents the data in a row by row layout.\n\ntribble(\n  ~x, ~y,\n  1, \"one\",\n  2, \"two\",\n  3, \"three\"\n)\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 one  \n2     2 two  \n3     3 three"
  },
  {
    "objectID": "chapters/tidy_data.html#janitor-clean-your-data",
    "href": "chapters/tidy_data.html#janitor-clean-your-data",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.6 janitor: Clean Your Data",
    "text": "6.6 janitor: Clean Your Data\nThe janitor package is designed to make the process of cleaning and tidying data as simple and efficient as possible. To learn more about the functions it provides, check out this vignette!\n\n6.6.1 clean_names()\n\nThe clean_names() function is used to clean variable names, especially those read in from Excel files using readxl::read_excel() and readr::read_csv(). It parses letter cases, separators, and special characters into a consistent format, converts certain characters like “%” to “percent” and “#” to “number” to retain meaning, and resolves issues of duplicate or empty names. It is recommended to call this function every time data is read.\n\n# Create a data.frame with dirty names\ntest_df &lt;- as.data.frame(matrix(ncol = 6))\nnames(test_df) &lt;- c(\n  \"camelCase\", \"ábc@!*\", \"% of respondents (2009)\",\n  \"Duplicate\", \"Duplicate\", \"\"\n)\ntest_df %&gt;% colnames()\n\n[1] \"camelCase\"               \"ábc@!*\"                 \n[3] \"% of respondents (2009)\" \"Duplicate\"              \n[5] \"Duplicate\"               \"\"                       \n\n\n\nlibrary(janitor)\ntest_df %&gt;%\n  clean_names() %&gt;%\n  colnames()\n\n[1] \"camel_case\"                  \"abc\"                        \n[3] \"percent_of_respondents_2009\" \"duplicate\"                  \n[5] \"duplicate_2\"                 \"x\"                          \n\n\n\n6.6.2 remove_empty()\n\nremove_empty() removes empty rows and columns, which is especially useful after reading Excel files.\n\ntest_df2 &lt;- data.frame(\n  numbers = c(1, NA, 3),\n  food = c(NA, NA, NA),\n  letters = c(\"a\", NA, \"c\")\n)\ntest_df2\n\n  numbers food letters\n1       1   NA       a\n2      NA   NA    &lt;NA&gt;\n3       3   NA       c\n\ntest_df2 %&gt;%\n  remove_empty(c(\"rows\", \"cols\"))\n\n  numbers letters\n1       1       a\n3       3       c\n\n\n\n6.6.3 remove_constant()\n\nremove_constant() drops columns from a data.frame that contain only a single constant value (with an na.rm option to control whether NAs should be considered as different values from the constant).\n\ntest_df3 &lt;- data.frame(cool_numbers = 1:3, boring = \"the same\")\ntest_df3\n\n  cool_numbers   boring\n1            1 the same\n2            2 the same\n3            3 the same\n\ntest_df3 %&gt;% remove_constant()\n\n  cool_numbers\n1            1\n2            2\n3            3\n\n\n\n6.6.4 convert_to_date() and convert_to_datetime()\n\nDo you remember loading data from Excel and seeing 36922.75 instead of dates? Well, convert_to_date() and convert_to_datetime() will convert this format and other date-time formats to actual dates! If you need more customization, check excel_numeric_to_date().\n\nconvert_to_date(36922.75)\n\n[1] \"2001-01-31\"\n\nconvert_to_datetime(36922.75)\n\n[1] \"2001-01-31 18:00:00 UTC\"\n\n\n\n6.6.5 row_to_names()\n\nrow_to_names() is a function that takes the names of variables stored in a row of a data frame and makes them the column names of the data frame. It can also remove the row that contained the names, and any rows above it, if desired.\n\ntest_df4 &lt;- data.frame(\n  x_1 = c(NA, \"Names\", 1:3),\n  x_2 = c(NA, \"Value\", 4:6)\n)\ntest_df4\n\n    x_1   x_2\n1  &lt;NA&gt;  &lt;NA&gt;\n2 Names Value\n3     1     4\n4     2     5\n5     3     6\n\nrow_to_names(test_df4, 2)\n\n  Names Value\n3     1     4\n4     2     5\n5     3     6"
  },
  {
    "objectID": "chapters/tidy_data.html#summary",
    "href": "chapters/tidy_data.html#summary",
    "title": "\n6  Tidy Data\n",
    "section": "\n6.7 Summary",
    "text": "6.7 Summary\nThis chapter introduced the concept of tidy data, focusing on its structured format where each variable corresponds to a column, each observation to a row, and each observational unit type to a table. We highlighted the distinctions between wide and long data formats, and delved into practical R tools for organizing data into this tidy structure, including an overview of tibbles and additional resources for data cleaning. Understanding these principles of tidy data is invaluable and will prove beneficial throughout your career. Next, we will shift our focus to relational data and the concept of joins!"
  },
  {
    "objectID": "chapters/data_bases.html#sec-relationships",
    "href": "chapters/data_bases.html#sec-relationships",
    "title": "\n7  Relational Databases\n",
    "section": "\n7.1 Relationship Types",
    "text": "7.1 Relationship Types\n\n7.1.1 One to One (1:1)\nIn a one-to-one relationship, a single row in the first table corresponds to just one row in the second table, and vice versa. For example, the relationship between Countries and their respective Capitals:\n\n\n\n\n\nG\n\n\ncluster_0\n\n\ncluster_1\n\n\nTable1\n\nCountryChinaFranceItaly\nTable2\n\nCapitalBeijingParisRome\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\n\n\n\n\n\n7.1.2 One to Many (1:M)\nIn a one-to-many relationship, a single row in the first table can be linked to multiple rows in the second table, but a row in the second table is related to only one row in the first table. For instance, one Professor can teach several Classes:\n\n\n\n\n\nG\n\n\ncluster_0\n\n\ncluster_1\n\n\nTable1\n\nProfessorHobbsDoe\nTable2\n\nClass690-02692-01405-01\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\n\n\n\n\n\n7.1.3 Many to Many (M:N)\nIn a many-to-many relationship, a single row in the first table can be associated with many rows in the second table, and similarly, a row in the second table can be associated with many rows in the first table. For example, multiple Students can be enrolled in multiple Classes:\n\n\n\n\n\nG\n\n\ncluster_0\n\n\ncluster_1\n\n\nTable1\n\nStudentJohnDavidCeleste\nTable2\n\nClass690-02692-01405-01\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w\n\n\nTable1:e-&gt;Table2:w"
  },
  {
    "objectID": "chapters/data_bases.html#the-concept-of-keys",
    "href": "chapters/data_bases.html#the-concept-of-keys",
    "title": "\n7  Relational Databases\n",
    "section": "\n7.2 The Concept of Keys",
    "text": "7.2 The Concept of Keys\nA Primary Key (PK) is a unique identifier for each row within a table; every table should possess a primary key. To establish relationships between tables, we integrate PKs from one table into another, where they become Foreign Keys (FKs). These FKs allow us to draw connections between related entities across different tables.\n\n\nKeys\n\n\n\n\n\nClassID (PK)\nProfessorID (FK)\nCredits\nLocation\n\n\n\n620-01\n1\n4\nLM-340\n\n\n623-01\n2\n2\nUM-102\n\n\n663-01\n2\n2\nLO-234\n\n\n\n\n\n\n\nProfessorID (PK)\nProfessor\n\n\n\n1\nArman\n\n\n2\nAlessandra"
  },
  {
    "objectID": "chapters/data_bases.html#types-of-joins",
    "href": "chapters/data_bases.html#types-of-joins",
    "title": "\n7  Relational Databases\n",
    "section": "\n7.3 Types of Joins",
    "text": "7.3 Types of Joins\nIn relational databases, a join operation is employed to merge two or more tables based on a related column between them. There are several types of joins. To exemplify how joins operate, we will use two tables: employees and projects. The employees table includes employee_id (Primary Key) and name, while the projects table contains project_id (Primary Key) and employee_id (Foreign Key).\n\nknitr::kable(employees, caption = \"Employees Table\")\nknitr::kable(projects, caption = \"Projects Table\")\n\n\n\n\nEmployees Table\n\nemployee_id\nname\n\n\n\n1\nJohn\n\n\n2\nJane\n\n\n3\nBob\n\n\n4\nAlice\n\n\n5\nTom\n\n\n\n\n\n\nProjects Table\n\nproject_id\nemployee_id\n\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n1\n\n\n5\n4\n\n\n6\n6\n\n\n\n\n\n\n\n\n7.3.1 Outer Joins\nOuter joins are utilized to return matched data and unmatched data from one or both tables, effectively creating a more comprehensive table.\n\n7.3.1.1 Left Join\nA left join retrieves all the rows from the left table and only the matched rows from the right table. Essentially, it enriches the left table with additional information.\n\n\nLeft Join Illustration\n\n\nleft_join_result &lt;- employees %&gt;%\n  left_join(projects, by = \"employee_id\")\n\n\n\n\nResult of Left Join\n\nemployee_id\nname\nproject_id\n\n\n\n1\nJohn\n1\n\n\n1\nJohn\n4\n\n\n2\nJane\n2\n\n\n3\nBob\n3\n\n\n4\nAlice\n5\n\n\n5\nTom\nNA\n\n\n\n\n\n\n7.3.1.2 Right Join\nA right join operates similarly to a left join, but it retrieves all the rows from the right table and only the matched rows from the left table.\n\n\nRight Join Illustration\n\n\nright_join_result &lt;- employees %&gt;%\n  right_join(projects, by = \"employee_id\")\n\n\n# echo: false\nknitr::kable(right_join_result, caption = \"Result of Right Join\")\n\n\nResult of Right Join\n\nemployee_id\nname\nproject_id\n\n\n\n1\nJohn\n1\n\n\n1\nJohn\n4\n\n\n2\nJane\n2\n\n\n3\nBob\n3\n\n\n4\nAlice\n5\n\n\n6\nNA\n6\n\n\n\n\n\n\n7.3.1.3 Full Join\nA full join returns all the rows from both tables, filling non-matching rows with null values. It essentially merges both tables.\n\n\nFull Join Illustration\n\n\nfull_join_result &lt;- employees %&gt;%\n  full_join(projects, by = \"employee_id\")\n\n\n\n\nResult of Full Join\n\nemployee_id\nname\nproject_id\n\n\n\n1\nJohn\n1\n\n\n1\nJohn\n4\n\n\n2\nJane\n2\n\n\n3\nBob\n3\n\n\n4\nAlice\n5\n\n\n5\nTom\nNA\n\n\n6\nNA\n6\n\n\n\n\n\n\n7.3.1.4 Inner Join\nAn inner join only returns the matched rows between two tables. Thus, only the rows that found a match in both tables will be retained.\n\n\nInner Join Illustration\n\n\ninner_join_result &lt;- employees %&gt;%\n  inner_join(projects, by = \"employee_id\")\n\n\n\n\nResult of Inner Join\n\nemployee_id\nname\nproject_id\n\n\n\n1\nJohn\n1\n\n\n1\nJohn\n4\n\n\n2\nJane\n2\n\n\n3\nBob\n3\n\n\n4\nAlice\n5\n\n\n\n\n\n\n7.3.2 Filtering Joins\n\n7.3.2.1 Anti Join\nAn anti join returns the rows from the left table that do not find a corresponding match in the right table, without adding any new columns to the output. It’s useful when you want to filter rows based on the absence of matching entries in the other table.\n\nanti_join_result &lt;- employees %&gt;%\n  anti_join(projects, by = \"employee_id\")\n# knitr::kable(anti_join_result, caption = \"Result of Anti Join\")\n\n\n\n\n\n\nResult of Anti Join\n\nemployee_id\nname\n\n\n5\nTom\n\n\n\n\n\nTom does not have a project assigned! Perhaps he could take on project 6?\n\n\n\n7.3.2.2 Semi Join\nA semi join is akin to an inner join in identifying matching rows between two tables. However, unlike an inner join, it does not add any new columns to the output. Instead, it filters the rows from the left table that have a corresponding match in the right table. You’d use a semi join when you want to filter the left table based on the presence of matching entries in the right table.\n\nsemi_join_result &lt;- employees %&gt;%\n  semi_join(projects, by = \"employee_id\")\n\n\n\n\nResult of Semi Join\n\nemployee_id\nname\n\n\n\n1\nJohn\n\n\n2\nJane\n\n\n3\nBob\n\n\n4\nAlice"
  },
  {
    "objectID": "chapters/data_bases.html#visualizing-databases",
    "href": "chapters/data_bases.html#visualizing-databases",
    "title": "\n7  Relational Databases\n",
    "section": "\n7.4 Visualizing Databases",
    "text": "7.4 Visualizing Databases\nWe’re going to Unified Modeling Language (UML) to visualize relationships in databases. This might be your first time hearing that there is a language behind diagrams. UML is a standard graphical notation to describe software designs. It is a powerful tools for planning, visualizing and documents your projects. There are different types of diagrams to depict structures, behaviors and interactions with the standard set of symbols and notation.\nUML coding tools like Mermaid and Graphviz are great options, but I find that drag-and-drop web applications such as LucidChart and Draw.io are more user-friendly. First, let’s introduce an entity, which is an object (place, person, thing) that we want to track in our database. In our case, these will be a customer, order, and product. Each entity possesses attributes, for example, a customer has customer_id, name, email, address, etc., and other entities also have a list of attributes. These entities and attributes are represented as rows and columns, respectively, in your tables. Tables can be interconnected, and these relationships are visualized by drawing a line between the tables. Cardinality is used to describe these relationships in numeric terms, akin to our discussion in Section 7.1.\n\n\nCardinality\n\nFor instance, let’s examine the relationship between a customer and an order. We ask: what is the relationship between a customer and an order? Using the min, max framework, what is the minimum and maximum number of orders a customer can have? A customer can have zero orders (min = 0) and an indefinite amount of orders (max = many). So, the relationship from customer to order is 0 or many. Now, let’s look in the opposite direction: how many customers can an order have? An order can have only one customer (min = 1, max = 1).\n\n\nCustomer-Order Relationship\n\nNext, let’s examine the relationship between an order and a product. An order can include one or many products, and each product can be in zero or many orders. The complete diagram would resemble the following:\n\n\nEntity-Relationship Diagram\n\nCreating such a diagram is recommended whenever you’re planning a project with a complex design. It clarifies the necessary tables and their relationships. You could also sketch a diagram whenever you’re unsure about a data set. If you’d like to delve deeper into this topic, check out the Lucid Software YouTube guides and Neso Academy’s Database Management Systems course."
  },
  {
    "objectID": "chapters/data_validation.html#manual-inspection",
    "href": "chapters/data_validation.html#manual-inspection",
    "title": "8  Optimizing Data Validation",
    "section": "8.1 Manual Inspection",
    "text": "8.1 Manual Inspection\nDespite the convenience of automation, remember that you can’t address what you’re not aware of. Sometimes, data may not be ready for immediate analysis and may require cleaning before validation. Therefore, it’s essential to manually open the files, inspect the tables and their values, and conduct preliminary exploratory analysis. This approach not only gives you a comprehensive understanding of the data at hand but can also save time in the long run by helping you avoid unfit data for analysis. Lastly, always verify your results by reviewing the output table, an important yet simple step to remain fully engaged with the raw data.\n\n\n\nhttps://xkcd.com/2582/"
  },
  {
    "objectID": "chapters/data_validation.html#handling-data-issues",
    "href": "chapters/data_validation.html#handling-data-issues",
    "title": "8  Optimizing Data Validation",
    "section": "8.2 Handling Data Issues",
    "text": "8.2 Handling Data Issues\n\n8.2.1 Base R\nIn the event of data issues, it’s crucial to stop the script execution and alert the user. Base R offers several functions to facilitate this. For instance, the is_numeric() function, along with its is_ counterparts, are traditional examples. Control flow functions like if, else, stop(), and logical operators prove to be quite useful. Lastly, don’t forget the duplicated(), unique(), and dplyr’s distinct() functions.\n\nx &lt;- c(1, 2, 3)\ndf &lt;- data.frame(x = c(x, 1), y = c(x * 2, 2))\n1if (!is.numeric(x)) stop(\"x is not numeric!\")\n2stopifnot(is.numeric(x))\n3if (!all(x &gt; 0)) stop(\"x contains non-positive values!\")\n4if (any(duplicated(x))) stop(\"x contains duplicated values!\")\n5if (nrow(dplyr::distinct(df)) != nrow(df)) warning(\"df has duplicated rows!\")\n\n\n1\n\nStops execution if x isn’t numeric.\n\n2\n\nHalts if x isn’t numeric.\n\n3\n\nStops the process if x contains non-positive values.\n\n4\n\nHalts execution if x has duplicates.\n\n5\n\nIssues a warning if df has duplicate rows.\n\n\n\n\n\n\n8.2.2 Assert Your Conditions\nassertr is an excellent package for tidyverse-compatible data validation. Rather than manually running checks, you can add an assert statement to verify your assumptions about the data. If the assumption holds, the code continues running; however, if it fails, an error is thrown and execution is halted.\nassertr provides functions such as verify(), assert(), insist(), and chain(), all of which set conditions your data must meet:\n\nverify(): This function checks whether a given logical condition holds true for the entire dataset. If not, it halts the execution and throws an error.\nassert(): This function applies a specific predicate function to selected columns in your data frame. The data passes validation only if all values in those columns satisfy the predicate function’s condition.\ninsist(): Similar to assert(), this function allows for specifying a proportion or number of values that must meet the predicate function condition.\nchain(): This function is used when you want to specify more than one predicate function in the same assert() or insist() call. The data passes validation only if all predicates are met.\n\nThe syntax of the package integrates smoothly into a typical dplyr pipeline. Here’s a brief example:\n\nlibrary(assertr)\nlibrary(dplyr)\n\n\n1\n\nVerify dataset isn’t empty.\n\n2\n\nAssert that specified Engine Types are present in row names.\n\n3\n\nEnsure that at least one ‘mpg’ value is within two standard deviations.\n\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(mtcars)\n\nmtcars %&gt;%\n  tibble::rownames_to_column() %&gt;%\n1  verify(nrow(.) &gt; 0) %&gt;%\n2  assert(in_set(0, 1), vs) %&gt;%\n3  insist(within_n_sds(2), mpg)\n\nError: assertr stopped execution\n\n\nColumn 'mpg' violates assertion 'within_n_sds(2)' 2 times\n    verb redux_fn       predicate column index value\n1 insist       NA within_n_sds(2)    mpg    18  32.4\n2 insist       NA within_n_sds(2)    mpg    20  33.9\n\n\nIn this example, verify() ensures mtcars has more than one row. assert() checks for the presence of certain row names, and insist() ensures at least one mpg value lies within two standard deviations. If any check fails, the pipeline stops and throws an error.\n“Helper” functions in assertr are predicate functions that return a logical true or false. These are used in conjunction with assert(), insist(), or verify(). Examples of helper functions include within_bounds(), not_na(), is.numeric(), in_set(), etc. You can also create and use your own predicate functions if needed.\nReplace your in-console data check with assertions, and if you’re not already conducting data checks, start now.\n\n\n8.2.3 Precise Validation with Pointblank\nThe pointblank package in R is specifically tailored for data validation. It’s designed with features to make data validation more reliable, better documented, interactive, and adaptable to various scenarios. Its key features include:\n\nAssertion functions: These functions allow setting quality standards for your data and halting execution if these standards are not met. They can check data types, value ranges, set memberships, distribution properties, etc.\nInformative interruptions: Pointblank provides detailed error messages when data issues arise, causing a halt in the R process.\nReport generation: A unique feature of Pointblank is that it creates comprehensive reports about the data validation process and its results.\nIntegration with dplyr and tidyverse: Pointblank complements the tidyverse suite of packages, especially dplyr, making it easy to apply data validation alongside data manipulation and visualization tools.\nAgent objects for ongoing validation: The concept of ‘agent’ objects is introduced for continuous data validation. An agent can hold various types of validation checks and be reused across different datasets.\n\nPointblank has a plethora of functionality built into it and is a great fit for important projects as it lets you create validation pipelines, HTML reports, and even distribute the reports to stakeholders. Below, we’ll create an agent and investigate the mtcars dataset. It’s worth noting that you’re not required to create an agent - you can use the functions similarly to assertr.\n\n\n\npoint blank workflow\n\n\n\n8.2.3.1 Step 1: Create a Validation Plan (an Agent)\n\nlibrary(pointblank)\n\nagent &lt;- create_agent(\n  tbl_name = \"a simple mtcars data validation\",\n  label = \"an example of using pointblank for data validation\",\n  tbl = mtcars # attach the data frame to validate\n)\n\n\n\n8.2.3.2 Step 2: Specify Checks\n\nagent &lt;- agent %&gt;%\n1  col_vals_gt(vars(mpg), value = 10) %&gt;%\n2  col_vals_lte(vars(hp), value = 335) %&gt;%\n3  col_exists(vars(vs, am)) %&gt;%\n4  col_vals_not_null(vars(cyl, gear))\n\n\n1\n\nCheck if ‘mpg’ values are greater than 10.\n\n2\n\nCheck if ‘hp’ values are less than or equal to 335.\n\n3\n\nCheck if ‘vs’ and ‘am’ columns exist.\n\n4\n\nCheck if ‘cyl’ and ‘gear’ columns have no missing values.\n\n\n\n\n\n\n8.2.3.3 Step 3: Execute Checks\n\ninterrogate(agent)\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Pointblank Validation\n    \n    \n      an example of using pointblank for data validation\ndata frame\na simple mtcars data validation\n\n    \n    \n      \n      \n      STEP\n      COLUMNS\n      VALUES\n      TBL\n      EVAL\n      UNITS\n      PASS\n      FAIL\n      W\n      S\n      N\n      EXT\n    \n  \n  \n    \n1\n\n      col_vals_gt                                                \n   col_vals_gt()\n\n\n\n  \n    ▮mpg\n  \n\n\n10\n\n                                                            \n\n✓\n\n32\n321.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n2\n\n      col_vals_lte                                                \n   col_vals_lte()\n\n\n\n  \n    ▮hp\n  \n\n\n335\n\n                                                            \n\n✓\n\n32\n321.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n3\n\n      col_exists                                                            \n   col_exists()\n\n\n\n  \n    ▮vs\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n4\n\n      col_exists                                                            \n   col_exists()\n\n\n\n  \n    ▮am\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n5\n\n      col_vals_not_null                                                                        \n   col_vals_not_null()\n\n\n\n  \n    ▮cyl\n  \n\n\n—\n                                                            \n\n✓\n\n32\n321.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n6\n\n      col_vals_not_null                                                                        \n   col_vals_not_null()\n\n\n\n  \n    ▮gear\n  \n\n\n—\n                                                            \n\n✓\n\n32\n321.00\n00.00\n—\n\n—\n\n—\n\n—\n\n  \n  \n    \n      2023-12-08 18:35:37 PST\n&lt; 1 s\n2023-12-08 18:35:37 PST"
  },
  {
    "objectID": "chapters/imputation.html#types-of-missing-data",
    "href": "chapters/imputation.html#types-of-missing-data",
    "title": "9  Imputation",
    "section": "9.1 Types of Missing Data",
    "text": "9.1 Types of Missing Data\nUnderstanding the type of missing data is crucial as it can guide the appropriate choice of imputation methods or handling strategies to minimize potential biases in the analysis.\n\nMissing Completely at Random (MCAR): The missingness in the data is entirely random, and the probability of missing data is the same for all observations, regardless of observed or unobserved values. MCAR is an ideal scenario, as missing data does not depend on any other variables in the dataset.\nMissing at Random (MAR): As mentioned earlier, MAR occurs when the missingness is related to observed variables in the dataset, but not to the unobserved values themselves. The probability of missing data depends on other observed variables but not on the unobserved values.\nMissing Not at Random (MNAR): In MNAR, the missingness is related to the unobserved values themselves. The probability of missing data depends on the values that are missing, leading to potential bias in the analysis.\nMissing by Design: Missing by design occurs when specific data points are intentionally missing, often as part of the study design or data collection process. In such cases, the missing data is systematic and has a purpose.\n\nBefore addressing missing data, it is often essential to distinguish between different types of missingness. Explicitly marking missing data as NA in R helps in identifying the patterns and dealing with the missing values appropriately."
  },
  {
    "objectID": "chapters/imputation.html#dealing-with-missing-data",
    "href": "chapters/imputation.html#dealing-with-missing-data",
    "title": "9  Imputation",
    "section": "9.2 Dealing with Missing Data",
    "text": "9.2 Dealing with Missing Data\nThere are several ways to handle missing data:\n\nKeep it: In some cases, it might be appropriate to retain the missing data if the missingness does not significantly affect the analysis.\nDrop it (Listwise Deletion or Complete Case Analysis): This approach involves removing rows with missing data. It’s simple and can be suitable if missing data is minimal and random. However, it can lead to information loss, potential bias, and reduced sample size. Despite these drawbacks, it’s a common method in quantitative research due to its simplicity.\nImpute it: Imputation involves estimating missing values based on observed data, which allows for a complete dataset and ensures that all cases are retained for analysis. However, if not done correctly will introduce bias.\nSet as Dummy or a Factor: Sometimes, missing values can be treated as a separate category. This can be done by creating a dummy variable or converting the variable into a factor. This approach acknowledges the missing data and incorporates it into the analysis as a distinct group.\n\nImputation, while a powerful tool, must be applied judiciously as it modifies the original dataset and can substantially influence analysis outcomes. It’s essential to comprehend the reasons behind the missing data, the assumptions involved, and the justification for using imputation in the specific context of your analysis. Imputation should never be a default choice, but a well-considered strategy.\n\n9.2.1 Explicitly Handling Missing Data with complete()\nWhen working with datasets, it’s crucial to understand that missing values aren’t always explicitly identifiable. These implicit absences can misleadingly suggest data completeness, emphasizing the necessity for appropriate identification and handling. The tidyr package’s complete() function offers a robust solution.\nThe complete() function generates a new dataframe featuring all potential combinations of specified columns, thereby assuring data comprehensiveness. This process, complemented by filling absent combinations with default values, facilitates precise analysis and mitigates the risk of implicit missing data.\nTake, for instance, a dataset tracking four students attending various classes over three days. Initially, this dataset might seem comprehensive. However, only the attending students were recorded, leaving a data gap for absentees.\nIn this case, we employ complete() to create complete_df, a new dataframe that encapsulates all conceivable combinations of student_id, day, and class. This method ensures accurate recording of each student’s attendance for every class on each day, irrespective of the initial data’s shortcomings. The “present” column’s missing values default to FALSE, clearly denoting unrecorded attendance.\n\n\nCode\n1df &lt;- tribble(\n  ~day, ~class, ~student_id, ~present,\n  1, \"English\", 1, T,\n  1, \"English\", 2, T,\n  1, \"English\", 4, T,\n  1, \"Science\", 2, T,\n  2, \"Math\", 1, T,\n  2, \"Math\", 2, T,\n  2, \"Math\", 4, T,\n  2, \"English\", 4, T,\n  2, \"English\", 1, T,\n  3, \"Math\", 1, T,\n  3, \"Math\", 2, T,\n  3, \"Math\", 1, T\n)\n\n\n\n1\n\ntribble() is a function to create a data frame in a readable format\n\n\n\n\n\n\n# A tibble: 12 × 4\n     day class   student_id present\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;lgl&gt;  \n 1     1 English          1 TRUE   \n 2     1 English          2 TRUE   \n 3     1 English          4 TRUE   \n 4     1 Science          2 TRUE   \n 5     2 Math             1 TRUE   \n 6     2 Math             2 TRUE   \n 7     2 Math             4 TRUE   \n 8     2 English          4 TRUE   \n 9     2 English          1 TRUE   \n10     3 Math             1 TRUE   \n11     3 Math             2 TRUE   \n12     3 Math             1 TRUE   \n\n\n\n1complete_df &lt;- df %&gt;%\n  complete(\n2    student_id = full_seq(student_id, 1),\n3    nesting(day, class),\n4    fill = list(present = FALSE),\n5    explicit = FALSE\n  ) %&gt;%\n6  arrange(day, class, student_id, present)\n\n\n1\n\nCreating a complete dataframe with all possible combinations of student_id and nested combinations of day and class\n\n2\n\nfull_seq() generates a sequence of student IDs, using the number of unique student IDs as the maximum value\n\n3\n\nnesting() creates only combinations that already exist in the data\n\n4\n\nFills missing attendance data with FALSE\n\n5\n\nLimit the fill to only the newly created (i.e. previously implicit)\n\n6\n\nOrders the resulting data frame by day, class, student_id, and present\n\n\n\n\n\n\n# A tibble: 21 × 4\n   student_id   day class   present\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;lgl&gt;  \n 1          1     1 English TRUE   \n 2          2     1 English TRUE   \n 3          3     1 English FALSE  \n 4          4     1 English TRUE   \n 5          1     1 Science FALSE  \n 6          2     1 Science TRUE   \n 7          3     1 Science FALSE  \n 8          4     1 Science FALSE  \n 9          1     2 English TRUE   \n10          2     2 English FALSE  \n11          3     2 English FALSE  \n12          4     2 English TRUE   \n13          1     2 Math    TRUE   \n14          2     2 Math    TRUE   \n15          3     2 Math    FALSE  \n16          4     2 Math    TRUE   \n17          1     3 Math    TRUE   \n18          1     3 Math    TRUE   \n19          2     3 Math    TRUE   \n20          3     3 Math    FALSE  \n21          4     3 Math    FALSE  \n\n\nBy using nesting() within complete(), we explicitly handle missing data, creating a comprehensive nested dataset suitable for further analysis. This approach guarantees that our analysis is based on a more complete and reliable dataset, providing accurate insights into student attendance across different classes and days.\n\n\n9.2.2 Simple Imputations\nTo explore different imputations we will use “airquality” dataset that contains daily measurements of air pollutants and weather conditions in New York City during a five-month period in 1973. It includes data on ozone concentration, solar radiation, temperature, wind speed, and relative humidity.\n\nhead(airquality)\nairquality &lt;- drop_na(airquality, Solar.R)\n1invisible(mice::md.pattern(airquality))\n\n\n1\n\nmd.pattern makes a matrix of missing values. invisible to only show plot, without matrix output.\n\n\n\n\n\n\nhead_na &lt;- function(data, n = 5) {\n  data %&gt;%\n    filter(is.na(Ozone)) %&gt;%\n    select(-Ozone) %&gt;%\n    head(n)\n}\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\n9.2.2.1 Fixed Value Imputation\nMissing values are replaced with a predetermined constant value. This method is simple and useful when you believe the fixed value reasonably represents the missing data.\n\n\n9.2.2.2 Mean and Median Imputation\nMissing values are replaced with the mean or median of the non-missing data in the same column. Using the mean is effective when data follow a normal distribution, but it can be affected by outliers, leading to biased imputations. In contrast, using the median is more robust to outliers and suitable for skewed or extreme data.\n\n\n9.2.2.3 Fill\nIn some scenarios, it’s rational to replace missing data with either preceding or succeeding values. This approach is particularly effective with datasets where the next available value logically substitutes the missing ones, such as in time-series or ordered data.\n\nairquality %&gt;%\n  arrange(Month, Day) %&gt;%\n  mutate(\n1    imp_fixed = replace(Ozone, is.na(Ozone), 0),\n2    imp_mean = replace(Ozone, is.na(Ozone), round(mean(Ozone, na.rm = TRUE), 2)),\n3    imp_median = replace(Ozone, is.na(Ozone), median(Ozone, na.rm = TRUE)),\n    imp_fill = Ozone,\n4    .keep = \"used\"\n  ) %&gt;%\n5  fill(imp_fill, .direction = \"down\") %&gt;%\n  head_na()\n\n\n1\n\nImpute with a fixed value (0 in this case).\n\n2\n\nImpute with the mean.\n\n3\n\nImpute with the median.\n\n4\n\nKeep only “used” columns.\n\n5\n\nFills missing values with the most recent non-missing value above.\n\n\n\n\n  imp_fixed imp_mean imp_median imp_fill\n1         0     42.1         31        8\n2         0     42.1         31       32\n3         0     42.1         31       32\n4         0     42.1         31       37\n5         0     42.1         31       37\n\n\nRemember that the choice of imputation method can significantly impact the results of your analysis. Always consider the nature of your data, the distribution of missingness, and the potential implications of each method before making a decision.\n\n\n9.2.2.4 K-Nearest Neighbors (KNN)\n\n# KNN imputation using the 'VIM' package\nlibrary(VIM)\nairquality_knn &lt;- kNN(airquality)\nhead_na(airquality_knn)\n\n [1] Solar.R     Wind        Temp        Month       Day         Ozone_imp  \n [7] Solar.R_imp Wind_imp    Temp_imp    Month_imp   Day_imp    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nKNN imputation can be a good choice when the data have a complex structure that simple methods can’t capture. It uses the relationships between variables to estimate missing values. However, it can be computationally intensive for large datasets.\n\n\n9.2.2.5 Maximum Likelihood\n\n# Maximum Likelihood using the 'norm' package\nlibrary(norm)\ns &lt;- prelim.norm(as.matrix(airquality))   #do preliminary manipulations\nthetahat &lt;- em.norm(s, showits = FALSE)   #find the mle\nrngseed(1337)   #set random number generator seed\nairquality_mlh &lt;- imp.norm(s,thetahat,airquality)\nhead_na(airquality_mlh)\n\n[1] Solar.R Wind    Temp    Month   Day    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nMaximum likelihood estimation can be a powerful method for imputing missing data, especially when the data are normally distributed. However, it makes strong assumptions about the data and can be complex to implement.\n\n\n9.2.2.6 Regression\n\n# Fit a linear regression model\nmodel &lt;- lm(Ozone ~ ., data = airquality)\n\n# Predict missing values\npredicted_values &lt;- predict(model, newdata = airquality)\n\n# Replace missing values with predicted values\nairquality %&gt;% \n  mutate(\n    Ozone = Ozone,\n    imp_lm = ifelse(is.na(Ozone), predicted_values, Ozone),\n    .keep = \"used\"\n  ) %&gt;% \n  head_na()\n\n      imp_lm\n1  35.446534\n2 -16.177404\n3   1.688479\n4  51.628995\n5  40.719713\n\n\nRegression imputation can be useful when there are relationships between variables that can be captured by a regression model. However, it can lead to underestimated variance and overestimated model fit.\n\n\n9.2.2.7 Forest\n\n# Install and load the missForest package\nlibrary(missForest)\n\nairquality %&gt;%\n  mutate(\n    Ozone = Ozone,\n    imp_forest = missForest(.)$ximp$Ozone,\n    .keep = \"used\"\n  ) %&gt;%\n  head_na()\n\n  imp_forest\n1   24.84667\n2   15.55000\n3   24.20333\n4   32.83000\n5   24.14000\n\n\nTree-based methods like Random Forests can handle complex data structures and can be a good choice when relationships between variables are non-linear or involve interactions. However, they can be computationally intensive and may not perform well with small sample sizes or sparse data.\nRemember, each of these methods has its own strengths and weaknesses, and the choice of method should be guided by the nature of your data and the specific requirements of your analysis. Always check the assumptions of the imputation method you’re using and consider the potential impact on your results.\n\n\n\n9.2.3 Multiple Imputations\nNow, listen closely, imputing can be better then dropping the data! Wait what? Yeh, because instead of dropping the data you preserve it! However, there is a method to this.\n(Jakobsen et al. 2017)\n\nJakobsen, Janus Christian, Christian Gluud, Jørn Wetterslev, and Per Winkel. 2017. “When and How Should Multiple Imputation Be Used for Handling Missing Data in Randomised Clinical Trials  a Practical Guide with Flowcharts.” BMC Medical Research Methodology 17 (1): 162. https://doi.org/10.1186/s12874-017-0442-1.\nMultiple imputation is a statistical technique that has been increasingly utilized since its inception in the early 1970s. It is a simulation-based method designed to address the issue of missing data in research studies. The process of multiple imputation is carried out in three main steps:\n\nImputation Step: In this initial stage, missing values in the dataset are identified and replaced with a set of plausible values, creating multiple completed datasets. These plausible values, or ‘imputations’, are generated based on a chosen imputation model. To reduce sampling variability from the imputation process, it is often preferable to generate 50 datasets or more.\nCompleted-Data Analysis (Estimation) Step: Once the imputed datasets are created, the desired analysis is performed separately on each dataset. For instance, if 50 datasets were generated during the imputation step, 50 separate analyses would be conducted.\nPooling Step: The results obtained from each completed-data analysis are then combined into a single multiple-imputation result. Each analysis result is considered to have the same statistical weight, so there is no need for a weighted meta-analysis.\n\nIt is crucial to ensure compatibility between the imputation model and the analysis model, or that the imputation model is more general than the analysis model. This means that the imputation model should include more independent covariates than the analysis model. For instance, if the analysis model includes significant interactions, then the imputation model should include them as well. Similarly, if the analysis model uses a transformed version of a variable, then the imputation model should use the same transformation.\n\n\n9.2.3.1 MICE (Multivariate Imputation by Chained Equations)\nThe mice package in R is a powerful tool for handling missing data through multiple imputation. It uses the Multivariate Imputation by Chained Equations (MICE) algorithm, which creates multiple imputations (replacement values) for multivariate missing data. The package creates multiple imputations by applying specified imputation methods to each missing value in an iterative process, also known as ‘chained equations’.\nHere’s a basic example of how to use the mice package:\nFirst, install and load the mice package:\n\nlibrary(mice)\n\n:::\nTODO: write about doing simple imputations\nMICE stands for Multivariate Imputation via Chained Equations, and it’s one of the most common packages for R users. It assumes the missing values are missing at random (MAR).\nThe basic idea behind the algorithm is to treat each variable that has missing values as a dependent variable in regression and treat the others as independent (predictors). You can learn more about MICE in this paper.\nThe R mice packages provide many univariate imputation methods, but we’ll use only a handful. First, let’s import the package and subset only the numerical columns to keep things simple. Only the Age attribute contains missing values:\nOnto the imputation now. We’ll use the following MICE imputation methods:\n\npmm: Predictive mean matching.\ncart: Classification and regression trees.\nlaso.norm: Lasso linear regression.\n\n:::\n\nairquality %&gt;% \n  mutate(\n  Ozone = Ozone,\n  imp_pmm = complete(mice(., method = \"pmm\", printFlag = FALSE))$Ozone,\n  imp_cart = complete(mice(., method = \"cart\", printFlag = FALSE))$Ozone,\n  imp_lasso = complete(mice(., method = \"lasso.norm\", printFlag = FALSE))$Ozone,\n  .keep = \"used\"\n) %&gt;%\n  head_na()\n\n  imp_pmm imp_cart  imp_lasso\n1      23       14  52.038597\n2      14        8 -47.142648\n3      37       32  -1.233898\n4      16       23  18.091410\n5      31       21  68.145293\n\n\nAssuming you have a dataset data with missing values, you can use mice() function to perform multiple imputation:\n\n# Perform multiple imputation\nimp &lt;- mice(airquality, m = 50, method = \"pmm\", seed = 1337, printFlag = FALSE)\n\n# m=5 specifies the number of multiple imputations (i.e., the number of complete datasets to generate)\n# method='pmm' specifies predictive mean matching method for imputation\n# seed=500 for reproducibility\n\nThe mice function supports several imputation methods. The choice of method depends on the nature of variables. Here are a few commonly used methods:\n\n'pmm': Predictive mean matching. Useful for numeric data.\n'logreg': Logistic regression. Useful for binary data.\n'polyreg': Polytomous logistic regression. Useful for ordered categorical data.\n'polr': Proportional odds model. Useful for ordered categorical data.\n\nAfter imputation, you can analyze each dataset separately. For example, if you want to fit a linear regression model and summarize the pooled results:\n\n\n         term     estimate   std.error statistic        df      p.value\n1 (Intercept) -56.70021557 24.82939110 -2.283593  77.04180 2.515150e-02\n2     Solar.R   0.04581614  0.02211941  2.071310 120.16600 4.047030e-02\n3        Wind  -3.28972276  0.71106796 -4.626453  70.93165 1.631604e-05\n4        Temp   1.85926001  0.27437473  6.776353  98.55404 9.089756e-10\n5       Month  -3.03408758  1.54608599 -1.962431 103.03587 5.240904e-02\n\n\nYou can also use gtsummary to create a summary table, here tbl_regression automatically pools results.\n\nlibrary(gtsummary)\ntbl_regression(fit)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    Solar.R\n0.05\n0.00, 0.09\n0.040\n    Wind\n-3.3\n-4.7, -1.9\n&lt;0.001\n    Temp\n1.9\n1.3, 2.4\n&lt;0.001\n    Month\n-3.0\n-6.1, 0.03\n0.052\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nRemember, the imputation model should be compatible with the analysis model. If the analysis model includes interactions or transformations, they should be included in the imputation model as well.\nAlso, it’s important to note that the mice package assumes that the missing data are Missing At Random (MAR), which means that the probability of a value being missing depends only on observed data and not on unobserved data. If this assumption is violated, the results of the imputation might be biased.\nImputation is a valuable tool for handling missing data, but it should be used judiciously and based on a thorough understanding of the data and the analysis objectives. Responsible imputation ensures that any assumptions made during the process align with the data generation process and results in more reliable and meaningful data analysis.\n\n\n9.2.3.2 Table of Imputations"
  },
  {
    "objectID": "chapters/repres.html#literate-programming",
    "href": "chapters/repres.html#literate-programming",
    "title": "10  Reproducible Research",
    "section": "10.1 Literate Programming",
    "text": "10.1 Literate Programming\n\n\n\n\n\n\n\nflowchart TD\n    A[Collected Data] --&gt;|Processing Code| B(Analytic Data)\n    B --&gt; |Analysis Code| C(Computational Results)\n    C --&gt; D{Presentation Code}\n    D --&gt; G[Figures]\n    D --&gt; E[Tables]\n    D --&gt; F[Summaries]\n\n    G --&gt; H[Article]\n    E --&gt; H\n    F --&gt; H\n\n    I[Text] --&gt; H\n\n\nFigure 10.1: Literate Programming Flow\n\n\n\n\nLiterate programming integrates text and code chunks, creating a seamless blend of human-readable explanations and machine-executable code. The code loads data, generates graphs, and runs models, while the text provides context and interprets the results. This approach enables researchers to produce documents that are both human- and machine-readable.\nOne of the earliest implementations of literate programming was Sweave, which combined LaTeX and R for documentation and programming. Since then, the field has evolved with the introduction of RMarkdown, Jupyter Notebooks, Python Markdown, etc. The latest development in this area is Quarto, which will be covered in this book. This tool continues to advance the concept of literate programming, offering researchers a comprehensive solution for creating transparent, reproducible, and well-documented research outputs.\n\n10.1.1 Example: Customer Satisfaction Survey Analysis\nLet’s explore how Figure 10.1 can be applied to a hypothetical customer satisfaction survey analysis project. This project aims to assess customer satisfaction through a survey and effectively communicate the findings. Here’s an overview of the workflow with detailed steps:\n\nCollected Data: Gather survey responses on customer satisfaction, preferences, and feedback. Ensure to retain the raw data for reference.\nProcessing Code: Develop a separate code to clean the collected data, addressing inconsistencies, missing values, and irrelevant entries. Document assumptions and processes meticulously, avoiding opinionated methods that may influence results.\nAnalytic Data: The cleaned data serves as the basis for analysis, providing accurate insights. Share this cleaned data with stakeholders.\nAnalysis Code: Utilize various analytical techniques to derive meaningful insights from the clean data, unveiling trends, patterns, and correlations. Document this analysis code comprehensively for publication.\nComputational Results: Generate computational results highlighting key findings such as average satisfaction scores, common complaints, and customer segments.\nPresentation Code: Develop code to create visualizations, charts, and graphs that effectively communicate survey results to stakeholders and other analysts:\n\nFigures: Visual representations illustrate trends and distributions, aiding in the understanding of Net Promoter Scores over time and customer segment distribution.\nTables: Use tables to showcase snippets of data, including summaries and visual information.\nSummaries: Summarize significant findings from tables, such as regression summaries and five-number summaries1.\n\nText: Incorporate descriptive text to provide context and explanations for visual and tabular components.\nArticle: Leverage the insights extracted to compose a comprehensive article or report on customer satisfaction, encompassing trends, analysis, and actionable recommendations.\n\n1 Minimum, First Quartile (Q1), Median (Q2), Third Quartile (Q3), MaximumRemember that while this is a simplified representation, real projects may involve more intricate steps and considerations."
  },
  {
    "objectID": "chapters/renv.html#renv",
    "href": "chapters/renv.html#renv",
    "title": "11  Reproducible Environment",
    "section": "11.1 renv",
    "text": "11.1 renv\nrenv enhances the isolation, portability, and reproducibility of R projects.\n\nIsolated: Each project gets its private library, preventing cross-project package conflicts.\nPortable: Projects can be transported across computers and platforms with ease. renv simplifies package installation.\nReproducible: renv documents exact package versions to ensure consistency across different environments.\n\n\n11.1.1 Workflow\n\n\n\nrenv workflow\n\n\nIntegrating renv into an existing project or initiating a new project is straightforward. Run renv::init() to initialize the environment, create a lock file ‘renv.lock’, and manage dependencies. Use renv::snapshot() to record the current state in the lockfile.\n\nrenv excludes files mentioned in your .gitignore.\n\nTo share code or run it in a new environment, use renv::restore() to reinstall packages from the lockfile. Incorporating reproducible environments into your projects is highly recommended. As your comfort with renv grows, explore its documentation and learn more about the package’s inner workings and additional functions.\nAnd if you have any doubts about setting up a reproducible environment. After spending 10+ hours hunting for bugs resulting from package updates, any doubts will vanish."
  },
  {
    "objectID": "chapters/modular_code.html#reuse-functions",
    "href": "chapters/modular_code.html#reuse-functions",
    "title": "12  Modular Code",
    "section": "12.1 Reuse Functions",
    "text": "12.1 Reuse Functions\nOne of the easiest steps to improve your code and make it easier to maintain and understand is to put all the repeating actions into functions. While it might be tempting to break your code down into small functions for each tiny task, such as creating separate functions for mean, standard deviation, t-score, and p-score calculations, the rule of thumb should be to focus on the goal of a function.\nIf you find that you need to reuse a portion of the larger function, it’s a good practice to go ahead and split it into a separate function. By abstracting your code into functions, you gain the benefits of code reusability and improved modularity. Additionally, abstracting code into functions helps break down deeply nested code into more manageable pieces, making it easier to read and understand.\nThe goal is to strike a balance between creating functions that serve a specific purpose and avoiding an excessive number of small functions. The purpose-driven approach ensures that each function communicates its intended goal effectively, leading to improved code readability. By abstracting your code and organizing it into purposeful functions, you’ll achieve code that is easier to comprehend and maintain.\nIf this reminds you of Functional Programming (FP) you are correct. If you are doing something more then a few times it likely belongs into a function.\nWe want to calculate mean, standard deviation, on our a multiple datasets. We could run mean and sd on each separately, but lets define a single function.\nBenefits of encapsulating code into functions for reusability and abstraction. Example: Creating a function to calculate the mean and standard deviation of a numeric vector.\n\ncalculate_stats &lt;- function(data) {\n  mean_value &lt;- mean(data)\n  sd_value &lt;- sd(data)\n  return(list(mean = mean_value, sd = sd_value))\n}"
  },
  {
    "objectID": "chapters/modular_code.html#split-it",
    "href": "chapters/modular_code.html#split-it",
    "title": "12  Modular Code",
    "section": "12.2 Split It",
    "text": "12.2 Split It\nOnce your file exceeds 100+ lines it makes sense to separate it into multiple files based on the function. This will drastically improve your code’s readability. You can use source to load the files, creating separate modules for data loading, data preprocessing, and data visualization.\nYou can run these scripts separately, for instance you first run a load_data script that saves the loaded data and then run the data_preprocessing and data_visualization scripts. Or you can source the files and use the functions in the main file. Additionally, you can load the scripts into the same local environment with local = TRUE.\n\n# data_loading.r\nload_data &lt;- function(file_path) {\n  # Code to load data from a file\n}\n\n# data_preprocessing.r\npreprocess_data &lt;- function(data) {\n  # Code to preprocess data\n}\n\n# data_visualization.r\nvisualize_data &lt;- function(data) {\n  # Code to visualize data\n}\n\n\nsource(data_loading.r, local = TRUE)\nsource(data_preprocessing.r, local = TRUE)\nsource(data_visualization.r, local = TRUE)\n\nNow let’s delve into the functionality of the source command in R, particularly when dealing with multiple files. The example shows the process through snippets encompassed in distinct files: “name.csv” holds a single entry, “Dima”; “load_data.r” loads the data from the csv; “add_title.r” defines the add_title function, combining a name and a title; “say_hi.r” defines say_hi, appending “Hi” to a name; and “main.r” utilizes the source command to load these modules and showcases their utilization in a sequence of chained operations. Ultimately, the code yields the output “Hi Mr. Dima,” highlighting how source integrates functions across multiple files.\n\n# name.csv\n1Dima\n\n\n1\n\nCSV file named “name.csv” with a single entry, “Dima”.\n\n\n\n\n\n# load_data.r\n1my_name &lt;- read.csv(\"name.csv\", header = FALSE)\n\n\n1\n\nRead name.csv and get the name of the column (name is stored and column name).\n\n\n\n\n\n# add_title.r\nsource(\"load_data.r\")\n1add_title &lt;- function(name, title) {\n  return(paste(title, name))\n}\n\n\n1\n\nDefines a function called add_title, which concatenates a title and name to create a new string.\n\n\n\n\n\n# say_hi.r\n1say_hi &lt;- function(name) {\n  return(paste(\"Hi\", name))\n}\n\n\n1\n\nDefines a function called say_hi, which concatenates a name and “Hi”.\n\n\n\n\n\n# main.r\n1source(\"say_hi.r\")\nsource(\"add_title.r\")\n\ntitle &lt;- \"Mr.\"\n\nadd_title(myName, title) |&gt;\n  say_hi() |&gt;\n  print()\n\n\n1\n\nNamed “main.r,” loads the say_hi and add_title modules from their respective files using the source function.\n\n\n\n\n\n\n[1] \"Hi Mr. Dima\""
  },
  {
    "objectID": "chapters/modular_code.html#box-it",
    "href": "chapters/modular_code.html#box-it",
    "title": "12  Modular Code",
    "section": "12.3 box It",
    "text": "12.3 box It\nThe ‘box’ system in R allows you to convert regular R files into reusable modules, enabling simplified code sharing without packaging. You can export functions by placing the #' @export directive before the function’s name. For instance:\n\n# say_hi.r\n#' @export\nsay_hi &lt;- function(name) {\n  return(paste(\"Hi\", name))\n}\n\nThe box::use function serves as a universal import declaration, superseding traditional library or require functions in base R. It provides more explicit and flexible loading of packages or modules, reducing errors.\nFor example, instead of loading entire libraries:\n\nlibrary(ggplot2)\n\nYou’d use:\n\nbox::use(ggplot2[...])\n\nBox::use allows specific function loading, which avoids name clashes and improves traceability of function origin. It allows aliasing to assign a different name to a package or its functions when imported:\n\n# load_data.r\nbox::use(\n1  utils[read = read.csv]\n)\n\n#' @export\nmy_name &lt;- read(\"name.csv\", header = FALSE) \n\n\n1\n\nLoad read.csv function from utils and rename it to read\n\n\n\n\n\n# add_title.r\n#' @export\nadd_title &lt;- function(name, title) {\n  return(paste(title, name))\n}\n\nThese modules can be stored in a central location or within individual projects and can be imported and used with the box::use function, like:\n\n# main.r\n1box::use(\n    ./say_hi,\n    ./add_title,\n    ./load_data[my_name]\n)\n\nbox::use(\n2  stringr[str_split_1]\n)\n\ntitle &lt;- \"Mr.\"\n\n3add_title$add_title(my_name, title) |&gt;\n  say_hi$say_hi() |&gt;\n4  str_split_1(pattern = \" \")\n\n\n1\n\nLoad the files\n\n2\n\nLoad function from stringr package.\n\n3\n\nAccess the function and the variable using module$variable.\n\n4\n\nstr_split_1 splits a string by space and returns a character vector.\n\n\n\n\n\n\n[1] \"Hi\"   \"Mr.\"  \"Dima\" \"!\"   \n\n\nModules in box share similarities with R packages, but are simpler to create and use, and offer features such as hierarchical nesting.\nbox is an amazing package that will greatly aid you on larger projects it is also a part of rhino shiny framework. There is a lot more to it, so make sure to read the official documentation."
  },
  {
    "objectID": "chapters/linter_styler.html#tidyverse-style",
    "href": "chapters/linter_styler.html#tidyverse-style",
    "title": "\n13  Style and Lint your Code\n",
    "section": "\n13.1 Tidyverse Style",
    "text": "13.1 Tidyverse Style\nWhile immersing yourself in coding, you might neglect aesthetics like proper indentation or line breaks, which could make your code less readable and more error-prone. It’s crucial to adopt certain coding practices to enhance readability and minimize errors. To help with this, we will explore R code conventions and introduce styler and lintr, packages that improve code aesthetics and detect potential errors.\nA good coding style, much like correct punctuation, significantly enhances readability. It’s important to remember that while some guidelines improve usability, others may be subjective. However, their true value lies in fostering consistency, which simplifies the coding process.\nTwo R packages, styler and lintr, support this style guide by offering interactive restyling of code and automated style checks, respectively.\n\n13.1.1 White Spaces and Indentation\nAdhere to a two-space indentation to illustrate the structure and hierarchy in your code. Function contents should also follow this two-space rule. For functions with pipes, start a new line for each pipe and indent them accordingly, ensuring clarity and readability.\n\n\nGood\n\nfor (i in 1:10) {\n  print(i)\n}\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  ungroup() %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(value)\n\n\n\n\nBad\n\nfor (i in 1:10){\nprint(i)}\n\niris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;%\nungroup %&gt;% gather(measure, value, -Species) %&gt;%\narrange(value)\n\n\n\n\n13.1.2 Naming Conventions\nR has a unique operation &lt;- used for assigning variables. You should always use &lt;- as an assignment operator over = (yes, Python enthusiasts, I’m looking at you). One of the hardest things in all of programming is naming your variables. In R, we use snake_case over camelCase (JavaScript enthusiasts, take note) or anything else. For variables, use nouns and don’t hesitate to make the names a bit longer. Aim for the perfect balance where your code can be readable yet concise. For functions, always use verbs, since functions perform actions.\n\n\nGood:\n\naverage_height &lt;- 1.70\nadd_row()\npermute()\n\n\n\n\nBad:\n\naverageHeight = 1.70\nAverageHeight = 1.70\nrow_adder()\npermutation()\n\n\n\n\n13.1.3 Use of Braces\nUtilize braces {} in your R code strategically to boost readability, grouping together commands that operate in tandem.\nFor if-else statements, place the opening brace { on the same line as the condition, and position the closing brace } on a new line. Ensure that the else statement shares a line with the closing brace of the preceding if section. This configuration facilitates easy identification of distinct code blocks.\n\n\nGood\n\nif (condition) {\n  action1()\n} else {\n  action2()\n}\n\n\n\n\nBad\n\nif (condition) \n{\naction1()\n}\nelse \n{\naction2()\n}\n\n\n\n\n13.1.4 Comments\nCommenting your code is a critical practice that can save you time and prevent confusion later. Even though your code should aim to be self-explanatory, comments provide invaluable context about the reasoning behind your code and help document key findings and decisions in data analysis.\nUse # and a space to write a comment. Stick to sentence case and only use a full stop at the end if your comment spans multiple sentences. Begin each line of the comment with # and a space.\nIf you find you need many comments to explain your code, consider refining it for clarity. A code that requires an excess of comments may be better suited to a verbose platform, such as Quarto.\n\n\nGood:\n# Calculate average height - this metric is used for normalization\n\n\n\nBad:\n# We are finding the average height\n\n#We will sum all of the heights and divide by the number of heights\n\n\n\n13.1.5 Long Functions\nAim to confine your code within 80 characters per line. This fits well on a standard printed page using a readable font size. If you find your code exceeding this limit, consider it a hint to encapsulate some of your code into a separate function.\nFor function calls that extend beyond a single line, allocate separate lines for the function name, each of its arguments, and the closing parenthesis. This practice enhances readability and future edits.\nGood\n\nlong_function_name &lt;- function(argument1,\n                               argument2,\n                               argument3,\n                               argument4) {\n  # function body\n}\n\nBad\n\nlong_function_name &lt;- function(argument1, argument2, argument3, argument4) {\n  # function body\n}\n\nThe R packages styler and lintr can automatically enforce these guidelines, making them beneficial tools for learning these conventions.\nThe whole style guide should take longer then 20 minutes to read, so please go through it: tidyverse style guide"
  },
  {
    "objectID": "chapters/linter_styler.html#styler",
    "href": "chapters/linter_styler.html#styler",
    "title": "\n13  Style and Lint your Code\n",
    "section": "\n13.2 Styler",
    "text": "13.2 Styler\nStyler, an R package, formats your code in line with the tidyverse style guide. It’s straightforward to install using install.packages(\"styler\"). After installation, you’ll find a new function in the Addins menu at the top of your RStudio window. It’s advisable to set a keyboard shortcut, such as “Command + Shift + a”, for the “Style Selection” function for easy access.\nBefore\n\niris %&gt;% select(Sepal.Length, Sepal.Width, Species) %&gt;% ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()\n\nAfter\n\niris %&gt;%\n  select(Sepal.Length, Sepal.Width, Species) %&gt;%\n  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +\n  geom_point()"
  },
  {
    "objectID": "chapters/linter_styler.html#linter",
    "href": "chapters/linter_styler.html#linter",
    "title": "\n13  Style and Lint your Code\n",
    "section": "\n13.3 Linter",
    "text": "13.3 Linter\nStatic code analysis is a method of debugging by examining the code without running it. In the context of R programming, linters are tools that perform static code analysis. The primary purpose of a linter is to catch potential errors, enforce coding standards, and to ensure consistent style across a project.\nR’s lintr package is a static code analysis tool that flags style, coding and syntax issues. Here’s a brief explanation of these three components:\n\nStyle Issues: lintr checks if the code adheres to the styling guidelines, such as those defined in the Tidyverse Style Guide. It checks for correct indentation, line lengths, usage of spaces around operators, and more.\nCoding Issues: lintr looks for potential coding errors and suboptimal code, such as usage of undefined variables, usage of = instead of &lt;- for assignments, or the presence of TODO comments, etc.\nSyntax Issues: lintr checks for any syntax errors in the code, like missing parentheses or incorrect usage of language keywords.\n\nStatic code analysis with lintr can be performed automatically within certain development environments, such as RStudio, or it can be invoked manually from the R console. It greatly helps in improving code quality, readability, and maintainability.\ninstall.packages(\"lintr\") lintr::lint(filename = \"R/bad.R\") Read the documentation for more information: lintr.\n\nStyler and Linter might appear similar, but they have different functions. Styler is faster and tidies up your code for better readability, while Linter is a bit slower but essential for spotting and avoiding errors in your code. So, Styler makes your code look neat, and Linter keeps it bug-free."
  },
  {
    "objectID": "chapters/linter_styler.html#why-care",
    "href": "chapters/linter_styler.html#why-care",
    "title": "\n13  Style and Lint your Code\n",
    "section": "\n13.4 Why Care?",
    "text": "13.4 Why Care?\nYou may wonder, if you’re comfortable with your coding style, why use these tools? While there are subjective benefits, the primary reason is that they enhance your code’s accessibility, eliminate irrelevant stylistic changes from git commits, and ensure cross-OS functionality.\n\nMac and Windows represent tabs with different symbols. If your code contains tabs, it may fail on a different OS. To ensure compatibility, use spaces (your editor can convert tabs into spaces)."
  },
  {
    "objectID": "chapters/command_line.html#learning-basic-commands",
    "href": "chapters/command_line.html#learning-basic-commands",
    "title": "14  Introduction to Command Line",
    "section": "14.1 Learning Basic Commands",
    "text": "14.1 Learning Basic Commands\nCommands form the core of CLI interactions, serving as instructions we type into the terminal to perform certain tasks, like file manipulation and directory navigation.\n\n14.1.1 Navigation\n\npwd: “Print Working Directory” shows your current directory.\ncd: “Change Directory” allows you to navigate between directories. The usage of cd with different arguments lets you navigate more efficiently:\n\ncd .. takes you one directory up.\ncd / takes you to the root directory.\ncd ~ takes you to your home directory.\ncd . refers to the current directory.\n\nls: “List” displays files and directories in the current directory.\n\n\n\n14.1.2 File Manipulation\n\ntouch: Creates a new file.\ncat: Displays file content.\ncp: “Copy” duplicates files or directories.\nmv: “Move” renames or relocates files.\nrm: “Remove” deletes files.\n\n\n\n14.1.3 Directory Management\n\nmkdir: “Make Directory” creates a new directory.\nrmdir: “Remove Directory” deletes an empty directory.\n\nYou can chain directory names together with a forward slash / for navigating. This applies to both absolute and relative paths. For instance, cd /home/username/projects/my_project navigates to my_project by providing an absolute path. On the other hand, cd projects/my_project navigates to the same location using a relative path. If you want to move up to the parent directory and then into a sibling directory, you could use cd ../sibling_directory. This command means “go up one level, then down into sibling_directory”.\nThese basic commands are the starting point for interacting with a command-line interface. As you gain familiarity, you’ll learn more complex commands and combinations for powerful functionalities. Remember, don’t hesitate to explore and experiment. The command line is a tool for you to harness to your advantage!"
  },
  {
    "objectID": "chapters/command_line.html#terminal",
    "href": "chapters/command_line.html#terminal",
    "title": "14  Introduction to Command Line",
    "section": "14.2 Terminal",
    "text": "14.2 Terminal\n\n14.2.1 Getting Started with Nano\nNano is a simple and user-friendly text editor commonly used in Unix-like operating systems. It`s an excellent choice for beginners due to its straightforward interface and commands.\n\n14.2.1.1 Starting Nano\nTo open a file in nano, type nano followed by the filename:\nnano filename.txt\nIf the file doesn`t exist, nano will create a new file with that name.\n\n\n\n14.2.2 Basic Navigation\nOnce you`ve opened a file, you can move around it using the arrow keys. Here are some additional navigation commands:\n\nCtrl + A: Move to the beginning of the line.\nCtrl + E: Move to the end of the line.\nCtrl + Y: Move to the previous page.\nCtrl + V: Move to the next page.\n\n\n14.2.2.1 Editing Text\nEditing text in nano is as straightforward as typing. Here are some additional commands:\n\nCtrl + K: Cut the current line of text.\nCtrl + U: Paste the cut text.\n\n\n\n14.2.2.2 Saving and Exiting\nWhen you`re done editing, you can save your changes and exit nano:\n\nCtrl + O: Write out. This command will save your changes. Nano will ask you to confirm or change the filename.\nCtrl + X: Exit. If you`ve made changes, nano will ask if you want to save them.\n\n\n\n14.2.2.3 Getting Help\nIf you’re stuck or need to know more commands, you can access nano’s help:\n\nCtrl + G: Display the help screen.\n\nThat’s it! You`re now equipped with the basics of comand line."
  },
  {
    "objectID": "chapters/version_control.html#git-and-github",
    "href": "chapters/version_control.html#git-and-github",
    "title": "15  Version Control with Git and Github",
    "section": "15.1 Git and Github",
    "text": "15.1 Git and Github\nGit is a widely-used version control system that efficiently manages and distributes projects of various sizes and complexities. Essential for developers, scientists, analysts, and writers, Git excels in tracking changes, fostering collaboration, and handling code. As a command-line interface, Git is accessed via the terminal.\nGitHub, a web-based platform, enhances Git’s functionality by offering a user-friendly interface for project management and collaboration. Additional features include issue tracking, code review, and project management tools. GitHub also serves as a developer’s social network, facilitating the sharing of work, discovery of projects, and open-source contributions.\n\nThe difference between Git and GitHub is the same as the difference between Porn and PornHub.\n\nStarting to use Git and GitHub can be a little overwhelming and confusing, especially when you only need a small subset of the functionality. My goal is to make you feel comfortable enough with the basics, so you can store, update, collaborate, and share your work on GitHub.\nThere have been many guides on installing Git on your machine and connecting it to RStudio. I will spare the world from my version. I recommend you use the guide from Hester (n.d.) Happy Git and GitHub for the useR and use it as a reference for your other adventures with Git. Once you went through the whole process of installation. You may want to connect it to RStudio; however, it is optional. Because the commands can be hard to memorize and connect to the actions and the state of your repository, you should install a Git Client, which will get an overview of the recent commit history, branches, and a GUI that simplifies Git operations. Try GitHub Destop or GitKraken.\n\nHester, Jenny Bryan, the STAT 545 TAs, Jim. n.d. Let’s Git Started | Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\nWhen inside of repository press “.” to start web version of VSCode to work with the repository."
  },
  {
    "objectID": "chapters/version_control.html#basics",
    "href": "chapters/version_control.html#basics",
    "title": "15  Version Control with Git and Github",
    "section": "15.2 Basics",
    "text": "15.2 Basics\nRepositories on GitHub are storage spaces for your projects, where you can manage, organize, and collaborate on files while keeping track of changes using Git’s version control system. To create a repository you can start with GitHub first or Local first. In other words, with GitHub first you start by initiating a repository on GitHub and then cloning it into your machine and with Local First you write somecode and connect your repository to the GitHub. I am generally using Local First as I often start the project and later decide whether I want it online. With GitHub desktop it is extremely easy to add a repo by just going to File -&gt; New Repository. There are also two types of repositories Private and Public. You can change the status in your repository’s settings.\nSay you have done some work and want to save it. You want to commit! To do that select all the file you want to add to the commit git add file.md or add all files with git add . and then git commit -m \"message\", it is required to add a message to your commit, so you don’t forget what changes you made. All of this can be done in your client and even RStudio with a click of few buttons. Nonetheless, I advise you get comfortable with using Command Line Interface (CLI). Now you have committed the changes, but they are not yet online, they are just recorded, so you can load them if you mess up. You can have multiple commits to keep on adding more and more changes before you are ready to share your work with the world. Once you are ready you want to git push and everything will appear on your GitHub!\nWhat if you want to download the version from GitHub, say you want to work on a different machine? ‘Git pull’ is a command that combines fetching updates from a remote repository and merging them into your local branch. It also allows you to synchronize your local branch with the latest changes from the remote, ensuring your project stays up-to-date and facilitating collaboration with other contributors.\nBut what happens if your partner updates the version on GitHub while you are working? Now you need somehow incorporate the changes into your work. To do that you first pull and if there are no conflicts, you didn’t change the same line, it will automatically merge both versions. But what if you worked on the document and there were conflicts? You need to resolve them, just like in middle school.\nResolving a merge conflict in Git involves identifying the conflicting changes and manually deciding which changes to keep or modify inside the file with the conflict. It will be depicted like this inside the file:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the line in your current branch\n========\nThis is the line in the branch you're merging.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch-name&gt;\nTo resolve the conflict, edit the file to keep the desired changes and remove the conflict markers. You might keep the change from the current branch, the branch you’re merging, or create a new line altogether:\nThis is the new, resolved line.\nSave the changes in the file and stage the file using git add. Commit the resolved merge conflict with git commit -m \"Resolved merge conflict in file\".\nRecall when my attempt to tidy code caused a crash (second story)? To prevent this, use branches in Git. Branches let you work on multiple features or fixes without affecting the main codebase. Once complete and tested, merge the branch back into the main branch.\nForking creates a personal copy of a repository under your account, while cloning creates a local copy on your computer. Forking is popular in open-source projects, allowing contributors to make changes without affecting the original project. After testing, submit a pull request to propose updates to the original repository.\nA pull request proposes merging changes from one branch to another, typically from a forked repository to the original repository. It facilitates discussion, review, and collaboration, enabling project maintainers to approve, modify, or reject proposed changes.\nGitHub issues help track bugs, requests, or discussions related to a project. They serve as a centralized forum for communication, task assignment, and progress monitoring, ensuring effective project management and timely resolution of concerns."
  },
  {
    "objectID": "chapters/version_control.html#contribute",
    "href": "chapters/version_control.html#contribute",
    "title": "15  Version Control with Git and Github",
    "section": "15.3 Contribute",
    "text": "15.3 Contribute\nGitHub streamlines collaboration on projects, offering several straightforward methods to contribute to any project. To practice, find a typo in this book and suggest an edit. Here’s how:\n\nClick on the GitHub icon and navigate to the file (chapter) in the repository.\nOpen the file and click the edit icon (pencil).\nMake the necessary changes, which will automatically create a fork and submit a pull request.\n\nAlternatively, you can fork the project, clone it to your computer, make changes, and submit a pull request. Another way to suggest changes or ask questions about the material is by opening an issue and leaving a comment. This approach allows you to provide feedback and engage in discussions with other contributors."
  },
  {
    "objectID": "chapters/version_control.html#guide-to-using-.gitignore",
    "href": "chapters/version_control.html#guide-to-using-.gitignore",
    "title": "15  Version Control with Git and Github",
    "section": "15.4 Guide to Using .gitignore",
    "text": "15.4 Guide to Using .gitignore\nThe .gitignore file instructs Git which files should not be tracked or ignored before you make a commit. It is especially useful in a team-based project where different developers have their own configurations, settings, and IDEs, or when you have temporary files that are generated when a program runs. Let’s walk through how to create and manage a .gitignore file.\n\n15.4.1 Creating a .gitignore file\nTo create a .gitignore file, navigate to your project’s root directory and create a new file named .gitignore. This can be done directly via your code editor or in the terminal:\ntouch .gitignore\n\n\n15.4.2 Specifying Files to Ignore\nOpen the .gitignore file in your text editor and specify the files, directories, or file patterns to ignore. Each new line should contain a new rule.\n\nTo ignore a directory, simply add the directory name, e.g., node_modules/\nTo ignore a file, add the full file name, e.g., debug.log\nTo ignore a file type, use *. followed by the file extension, e.g., *.log\nComments can be added by starting a line with a ##, e.g., ## This is a comment\n\nHere’s an example .gitignore file:\n## Ignore node_modules directory\nnode_modules/\n\n## Ignore all .log files\n*.log\n\n## Ignore the debug.log file\ndebug.log\n\n\n15.4.3 The Global .gitignore file\nFor files you’d like to ignore globally, across all projects, you can create a global .gitignore file:\ngit config --global core.excludesfile '~/.gitignore'\nNow you can define all rules in the ~/.gitignore file, and these will be applied across all repositories.\n\n\n15.4.4 .gitignore in Other Programs\nWhile .gitignore was originated from and is most commonly associated with Git, the concept of defining files and directories to ignore has been adopted by other software tools. These tools include various integrated development environments (IDEs), text editors, and even some operating systems. Many of these tools support .gitignore syntax and semantics, offering a familiar way to exclude files and directories. It’s always best to check the documentation of your specific tools to see how they handle file ignoring."
  },
  {
    "objectID": "chapters/version_control.html#other-resources",
    "href": "chapters/version_control.html#other-resources",
    "title": "15  Version Control with Git and Github",
    "section": "15.5 Other Resources",
    "text": "15.5 Other Resources\nFor most programming languages and popular software tools, standard .gitignore files have been created and maintained by the open-source community. GitHub maintains a repository of these files. You can use them as a starting point for your projects.\nFor more complex scenarios, please refer to the official Git documentation on the .gitignore file.\nRemember, a .gitignore file helps to keep your repositories clean from unnecessary files. Make good use of it!"
  },
  {
    "objectID": "chapters/litreview.html#search",
    "href": "chapters/litreview.html#search",
    "title": "16  Literature Research",
    "section": "16.1 Search",
    "text": "16.1 Search\nAnd now the hunt begins!\nBegin your literature search by consulting a professor or an expert in the field for guidance. Review articles are also an excellent starting point, as they provide an overview of recent developments, summarize key findings, and identify gaps in knowledge.\nNext, explore peer-reviewed databases with your University’s Subscription such as Scopus and Web of Science, which offer high-quality articles due to their thorough peer-review process. Your school or university library is another valuable resource for accessing a wide range of academic materials, often for free.\n\n\n\n\n\n\nWarning\n\n\n\nLook out for predatory journals! These journals might sound legit, but they will publish anything for a fee. If you feel suspicious, look at the web page of the journal and check Beall’s List.\n\n\nAfter exhausting these resources, turn to Google Scholar for a larger collection of articles, but be cautious of questionable publications. If you’re interested in a book, check the author’s personal website for free chapters or the entire book.\nTo jump start your literature review, try AI platforms like Paper Digest or Elicit, which provide summaries of your topic and help find relevant papers to your research question. Visualize and explore connections between articles using tools like Research Rabbit or Lit Maps, both of which integrate with Zotero for easy library management.\nFor efficient research organization, consider using the Arc browser, which allows you to create organized spaces for each project and easily navigate folders for all your links and notes."
  },
  {
    "objectID": "chapters/litreview.html#reference-management",
    "href": "chapters/litreview.html#reference-management",
    "title": "16  Literature Research",
    "section": "16.2 Reference Management",
    "text": "16.2 Reference Management\nCitation managers are indispensable tools for organizing and managing your literature sources. The greatest of all time is Zotero, a free, open-source, and cross-platform tool with a vast library of extensions. With Zotero, you can add tags, related references, notes, annotations, and effortlessly export everything into your preferred note-taking app. It is also compatible with most writing programs, making citation a breeze in any environment. To simplify obtaining references from within the browser, take advantage of the Zotero web extension. Additionally, download the BetterBibTeX plugin, which enhances Zotero’s capabilities by generating more accurate and efficient citation keys and automating the export of your library to BibTeX. Numerous online resources are available to help you get started, so be sure to make the most of them!"
  },
  {
    "objectID": "chapters/litreview.html#reading",
    "href": "chapters/litreview.html#reading",
    "title": "16  Literature Research",
    "section": "16.3 Reading",
    "text": "16.3 Reading\nNow you have amassed your literature and all ready to go through them one-by-one. Before you start, make sure you filter all the papers based on their quality, impact, and relevance, concentrating on the most critical literature. A quick skim of title, abstract, conclusions, and figures should assist you with that. Once you did that the most relevant papers should boil up to the top and now it is time for deep work. Don’t be fooled by the structure of the papers, the section are not meant to be read in the order they appear. When reading a paper, consider approaching the conclusion, discussion, and methods sections last, allowing you to comprehend the key findings and implications before delving into technical details. To aid with the (sometime rather boring) reading using Speechify, a text-to-speech app that will read the articles out loud, keeping you on pace and making information easier to absorb. It is also common to have an article open in Speechify and Zotero to take notes."
  },
  {
    "objectID": "chapters/litreview.html#taking-notes",
    "href": "chapters/litreview.html#taking-notes",
    "title": "16  Literature Research",
    "section": "16.4 Taking notes",
    "text": "16.4 Taking notes\nAs you read, it’s crucial to write simultaneously. The approach you take will depend on the scope of your project. For shorter papers, consider writing notes in a Google document and later refining them into a coherent essay. For more extensive research projects, efficient note-taking and organization are essential.\nTry using tools like Obsidian to arrange notes and create networks of ideas using local plain markdown files. Alternatively, Notion is a versatile platform for organizing notes, tasks, and databases. Make the most of Zotero’s export capabilities to transfer your notes into Obsidian or other software.\nDon’t overlook the power of AI tools, such as ChatGPT, to enhance your literature review process. Use them to summarize text sections, improve your writing, or convert copied text into markdown tables. By leveraging these cutting-edge tools, you’ll avoid slowing down your work and ensure a more efficient review process. However, do not overly depend on them to do your work, always proofread, edit the output and paraphrase the output. It is a tool not a replacement of you expertise."
  },
  {
    "objectID": "chapters/write.html#wysiwyg",
    "href": "chapters/write.html#wysiwyg",
    "title": "17  Write",
    "section": "17.1 WYSIWYG",
    "text": "17.1 WYSIWYG\nWYSIWYG (What You See Is What You Get) editors are programs that allow you to make and edit content visually without having to know how to code. This means that you can see how your content will look as you’re creating it. They’re very helpful for people who don’t have experience with coding or markup languages. Examples of WYSIWYG editors that you may already be familiar with include Google Docs and Microsoft Word. Learning how to navigate these widely used tools will teach you valuable skills such as formatting and writing. Plus, after using them for a while, you’ll start to appreciate the simplicity and efficiency of markup languages.\nI strongly recommend that you develop expertise in these tools, as the skills you acquire are highly transferable to other similar editors. Additionally, it’s a valuable investment as your colleagues are likely to use them as well. To get started, I suggest checking out the MOS Certification Series on LinkedIn Learning. These tutorials are designed for those preparing to take MOS exams and cover a broad range of functionality in Word, Excel, and PowerPoint. Even if you don’t plan on taking the exams, I highly encourage you to watch the tutorials. They’re a great resource for building your skills and improving your proficiency with these essential tools!\nHere is the link!"
  },
  {
    "objectID": "chapters/write.html#markup-languages",
    "href": "chapters/write.html#markup-languages",
    "title": "17  Write",
    "section": "17.2 Markup Languages",
    "text": "17.2 Markup Languages\nMarkup languages are sets of codes that provide structure and formatting to documents, such as web pages, eBooks, and scientific papers. Unlike WYSIWYG editors, which allow users to create content visually, markup languages require the use of specific tags or codes to indicate how the content should be displayed. These tags define the document structure, formatting, and other attributes. Some of the most commonly used markup languages include HTML, LaTeX, and Markdown. By learning a markup language, you will have greater control over the appearance and functionality of your digital documents. Additionally, markup languages can help you focus more on writing, allowing templates to handle all the formatting. I promise after you switch, there is no coming back! Your assignments and web pages will look beautiful every time.\n\n17.2.1 HTML\nWhen you hear markup language, you might think of Hypertext Markup Language (HTML), which is used everywhere on the web to build structure and content. HTML uses tags to describe how to display content in a browser. For instance, &lt;br&gt; will create a line break, &lt;img&gt; will add an image and so on. HTML is an essential component of web development, and learning how to use HTML is the first step towards customizing your own web pages and web applications. It is also often used in manipulating text in your graphs.\n\n\n17.2.2 LaTeX\nHave you ever wondered why academic papers look so beautiful or why every textbook looks the same? They are all written in LaTeX, a typesetting system used for academic and scientific publishing. It is a markup language that enables precise formatting of complex technical documents. It is also famous for beautiful mathematical equations. With LaTeX, users can create professional-looking documents with a high degree of customization and flexibility. It is easy to pick up, but hard to master. You can find many introductory tutorials online. Also, ChatGPT does a great job at helping with LaTeX questions. If you have an equation you want to transfer into LaTeX go to MathPix. It lets you convert pictures or screenshots into code.\nNow you need to find an editor! The default is to use the online editor OverLeaf. You can start writing without much headache, perfect! And if you have any questions, refer to their extensive collection of guides.\n\n\n17.2.3 Markdown\nMarkdown is perfect down to the last-minute detail. It was designed as a lightweight markup language for creating formatted text using a plain-text editor. It appeals to human readers in its source code form in comparison to the complexity and inefficiency of existing markup languages like HTML.\nMarkdown is popular among developers, writers, and bloggers due to its simplicity and flexibility. It can also include links, images, and other multimedia elements. Markdown is widely supported and can be used with a variety of tools and platforms, including text editors, note-taking apps, and content management systems. By learning Markdown, you can create well-formatted and easy-to-read content quickly and efficiently without the need for complex formatting tools or specialized software. This book is written entirely in Markdown!\nYou can write Markdown in almost any text editor. Nonetheless, I recommend MacDown for Mac and Markdown Pad for Windows, StackEdit for Online, or if you are willing to pay, try Typora.\n\n\n17.2.4 Yet Another Markup Language (YAML)\nYAML (Yet Another Markup Language) is a human-readable data serialization language often used for configuration files and data exchange. It uses indentation, key-value pairs, and directories and supports different data types. Directories are used to organize data as key-value pairs with indentation to indicate hierarchy. The key represents a unique identifier or name, and the value represents associated data or content. YAML is easy to read and write and is popular in web development and software configuration. In YAML,\n\n\nbook:\n  title: \"Dead Souls\"\n  author: \"Nikolai Gogol\"\n  date: today\n  chapters:\n    - index.qmd\n    - chapter1.qmd\n  appendix:\n    - appendix1.qmd\n\n% Key value pairs\ntitle: \"Dead Souls\"\nauthor: \"Nikolai Gogol\"\ndate: today\n\n% Directory/Map  \nchapters:\n  - index.qmd\n  - chapter1.qmd\n\n\n\n\n17.2.5 Pandoc\nPandoc (all documents) converts files from one markup format into another. It supports a wide range of formats, including HTML, Markdown, LaTeX, PDF, and Microsoft Word. With Pandoc, users can convert documents from one markup language to another quickly and easily without having to edit or reformat the content manually.\nPandoc can be used for a variety of purposes, such as converting a Markdown file to HTML for a web page or converting a LaTeX document to PDF for printing. It can also be used to merge multiple documents into a single file or to extract content from a document in a specific format.\nPandoc is highly customizable, with many options and settings available for controlling the output format and appearance of the converted document. It is widely used in academic and scientific publishing, as well as in web development and documentation. By using Pandoc, you can save time and effort by automating the conversion process between different markup formats.\nPandoc is a command-line tool, but it can be integrated into some markdown editors. It extends the capabilities of Markdown by adding support for tables, footnotes, citations, and more. For example, this book was written in Markdown and converted into HTML and PDF using Pandoc."
  },
  {
    "objectID": "chapters/write.html#quarto",
    "href": "chapters/write.html#quarto",
    "title": "17  Write",
    "section": "17.3 Quarto",
    "text": "17.3 Quarto\nAt this point, you might be wondering, “why did I have to read about these technologies?” Because the combination of LaTeX, Markdown, Pandoc, and YAML provides a powerful and flexible set for the creation of documents. Those familiar with Sweave, Rmarkdown, and Jupyter Notebooks will find Quarto to be a similar tool. Quarto builds upon the success of Rmarkdown and Jupyter Notebooks, combining the best features of Markdown with new functionality and eliminating the need for additional packages. It provides attractive default formatting options and easy customization. If you have experience writing in Markdown, Quarto will be a breeze to use.\nWhat can it do?\n\nCreate Reproducible Documents and Reports.\nCreate Dynamic Content with Python, R, Julia, and Observable.\nCreate professional-grade content, including articles, reports, presentations, websites, blogs, and books, in a variety of formats such as HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\nCreate interactive tutorials and notebooks.\n\n\n17.3.1 Installing\nTo use Quarto you don’t need any special software, if you would like, you can even use a text editor to create your .qmd files and command line to render the document. However, working in IDE will make your life much easier. I prefer to use RStudio, after all, Quarto is a product of Posit (former RStudio), but Visual Studio Code also works well. If you have not installed RStudio nor Visual Studio Code yet, follow (setup.qmd?).\n\n17.3.1.1 RStudio\nFrom within Rstudio you can install Quarto as you install any other package. Additionally, you should install TinyTeX, a minimal set of packages required to compile LaTeX documents. You can get both by running the following command in R.\ninstall.packages(c(\"quarto\", \"tinytex\"))\n\n\n17.3.1.2 Visual Studio Code\nInstall the Quarto extension by going to Extensions -&gt; search for “Quarto” -&gt; install. Then run the following command in the terminal if you don’t have TinyTeX installed:\nquarto install tinytex\n\n\n\n17.3.2 Your First Document\nWrite Create a Quarto project. Go to top panel -&gt; File -&gt; New Project -&gt; Directory -&gt; Quarto Project -&gt; Create\nFile -&gt; Quarto Document -&gt; Write the Title of your document -&gt; Select the format you want (HTML is the default) -&gt; Create\nAn (optional) YAML header is demarcated by three dashes (—) on either end. You can learn more about YAML on Quarto’s website. There are a lot of useful options for display.\n---\ntitle: \"Title\"\neditor: visual\nformat: pdf\n---\nIf you prefer the WYSIWYG style, you can switch to the visual editor in the top left corner. Additionally, if you tick “Render to Save,” a new document preview will be updated after each save.\nNow it is time to get writing! To start, H1 heading can be defined in yaml title: \"Title\" and as # Title.\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n# Header 1\n18 Chapter\n\n\n## Header 2\n18.1 Section\n\n\n### Header 3\n18.1.1 Subsection\n\n\n#### Header 4\n18.1.1.1 Subsubsection\n\n\n##### Header 5\n18.1.1.1.1 Paragraph\n\n\n###### Header 6\n18.1.1.1.1.1 Subparagraph\n\n\n*italics*\nitalics\n\n\n**bold**\nbold\n\n\nsuperscript^2^\nsuperscript2\n\n\n&lt;https://nber.org&gt;\nhttps://nber.org\n\n\n[NBER](https://nber.org)\nNBER\n\n\n![caption](monalisa.jpeg)\n\n\n\nunordered list\n+ item\n\nsub-item\nsub-sub-item\n\nunordered list\n\nitem\n\nsub-item\n\nsub-sub-item\n\n\n\n\n\nordered list\n1. item\nI. sub-item\n\nsub-sub-item\n\nordered list1\n\nitem\n\nsub-item\n\nsub-sub-item\n\n\n\n\n\ninline math: $E = mc^{2}$\ninline math: \\(E = mc^{2}\\)\n\n\ndisplay math:\n$$E = mc^{2}$$\ndisplay math:\n\\[E = mc^{2}\\]\n\n\n\n1 Use Visual Mode to select your desired style!If you want to add a line break add an empty line between your paragraphs, otherwise it will continue as the same text.\nUse `…` to add inline code: print(hi, friend)\nUse ``` to delimit blocks of source code:\n```\nprint(hi, friend)\n```\nMaking tables in Markdown is not complicated. The most frequently used table is the pipe table. It allows you to see alignment and captions with :. Tables can get complicated pretty quickly, if you ever get stuck, refer to Quarto’s table documentation.\n\n\n| Default | Left | Middle | Right   | \n|---------|:-----|:------:|--------:|\n| Hola    | Pitt | 3.141  | Nile    | \n| Bonjour | Li   | 2.718  | Amazon  | \n| Salut   | Roth | 4.123  | Yangtze | \n\n: Table Demonstration\n\n\nTable Demonstration\n\n\nDefault\nLeft\nMiddle\nRight\n\n\n\n\nHola\nPitt\n3.141\nNile\n\n\nBonjour\nLi\n2.718\nAmazon\n\n\nSalut\nRoth\n4.123\nYangtze\n\n\n\n\n\nIf you ever feel lost or struggle with formatting, consider using the visual editor. It provides a familiar interface and is particularly useful for creating and previewing tables. To adjust options, for example, the number of list options, simply click on the circle icon with an ellipsis next to it, and a selection menu will appear. In addition to this, the visual editor offers extensive customization options for other elements, such as images and tables.\n\n\n\n\n\n\nNote\n\n\n\nYou can copy-paste (Ctrl + C; Ctrl + V) a picture in Visual Mode!\n\n\nYou can learn more at Quarto’s website."
  },
  {
    "objectID": "chapters/layout_refs.html#knitr",
    "href": "chapters/layout_refs.html#knitr",
    "title": "\n18  Layout and References\n",
    "section": "\n18.1 Knitr",
    "text": "18.1 Knitr\nknitr is a package that takes care of the middle step between evaluating code and producing pdf/html. The package runs your code and places its output into a final markdown file, which is later converted by pandoc.\nknitr lets you set cell options that influence code blocks’ execution and output. They are put at the top of a block within comments. For example:\n\nCollapse Code```{r}\n#| echo: \"fenced\"\n#| label: fig-plots\n#| fig-cap: Plots\n#| fig-subcap:\n#|   - \"Sunspot\"\n#|   - \"US Population\"\n#| layout-ncol: 2\n#| code-fold: show\n#| code-summary: \"Collapse Code\"\n\nplot(sunspot.year)\nplot(uspop)\n```\n\n\n\n\n\n(a) Sunspot\n\n\n\n\n\n(b) US Population\n\n\n\nFigure 18.1: Plots\n\n\n\nThere is large number of options, but I will show the most commonly used ones. To begin in Figure 18.1, label is a unique id for a code cell, which can be referred to in text with @fig-plots. Similarly, you can refer to tables, chapter, and files. fig-cap defines a caption for the entire plot. fig-subcap gives the two plots their individual sub-captions. layout-ncol let’s us display our plots, pictures, etc. in separate columns. And plot() makes the plots. If you would like your code to fold use code-fold = true, above option show was used to have it opened by default. code-summary defines text for collapsed code blocks.\nAnother common options to use within code blocks are:\n\nfrom R Markdown Cheat Sheet\n\n\n\n\n\n\nOption\nValue\nExplanation\n\n\n\neval\ntrue\nWhether to evaluate the code and include its results\n\n\necho\ntrue\nWhether to display code along with its results\n\n\nwarning\ntrue\nWhether to display warnings\n\n\nerror\nfalse\nWhether to display errors\n\n\nmessage\ntrue\nWhether to display messages\n\n\ninclude\ntrue\nPrevents any output (code, results) from being included\n\n\ntidy\nfalse\nWhether to reformat code in a tidy way when displaying it\n\n\nresults\n“markup”\nType of output format: “markup”, “asis”, “hold”, or “hide”\n\n\ncache\nfalse\nWhether to cache results for future renders\n\n\n\ntidy: true is super useful once you want to include code inside your document as it will format it nicely."
  },
  {
    "objectID": "chapters/layout_refs.html#div-blocks",
    "href": "chapters/layout_refs.html#div-blocks",
    "title": "\n18  Layout and References\n",
    "section": "\n18.2 Div Blocks",
    "text": "18.2 Div Blocks\nIf you are familiar with HTML you will recognize &lt;div&gt; blocks. You can add div blocks with wrapping text in ::: or more semicolons. It is useful when you want put pictures in a grid. Here is a simple example:\n::: {layout-ncol=\"2\"}\n![Surus](surus.png)\n\n![Hanno](hanno.png)\n:::\nIt must be separated from the preceding and following blocks by blank lines. Divs can be nested inside other Divs. For example, here we put a note and some text onto the margin.\n:::: column-margin\n\n::: callout-note\nHere is a Note!\n:::\n\nMore content.\n::::\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a Note!\n\n\nMore content.\nThe pagebreak short code enables you to insert a native page break into a document that will be compatible with all the other formats:\nFirst Page\n\n{{&lt; pagebreak &gt;}}\n\nSecond Page\nBecause R, YAML, HTML, LaTeX have different notations for  commenting. So, the one that will work universally within quarto is HTML’s &lt;!-- comment here --&gt;."
  },
  {
    "objectID": "chapters/layout_refs.html#diagrams",
    "href": "chapters/layout_refs.html#diagrams",
    "title": "\n18  Layout and References\n",
    "section": "\n18.3 Diagrams",
    "text": "18.3 Diagrams\nYou can also create beautiful UML (Unified Modeling Language) diagrams within quarto with Mermaid and Graphviz. The flow chart below was maid with Mermaid!\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "chapters/layout_refs.html#citations",
    "href": "chapters/layout_refs.html#citations",
    "title": "\n18  Layout and References\n",
    "section": "\n18.4 Citations",
    "text": "18.4 Citations\n\n“Proper citation adds credibility to your work and acknowledges the work of others.” - Chat GPT\n\nAdding citations to your work shouldn’t be stressful or confusing. With Quarto’s seamless integration with Zotero, you can easily add citations in your preferred style and create a reference list, all without hassle. How cool is that? I think pretty cool.\nQuarto utilizes Pandoc to generate citations and bibliographies in your preferred style. To source your citations, you’ll need a .bib or .bibtex file, and optionally a .csl file for the citation style. Simply, add bibliography: references.bib to you YAML header in _quarto.yml.\n\nbibliography: references.bib\n\nYou can easily cite your article using @yourcitation9999. Visual mode also provides suggestions, and entering the article’s DOI will help locate and insert it even if it is not in your bibliography. For more information on citation methods, see Quarto Citation and Pandoc Citations.\n\n\n\n\n\n\nMarkdown Format\nOutput (author-date format)\n\n\n\n@abadie2017 says cluster you SE.\n\nAbadie et al. (2017) says cluster you SE.\n\n\nSome thing smart [@abadie2017; @bai2009].\nSome thing smart (Abadie et al. 2017; Bai 2009).\n\n\nAbadie says cluster [-@abadie2017].\nAbadie says cluster (2017).\n\n\n\n\nAbadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge. 2017. “When Should You Adjust Standard Errors for Clustering?” Cambridge, MA. https://doi.org/10.3386/w24003.\n\nBai, Jushan. 2009. “Panel Data Models with Interactive Fixed Effects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ecta6135.\nIf you’ve successfully created your bibliography in Zotero, adding citations to your document will be a breeze. Simply start typing and Zotero will suggest citations to add to your bibliography file. For a paper with more than 10 citations, I recommend using Better Bibtex, which allows you to connect citation keys to the paper as you write, just make sure Zotero is open.\nTo generate your citations from a document (say cited in Obsidian) without having to re-cite everything, you can use the bbt_update_bib() function from the rbbt package. Ensure that Zotero is running and that you’re in the markdown document where you want to update citations. Run the bbt_update_bib() function to create a bibliography, and specify any additional arguments as needed.\n\nbbt_update_bib(\n  path_rmd, # Path to your Markdown document.\n  path_bib = bbt_guess_bib_file(path_rmd), # Path to the references.bib file\n  translator = bbt_guess_translator(path_bib), # type of bibliography file to generate: CSL-JSON, BibLaTeX, BibTeX, and CSL YAML.\n)"
  },
  {
    "objectID": "chapters/template.html#getting-started",
    "href": "chapters/template.html#getting-started",
    "title": "19  Thesis Template",
    "section": "19.1 Getting Started",
    "text": "19.1 Getting Started\nTo use this template, you will need to install Quarto and TinyTeX.\n\n19.1.1 RStudio\nInstall Quarto and TinyTeX by running the following command in R:\n\ninstall.packages(c(\"quarto\", \"tinytex\"))\n\n\n19.1.2 Visual Studio Code\nInstall the Quarto extension by going to Extensions -&gt; search for “Quarto” -&gt; install. Then run the following command in the terminal if you don’t have TinyTeX installed:\nquarto install tinytex"
  },
  {
    "objectID": "chapters/template.html#adding-the-quarto-template",
    "href": "chapters/template.html#adding-the-quarto-template",
    "title": "19  Thesis Template",
    "section": "19.2 Adding the Quarto Template",
    "text": "19.2 Adding the Quarto Template\nTo add the template to your project, run the following command in your terminal:\nquarto use template nikitoshina/USFCA-Thesis-Template\nThis will download all the necessary folders with the LaTeX files for your thesis. You can render your project from within your folder by running the following command in terminal:\nquarto render"
  },
  {
    "objectID": "chapters/template.html#usage",
    "href": "chapters/template.html#usage",
    "title": "19  Thesis Template",
    "section": "19.3 Usage",
    "text": "19.3 Usage\nYour first chapter should be written in index.qmd, and you can add additional chapters to the /Chapters folder. You can add information about the document such as abstract in the _quarto.yml.\n\nIf you want to create your own template, I recommend starting with a Word template from pandoc and customizing it to your preferences, rather than dealing with LaTeX and pandoc variables."
  },
  {
    "objectID": "chapters/collaboration.html#trackdown-and-google-docs",
    "href": "chapters/collaboration.html#trackdown-and-google-docs",
    "title": "\n20  Collaboration\n",
    "section": "\n20.1 trackdown and Google Docs",
    "text": "20.1 trackdown and Google Docs\nProjects often involve collaborators with varying technical proficiency, and using markdown files might pose challenges for some stakeholders. One approach could be to render a Microsoft Word document and transfer edits back into markdown later. However, this method can be laborious, and our goal is to work smarter, not harder. A solution to this issue is to use a package called trackdown, which enables collaboration on narrative text through Google Docs. To edit .qmd files, you will need the version of trackdown &gt;= 1.3, which can be downloaded from GitHub using the following command:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"claudiozandonella/trackdown\",\n                         build_vignettes = TRUE)\nlibrary(trackdown)\n\nAt the moment, the API credentials for the package have been exhausted, so you’ll need to set up your own. To do this, follow the straightforward guide provided by the developers: https://claudiozandonella.github.io/trackdown/articles/oauth-app-configuration.html.\nAfter setting everything up, you can upload your file to Google Docs using upload_file(\"your_file.qmd\") and share it with collaborators. If you need to make last-minute changes, use update_file(\"your_file.qmd\") to update the Google Doc, beware it will overwrite the document. When the review process is complete, execute trackdown:download_file(), and the changes will be automatically integrated.\n\n\n\n\n\n\nNote\n\n\n\nSave the file before uploading and updating!\n\n\nWhen you’re ready to add code for figures, tables, or analysis results, avoid doing so in Google Docs. Instead, first download the document. Ensure all changes made by collaborators in Google Docs are accepted (or rejected) before downloading. To accept all changes at once, use “Tools &gt; Review suggested edits &gt; Accept all”. Then, download the edited document from Google Drive using download_file(file = \"your_file.qmd\"). Once you are done implementing the changes you can again use update_file(\"your_file.qmd\") to update the file in Google Drive. One handy trick is to use render_file(file = \"your_file.qmd\") to both download and render file. You can accept all the changes run render_file() and check whether the file renders correctly and undo all the changes in the Google Doc to selectively accept the changes.\nWhile collaborating on .qmd documents, use Google Docs for narrative text and Git for code. Avoid writing or editing code in Google Docs, as it’s prone to errors. Write code in an IDE like RStudio instead. When using trackdown remember that formatting done in Google Docs will be lost. Use proper Markdown or LaTeX syntax for formatting.\nThe workflow is iterative, with the document being uploaded/updated on Google Drive for narrative text editing and downloaded locally for code writing with Git. Note that simultaneous collaboration on narrative text and code is not possible with trackdown, as changes in both versions cannot be automatically merged. Structuring the workflow sequentially ensures a smooth experience. When adopting such workflow, limit the use of R code in the R Markdown file and separate code into different files to prevent interference between code and narrative text. This way you will have to separate system for working on code and narrative avoiding the clash.\nFor more information on the package have a look at the vignettes and visit its GitHub page https://quarto.org/."
  },
  {
    "objectID": "chapters/survey_se.html#total-survey-error-tse",
    "href": "chapters/survey_se.html#total-survey-error-tse",
    "title": "21  Survey Error",
    "section": "21.1 Total Survey Error (TSE)",
    "text": "21.1 Total Survey Error (TSE)\nWhat makes a survey credible? The answer lies in its accuracy, effective design, and minimal error. Total Survey Error (TSE) is a framework enabling researchers to identify and mitigate the various errors that can surface in surveys. By understanding these errors, proactive measures can be taken to minimize them in subsequent surveys.\nTSE consists of several components:\nTotal Survey Error = Specification error + Processing error + Sampling error + Coverage error + Non-response error + Measurement error\n\nSpecification error: Specification errors manifest when there’s no distinct link between theoretical concepts or constructs and the survey variables. It’s vital to ensure that the survey questions aptly capture the intended concepts. Uninsightful questions yield uninformative results, so thorough literature reviews are critical.\nProcessing error: These errors pertain to issues arising during the collection and management of survey data. Such errors could comprise data entry inaccuracies, coding mistakes, or other inconsistencies that may occur during data processing. Adherence to reproducible research practices is vital!\nSampling error: Sampling errors occur because surveys typically gather data from a subset of individuals rather than the entire population of interest. Such errors stem from the fact that the sample results may not flawlessly reflect the true population values.\nCoverage error: Coverage errors happen when individuals from the target population are absent from the sampling frame used to draw a representative sample. This can lead to under-representation or over-representation of certain groups within the survey.\nNon-response error: Non-response errors occur when individuals selected for the survey either choose not to participate or fail to provide comprehensive responses. This could introduce bias if non-respondents differ significantly from respondents in crucial aspects.\nMeasurement error: Measurement errors arise from discrepancies between the estimated value and the “true” value, often attributed to survey design elements. Such errors can involve inaccuracies in question phrasing, response options, or respondent interpretation."
  },
  {
    "objectID": "chapters/survey_se.html#basic-theoretical-model-of-the-survey-process",
    "href": "chapters/survey_se.html#basic-theoretical-model-of-the-survey-process",
    "title": "21  Survey Error",
    "section": "21.2 Basic Theoretical Model of the Survey Process",
    "text": "21.2 Basic Theoretical Model of the Survey Process\nUnderstanding the errors intrinsic to surveys, let’s delve into the respondent’s mental process while answering question — where potential pitfalls may lie. The survey process encompasses several steps that respondents undertake when addressing survey questions:\n\nRetrieval of information: The respondent comprehends and forms an understanding of the concepts and terms presented in the question.\nComprehension of the question: The respondent retrieves pertinent information from memory to formulate an answer.\nJudgment of appropriate answer: The respondent determines how to communicate the retrieved information and selects the most suitable answer.\nCommunication of answer: The respondent provides a response to the question.\n\nConsider your thought process when answering the following question and identify any potential confusion:\n\nOver the last five working days, what percentage of time did you use corporate-grade communication software?\n\nTotal Survey Error can be represented scientifically, showing when each error might occur.\n\n\n\nTotal Survey Error diagram as presented in Groves et al. (2011).\n\nGroves, Robert M, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski, Eleanor Singer, and Roger Tourangeau. 2011. Survey Methodology. John Wiley & Sons.\n\n\nWe will delve into each of these stages and build an understanding of how to avoid common errors.\nCaroline Jarrett (2021) provides one of my favorite depictions (a hit with marketers!).\n\nJarrett, Caroline. 2021. Surveys That Work: A Practical Guide for Designing and Running Better Surveys. Rosenfeld Media.\n\n\n\nSurvey Octopus\n\n\n\n\n21.2.1 Establishing goals\nThe utility of a survey is only as great as the insights it provides.\n\nName of the game is “Ask stupid questions – get stupid answers”.\n\nBefore you even start with designing your survey ask the three questions:\n\nWhat do you want to understand?\nWhy do you need this understanding?\nWhat decisions will be guided by the answers?\n\nEnsure to define the problem, conduct an exhaustive literature review, identify gaps in the discussion, and determine why it’s important to address these gaps. Then use theory to form hypotheses and decide on the testing methods.\n\n\n21.2.2 Target Population and Sampling Frame\nFor a successful survey, we must outline our target population and sampling frame. The target population represents the group we aim to study, while the sampling frame is the source from which we draw our sample.\nHowever, obtaining complete coverage can be challenging. Undercoverage happens when certain individuals or groups in the target population are inadequately represented in the sampling frame.\n\n\n\nUndercoverage\n\n\nThis can result in biased outcomes. Conversely, overcoverage refers to the inclusion of individuals or groups in the sampling frame who are not part of the target population, potentially leading to extraneous data collection and biases.\n\n\n\nOvercoverage\n\n\nTo ensure a representative sample, it’s crucial to minimize undercoverage and overcoverage. This can be achieved by meticulously designing the sampling frame, considering aspects such as demographics, location, marketing strategy, and time constraints. Frequent evaluation and adjustments of the sampling frame are key to addressing any coverage issues that might surface.\nIf you happen to miss your target population, you might find yourself taking a detour in the footsteps of Mr. Worldwide to Alaska. In the 2012 incident Pitbull participated in a commercial where Walmart clients could vote for the store where the artist would perform. Well, asking a question on the internet is akin to asking a question to everyone, and things took an unexpected turn. The vote was hijacked, and Pitbull ended up being sent to Alaska. So, it’s a reminder that when conducting surveys, it’s crucial to ensure you’re reaching the right people and avoiding any unintended outcomes. \n\n\n21.2.3 Representativeness\nAchieving representativeness in surveys is crucial for obtaining a balanced reflection of the population. This ensures that the selected sample accurately depicts the characteristics, opinions, or behaviors of the larger population.\nAiming for representativeness helps portray a comprehensive image reflecting the true diversity of the study population."
  },
  {
    "objectID": "chapters/survey_se.html#iterative-design",
    "href": "chapters/survey_se.html#iterative-design",
    "title": "21  Survey Error",
    "section": "21.3 Iterative Design",
    "text": "21.3 Iterative Design\n\nBetter to ask the right people than many people.\n\nIterative design is key in survey research, providing insights into the target population. It involves cycles of learning, refining, and scaling to enhance the effectiveness of surveys:\n\nInterview 1 person.\nTalk to 10 people.\nSurvey 100 people.\n\nThis iterative process enables researchers to glean insights from interactions, refine survey instruments, and scale up for extensive analysis, thereby enhancing understanding, ensuring reliable data, and informing decision-making."
  },
  {
    "objectID": "chapters/survey_se.html#the-people-you-ask-and-those-who-respond",
    "href": "chapters/survey_se.html#the-people-you-ask-and-those-who-respond",
    "title": "21  Survey Error",
    "section": "21.4 The People You Ask and Those Who Respond",
    "text": "21.4 The People You Ask and Those Who Respond\nIn fieldwork, we interact with participants and collect their responses. However, it’s crucial to address potential challenges in reaching the desired target group. Here are the key considerations:\n\nTarget Population: The specific group we aim to understand or represent through the survey.\nSample Frame: The list or source used to select our survey sample, ensuring it aligns with the target population.\nPeople you ask: Individuals approached to participate in the survey, selected based on the sample frame and target population.\nPeople who respond: The participants who voluntarily provide their responses. The response rate plays a role in ensuring data quality.\n\nBy carefully defining the target population, using an appropriate sample frame, effectively engaging participants, and analyzing respondent characteristics, we enhance the reliability and validity of our survey findings.\n\n\n\nFieldwork"
  },
  {
    "objectID": "chapters/survey_se.html#what-responses-depend-on",
    "href": "chapters/survey_se.html#what-responses-depend-on",
    "title": "21  Survey Error",
    "section": "21.5 What Responses Depend On",
    "text": "21.5 What Responses Depend On\nSurvey participant responses are influenced by key factors, including effort, reward, and trust. Recognizing these dependencies can enhance the quality and reliability of survey data.\nThe diagram below, adapted from Jarrett and Gaffney (2009)’s work on web form design, illustrates the interplay between these factors:\n\nJarrett, Caroline, and Gerry Gaffney. 2009. Forms That Work: Designing Web Forms for Usability. Morgan Kaufmann.\n\n\n\nResponse Dependencies\n\n\n\nEffort: The amount of effort required from participants to complete the survey can significantly impact their willingness to respond. Long, complex, or time-consuming surveys may discourage participation, leading to lower response rates.\nReward: Participants often seek some form of incentive or benefit for their involvement in the survey. This can include tangible rewards, such as monetary compensation or gift cards, or intangible rewards, such as the satisfaction of contributing to research or personal interest in the survey topic. Offering appropriate rewards can motivate participants to provide accurate and thoughtful responses.\nTrust: Building trust with survey participants is crucial for encouraging open and honest responses. Participants need to feel confident that their privacy and confidentiality are respected, their data will be handled securely, and the survey is conducted by a reputable organization. Clear communication about data protection measures and ethical considerations helps establish trust and increases the likelihood of obtaining reliable responses.\n\nBy taking into account the effort required, providing appropriate rewards, and building trust in the survey process, researchers can foster an environment conducive to obtaining high-quality responses from participants."
  },
  {
    "objectID": "chapters/survey_se.html#how-bad-are-u",
    "href": "chapters/survey_se.html#how-bad-are-u",
    "title": "21  Survey Error",
    "section": "21.6 How BAD Are U???",
    "text": "21.6 How BAD Are U???\nIn an intriguing study titled John, Acquisti, and Loewenstein (2011), experiments were conducted to understand the factors that influence people’s willingness to divulge personal information. One notable experiment involved altering the design of a survey page and observing its impact on disclosure rates.\n\nJohn, Leslie K, Alessandro Acquisti, and George Loewenstein. 2011. “Strangers on a Plane: Context-Dependent Willingness to Divulge Sensitive Information.” Journal of Consumer Research 37 (5): 858–73.\nStudents from Carnegie Mellon University were recruited for the study, and variations were made to the survey’s title and interface. In the frivolous condition, the survey bore a humorous title, “How BAD are U??”, and a playful appearance to downplay privacy concerns. Conversely, the baseline condition had the survey presented within a professional context, titled “Carnegie Mellon University Survey of Ethical Standards.”\nInterestingly, participants in the frivolous condition were, on average, 1.7 times more likely to confess to engaging in sensitive behaviors compared to those in the baseline condition. For example, individuals in the frivolous condition were over twice as likely to admit to taking nude photos of themselves or a partner, with a 31.8% admission rate in the frivolous condition versus a 15.7% rate in the baseline condition.\nThese findings suggest that individuals might feel more at ease sharing personal information on platforms that don’t appear strictly professional, even though these platforms could potentially pose a higher risk of data misuse. The results highlight the complex interplay between context, trust, and disclosure when it comes to personal information.\nThis study brings to light the intricate nature of human behavior and the various factors that influence our readiness to reveal personal information in different settings. Understanding these dynamics is pivotal in designing surveys and online platforms that achieve the right balance between user trust and data protection."
  },
  {
    "objectID": "chapters/survey_se.html#answering-a-question",
    "href": "chapters/survey_se.html#answering-a-question",
    "title": "21  Survey Error",
    "section": "21.7 Answering a Question",
    "text": "21.7 Answering a Question\nAccording to Tourangeau, Rips, and Rasinski (2000), there are four key steps involved in effectively answering a survey question. These steps provide valuable insights into the respondent’s process and are critical to designing effective surveys:\n\nTourangeau, Roger, Lance J Rips, and Kenneth Rasinski. 2000. “The Psychology of Survey Response.”\n\nUnderstand the question: The first step in answering a survey question involves comprehension. Respondents need to accurately understand the question in order to provide a meaningful response. Therefore, the clarity and readability of the question are of utmost importance. For example:\n\nUnderstand: In the last five days at work, what percentage of time did you use corporate-grade communication software?\n\nFind an answer: Once the question is understood, respondents must search their memory or knowledge to identify a suitable answer. This answer should be based on the information available to them. For instance:\n\nRemember:\n\nWhat color of shirt did you wear Tuesday 2 weeks ago?\nWhat color of shirt did you wear for New Years?\nHow did you celebrate your 18th birthday?\nWhat did you have for breakfast yesterday?\nWhen did the American Civil War start?\n\n\nJudge the answer: After identifying a potential answer, respondents evaluate whether they feel comfortable sharing it. Factors such as privacy, social desirability, and the sensitivity of the information play a role in this judgment process. For example:\n\nAgree or Disagree with the following statement: “I approve of the current management’s actions.”\n\nStrongly agree\nAgree\nNeither\nDisagree\nStrongly disagree\n\n\n\nThough the question seems straightforward, expressing disagreement with the current management’s actions could have potential consequences, such as risk of dismissal if the management becomes aware of your lack of support.\n\nPlace the answer: Finally, respondents need to appropriately map their answer onto the response options provided. This could involve selecting a specific category, rating on a scale, or providing a written response. For example:\n\nWhat is your major?\nWhat is your career?\nWhat is your major? ____\nWhat is your career? ____\n\n\n\n21.7.1 Enhancing Question Quality\nThe process of refining survey questions can drastically improve the quality of responses received. For instance, the original question:\n“In the last five days at work what percentage of time did you use corporate grade communication software?”\ncan be revised into multiple more specific questions, enhancing clarity and ensuring accurate responses. An example of these revised questions are:\n\nHow would you improve the question?\n\n“On your most recent working day, what percentage of time did you use messaging software other than email (Slack, Discord, WhatsApp, Telegram, etc.)?”\n“What do you use for communication at work? (Select all that apply)”\n\nEmail\nSlack\nDiscord\nWhatsApp\nOther\n\nInsights from survey response psychology can aid researchers in designing high-quality questionnaires that minimize respondent burden, enhance data quality, and create a better overall survey experience."
  },
  {
    "objectID": "chapters/good_questions.html#guidelines-for-effective-question-design",
    "href": "chapters/good_questions.html#guidelines-for-effective-question-design",
    "title": "22  Design Questions",
    "section": "22.1 Guidelines for Effective Question Design",
    "text": "22.1 Guidelines for Effective Question Design\nTo develop effective survey questions, Krosnick (2018) suggests the following guidelines:\n\nKrosnick, Jon A. 2018. “Questionnaire Design.” The Palgrave Handbook of Survey Research, 439–55.\n\nUse simple and familiar words to ensure clarity and comprehension. Avoid technical terms, jargon, and slang.\nEmploy simple sentence structures and syntax for easy understanding.\nAvoid words with ambiguous meanings to ensure all respondents interpret the questions similarly.\nMake questions specific and concrete rather than general and abstract to obtain consistent and reliable responses.\nProvide response options that are exhaustive (covering all possible choices) and mutually exclusive (no overlap between options).\nAvoid leading or loaded questions that sway respondents toward a particular answer.\nAsk about one thing at a time to prevent confusion and promote accurate responses. Avoid double-barreled questions that combine multiple topics.\nSteer clear of questions with single or double negations, as they can be confusing and lead to misinterpretation, even if some disciplines require a portion of questions to use negation."
  },
  {
    "objectID": "chapters/good_questions.html#the-impact-of-question-order",
    "href": "chapters/good_questions.html#the-impact-of-question-order",
    "title": "22  Design Questions",
    "section": "22.2 The Impact of Question Order",
    "text": "22.2 The Impact of Question Order\nThe sequence of questions in a survey can significantly impact responses and respondent engagement. Here are some considerations:\n\nStart with easy and pleasant questions to build rapport and boost respondent confidence.\nBegin with questions directly addressing the main topic as described to the respondent prior to the survey.\nGroup related questions together to maintain continuity and coherence.\nSequence questions on a specific topic from general to specific, gradually narrowing the focus.\nPlace sensitive questions that may make respondents uncomfortable towards the end of the questionnaire, allowing participants to warm up before addressing personal or sensitive topics.\nInclude filter questions to avoid asking irrelevant queries. This ensures participants only answer questions applicable to them.\n\nAdhering to these principles of question design and order can enhance the quality and reliability of survey data.\nIf you’re looking for tested questionnaires, I highly recommend the repository created by Gabriel Gilmore & Sara Finney, which offers a comprehensive collection of validated questionnaires.\n\nMeasurement error is a difference between answers you get and true values. You can also think about it as mismatches between the questions you ask and the answers that people give you."
  },
  {
    "objectID": "chapters/good_questions.html#survey-says-bacon",
    "href": "chapters/good_questions.html#survey-says-bacon",
    "title": "22  Design Questions",
    "section": "22.3 Survey Says: Bacon!",
    "text": "22.3 Survey Says: Bacon!\nEdward Bernays, a prominent figure in public relations, was approached by a bacon company seeking to boost sales.\nInitially, Bernays conducted research and found that a substantial breakfast was likely beneficial for health. Armed with this information, he approached the physician on his team and requested his endorsement, leading to a survey sent to 5,000 colleagues, all of whom agreed with the health benefits of a hearty breakfast.\nThe results were sent to the media culminating in headlines proclaiming “4,500 Physicians Advocate Heavy Breakfast for Improved Health.” This endorsement, along with the suggestion of incorporating bacon and eggs into breakfast, had a significant impact on public perception.\nIt’s worth noting that the bacon product itself was not in question. Rather, Bernays utilized the opportunity to align the product with a broader health message, successfully increasing bacon sales.\n\nWhat is wrong with the question and the conclusion?\n\nWhat is better for health, a slight or substantial breakfast?\n\nSlight\nSubstantial\n\n“4,500 physicians urge heavy breakfast in order to improve the health of the American people”\nHave more Eggs and Bacon for breakfast!\n\n\n\nBacon"
  },
  {
    "objectID": "chapters/good_questions.html#types-of-question-formats",
    "href": "chapters/good_questions.html#types-of-question-formats",
    "title": "22  Design Questions",
    "section": "22.4 Types of Question Formats",
    "text": "22.4 Types of Question Formats\nDifferent types of question formats in a questionnaire enable capturing diverse data types and facilitate varied response types from participants. Three commonly used question formats are:\n\nRadio Buttons: Ideal for questions that require respondents to select only one answer from a list of options. The respondent chooses a single option by clicking on the corresponding radio button.\nCheckboxes: Suitable for questions that allow respondents to select multiple answers from a list of options. The respondent can select multiple checkboxes to indicate their choices.\nText Boxes: Used when respondents need to provide a written response or input data not covered by predefined options. Text boxes allow users to type in their responses freely.\n\nProperly utilizing these question formats ensures that your questionnaire captures the necessary information and provides clear response options. However, it’s essential to provide clear instructions on how participants should input their responses, else the data might be susceptible to interpretation.\n \nThe choice of question format should align with the nature of the question and the type of response you are seeking."
  },
  {
    "objectID": "chapters/good_questions.html#likert-scales",
    "href": "chapters/good_questions.html#likert-scales",
    "title": "22  Design Questions",
    "section": "22.5 Likert Scales",
    "text": "22.5 Likert Scales\nLikert scales are commonly used in surveys to measure attitudes, opinions, and perceptions. Here are some considerations when choosing the number of scale points:\n\n5 or 7 Points: Both 5-point and 7-point scales are widely used and offer sufficient response options for capturing a range of opinions. There is generally no significant difference in response patterns between these two options.\nConsider Respondent Ease: Using a lower number of scale points, such as 5, makes it easier for respondents to quickly choose their answer. This leads to higher response rates and less respondent fatigue.\nHigher Points for Specific Needs: In some cases, a higher number of scale points may be necessary to capture nuanced responses or when greater precision is required. This could be relevant in specialized research or when examining highly specific attitudes or behaviors.\nFollow Established Protocols: If your survey is part of a larger study or research field, it is advisable to use the scale points commonly used in that context. This ensures consistency and comparability with existing research.\n\n\nAt the end, the person who decides the scale is the one who pays ;)\n\nUltimately, the decision on the number of scale points rests with the survey designer, considering specific research goals, target audience, and practical considerations.\nFor a guide on constructing questions with Likert scales, you can refer to this document based on Vagias (2006).\n\nVagias, Wade M. 2006. “Likert-Type Scale Response Anchors.” Clemson International Institute for Tourism & Research Development, Department of Parks, Recreation and Tourism Management. Clemson University."
  },
  {
    "objectID": "chapters/good_questions.html#i-dont-know-and-na-options",
    "href": "chapters/good_questions.html#i-dont-know-and-na-options",
    "title": "22  Design Questions",
    "section": "22.6 “I don’t know” and N/A Options",
    "text": "22.6 “I don’t know” and N/A Options\nIncluding “I don’t know” and N/A (not applicable) response options in surveys can impact data quality. Some considerations are:\n\nPotential Impact on Data Quality: Inclusion of “I don’t know” and N/A options may not necessarily improve data quality. It can sometimes limit the measurement of meaningful opinions by providing an ‘easy way out’ for respondents who might otherwise give a thoughtful response.\nInfluence of Mental Fatigue: Respondents experiencing fatigue or uncertainty about their opinions may select neutral options like “I don’t know,” resulting in a higher proportion of neutral responses and potentially masking genuine attitudes.\nOptional Questions: All questions in a survey are optional. Respondents can choose to skip questions they do not want to answer or do not apply to them. Making questions optional allows individuals to provide answers only to the questions they feel comfortable or knowledgeable answering.\n\nIt’s crucial to consider these factors when designing surveys and deciding whether to include “I don’t know” and N/A response options. Krosnick et al. (2002) explored the impact of “no opinion” response options and their effects on data quality, highlighting potential trade-offs associated with their inclusion.\n\nKrosnick, Jon A, Allyson L Holbrook, Matthew K Berent, Richard T Carson, W Michael Hanemann, Raymond J Kopp, Robert Cameron Mitchell, Stanley Presser, Paul A Ruud, and V Kerry Smith. 2002. “The Impact of\" No Opinion\" Response Options on Data Quality: Non-Attitude Reduction or an Invitation to Satisfice?” Public Opinion Quarterly 66 (3): 371–403.\nIn summary, survey designers should consider the specific research goals, target population, and potential effects on response patterns when deciding whether to include these response options in their surveys."
  },
  {
    "objectID": "chapters/good_questions.html#survey-length",
    "href": "chapters/good_questions.html#survey-length",
    "title": "22  Design Questions",
    "section": "22.7 Survey Length",
    "text": "22.7 Survey Length\nThe length of a survey can have significant implications for participant engagement and data quality. Below are some key points to keep in mind:\n\nDrop-out Rates: In web-based university studies (Hoerger 2010), there is typically an instantaneous drop-out rate of around 10%, with an additional 2% drop for every 100 survey items. Consequently, longer surveys could lead to higher attrition rates as participants might become fatigued or lose interest.\nSatisficing and Quality of Responses: Lengthy surveys, particularly in lab-based research, can result in satisficing behavior (Vriesema and Gehlbach 2021) where participants offer “good enough” responses instead of providing thoughtful and accurate answers. This could negatively impact data quality. However, these adverse effects may not significantly affect the overall results (Bansak et al. 2021).\nOnline Marketing Questionnaires: For online marketing surveys, keeping the survey length short is advisable (Wigmore 2022). Surveys longer than a few questions are likely to experience significant drop-out rates. Surveys taking longer than 10 minutes are strongly discouraged to maintain participant engagement.\n\n\nHoerger, Michael. 2010. “Participant Dropout as a Function of Survey Length in Internet-Mediated University Studies: Implications for Study Design and Voluntary Participation in Psychological Research.” Cyberpsychology, Behavior, and Social Networking 13 (6): 697–700.\n\nVriesema, Christine Calderon, and Hunter Gehlbach. 2021. “Assessing Survey Satisficing: The Impact of Unmotivated Questionnaire Responding on Data Quality.” Educational Researcher 50 (9): 618–27.\n\nBansak, Kirk, Jens Hainmueller, Daniel J Hopkins, and Teppei Yamamoto. 2021. “Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments.” Political Science Research and Methods 9 (1): 53–71.\n\nWigmore, Steve. 2022. “What Is a Good Survey Length for Online Research?” https://www.kantar.com/north-america/inspiration/research-services/what-is-a-good-survey-length-for-online-research-pf.\nSurvey designers should strive to balance between acquiring the necessary information and keeping the survey manageable for respondents to ensure optimal participant engagement and data quality."
  },
  {
    "objectID": "chapters/good_questions.html#survey-invitation",
    "href": "chapters/good_questions.html#survey-invitation",
    "title": "22  Design Questions",
    "section": "22.8 Survey Invitation",
    "text": "22.8 Survey Invitation\nWhen inviting participants to a survey, it’s crucial to establish trust and provide necessary information. Here are some key elements to consider:\n\nIntroduction: Clearly identifying yourself and explaining the reason for reaching out to the specific person can help build trust and credibility.\nPurpose and Benefits: Outlining the purpose of the survey and the benefits of participation can motivate the respondents. Explain how their responses will contribute to the research or decision-making process.\nIncentives: If applicable, mentioning any incentives or rewards offered for participating in the survey can help motivate individuals to complete the survey.\nSurvey Overview: Providing an overview of the topics or questions covered in the survey helps manage expectations and assist participants in managing their time.\nClosing Time: Specifying the deadline or closing time for the survey completion can create a sense of urgency and ensure timely responses.\n\nIn addition to these, consider adding a “Thank You” page after participants complete the survey. This shows appreciation for their time and participation.\nA clear and concise privacy statement addressing privacy concerns can assure participants that their responses will remain confidential and used solely for research purposes.\nLastly, outlining your follow-up plan can let participants know if and how you will share the survey results or any actions that will be taken based on the findings.\nIncorporating these elements into your survey invitation can improve participant engagement and enhance the overall survey experience."
  },
  {
    "objectID": "chapters/survey_tools.html",
    "href": "chapters/survey_tools.html",
    "title": "23  Survey Tools",
    "section": "",
    "text": "Researchers have a wide array of survey tools to choose from, each offering unique features and advantages. Understanding these options is crucial to ensuring effective data collection, thus leading to insightful results. This guide will present both physical and digital survey tools, as well as platforms for participant recruitment.\n\n23.0.1 Physical Survey Methods\nTraditional physical survey methods provide diverse ways to collect data, especially beneficial in situations where digital access may be limited.\n\nPaper Surveys: These are versatile and can be distributed in various settings, reaching a broad audience.\nIn-person Interviews: This method allows for direct participant interaction, clarification of complex questions, and collection of nuanced responses.\nTelephone Surveys: These surveys are efficient for collecting data from geographically dispersed samples, but potential biases should be considered, such as excluding individuals without telephones.\nMail Surveys: This approach reaches a wide range of individuals, especially those without internet access. Response rates and turnaround times may vary.\nIntercept Surveys: Involves approaching individuals in public spaces for participation, allowing for immediate data collection, yet potential issues with representativeness and privacy may arise.\n\nFor all physical methods, adherence to ethical guidelines for data collection and participant privacy is imperative.\n\n\n23.0.2 Digital Survey Tools\nA variety of digital survey platforms cater to different research needs, providing an array of features, usability levels, and costs.\n\nGoogle Forms: Offers user-friendly and free services, suitable for simple surveys or budget-constrained projects.\nTypeform: Provides an interactive interface for engaging surveys, enhancing user experience and response rates.\nQualtrics: A comprehensive platform favoring complex survey design with advanced features like conditional logic, built-in distribution capabilities, collaboration features, and extensive customization. Many universities have institutional subscriptions to Qualtrics, easing the financial burden on individual researchers.\nSMARTRIQS: An open-source tool that integrates with Qualtrics for conducting interactive online experiments.\nLimeSurvey: A customizable, open-source survey tool ideal for complex survey flows, especially when Qualtrics is not available.\nO-tree: Designed specifically for economic experiments and social science research with features like decision-making games, real-time data collection, and payment processing.\nZ-Tree: Z-Tree (Zurich Toolbox for Readymade Economic Experiments) is a software for developing and conducting economic experiments. It provides a control language designed to be easy to learn and use, enabling users to quickly create complex experiments.\nChoiceFlow: ChoiceFlow is a framework for designing and running complex behavioral experiments online. It supports a wide range of experimental designs and integrates seamlessly with major online labor marketplaces, enabling researchers to efficiently recruit and pay participants.\n\n\n\n23.0.3 Participant Recruitment Platforms\nPlatforms like Amazon’s Mechanical Turk (MTurk), Prolific, and Respondent offer an additional dimension to survey tools, serving as spaces for recruiting and compensating participants.\n\nAmazon’s Mechanical Turk (MTurk): A crowdsourcing platform providing a large, diverse participant pool at a relatively low cost, albeit with considerations for fair compensation and data quality.\nProlific: Similar to MTurk, but specifically designed for academic research with pre-screened participants and emphasis on ethical standards.\nRespondent: Specializes in finding professionals for high-quality research, particularly useful for B2B market research or studies requiring professional expertise.\n\nIn conclusion, the choice of survey tool and recruitment platform should align with the specific needs, goals, and ethical considerations of the research. Be it simplicity, advanced features, interactive experiments, or specific subject area focus, the correct tool can significantly enhance the effectiveness and insightfulness of the research."
  },
  {
    "objectID": "chapters/api.html#monsieur",
    "href": "chapters/api.html#monsieur",
    "title": "\n24  APIs\n",
    "section": "\n24.1 Monsieur!",
    "text": "24.1 Monsieur!\nCommunicating with an API involves sending a request with the required data. There are several types of requests you can make:\n\nGET Request: This is akin to asking a waiter for a meal. It involves requesting data without making changes. In the context of APIs, a GET request fetches information from the server.\nPOST Request: This is analogous to ordering a new dish to be added to a menu. It entails giving the API precise instructions to ask the server to add a new item. In API terminology, a POST request transmits data to the server, generating a new entry.\nPUT or PATCH Request: Suppose you want to adjust your order, say, add extra cheese to your burger. This action corresponds to a PUT or PATCH request in the API domain, used to update existing information. While PUT updates the entire record, PATCH only updates the specific parts indicated.\nDELETE Request: Imagine canceling and removing an item from your order after receiving your meal. This action aligns with a DELETE request, which deletes existing data from the server.\n\nThese basic types of HTTP requests are akin to your interaction with a waiter at a restaurant. Just as clear communication with the waiter is necessary for a correct order, using the appropriate request type is crucial when dealing with APIs to obtain the required data or perform the desired operation."
  },
  {
    "objectID": "chapters/api.html#your-first-order",
    "href": "chapters/api.html#your-first-order",
    "title": "\n24  APIs\n",
    "section": "\n24.2 Your First Order",
    "text": "24.2 Your First Order\nFundamentally, an API request is similar to your browser downloading a webpage when you open a link. For instance, you can fetch data from the US Census ACS 2021 by following this link: https://api.census.gov/data/2021/acs/acs5?get=NAME,group(B01001)&for=us:1\nUsually, an interface is needed to interact with an API, rather than constructing a text link. There’s a plethora of them, but I’m partial to HTTPie, a CLI tool that also provides online and local applications!\nTo install it, simply run brew install httpie in a terminal if you have Homebrew or follow the instructions on their website, where you can also access their applications.\nWhether you use http in your terminal or go to their website, you can easily make your first API request.\nhttps httpie.io/hello\nThis command will return a header with information and a return message formatted as a JSON string:\n\n{\n    \"ahoy\"` [\n        \"Hello, World! 👋 Thank you for trying out HTTPie 🥳\",\n        \"We hope this will become a friendship.\"\n    ],\n    \"links\"` {\n        \"discord\"` \"https://httpie.io/discord\",\n        \"github\"` \"https://github.com/httpie\",\n        \"homepage\"` \"https://httpie.io\",\n        \"twitter\"` \"https://twitter.com/httpie\"\n    }\n}\n\n\nJSON (JavaScript Object Notation) is a light, readable data format that both humans and machines can readily parse and generate. Its structure consists of key-value pairs and arrays, making it a prevalent choice in web APIs and data interchange.\n\nAn API request mainly includes three components: the request endpoint, header, and body.\n\nAn API endpoint refers to a specific URL or URI of a web service, serving as the access point for making requests and receiving responses from the API.\nRequest Header: This contains supplemental details about the request, such as metadata or server instructions. It includes information like the HTTP method (GET, POST, PUT, DELETE), content type, authentication credentials, and more. Headers provide context or control the behavior of the API request.\nRequest Body: This carries any data or parameters that need to be sent to the server with the API request. Primarily used for methods like POST, PUT, or PATCH, the body can contain various data formats such as JSON, XML, form data, or binary data, depending on the API’s requirements.\n\nTogether, the request header and body allow for effective communication with an API, specifying the necessary information, desired action, data to be sent, and any additional instructions or authentication details.\nLet’s add a header and body to a request:\nhttp PUT pie.dev/put X-API-Token:123 name=John\nHere, we’re using a PUT method to send data to an API endpoint pie.dev/put, adding an X-API-Token 123 header, and specifying a ‘name’ field value as ‘John’ in the body. You will receive a JSON response with details on the successful POST request!\nIf this seems overwhelming, or you prefer a more convenient way to interact with APIs, I recommend using the HTTPie application or a service like PostMan, which provides a GUI."
  },
  {
    "objectID": "chapters/api.html#utilizing-apis-in-r",
    "href": "chapters/api.html#utilizing-apis-in-r",
    "title": "\n24  APIs\n",
    "section": "\n24.3 Utilizing APIs in R",
    "text": "24.3 Utilizing APIs in R\nIn R, APIs open the door to a wealth of intriguing datasets for your data analysis projects. Let’s delve into API implementation in R using the httr and jsonlite packages.\n\ninstall.packages(c(\"httr\", \"jsonlite\"))\n\nOnce installed, load the packages:\n\nlibrary(httr)\nlibrary(jsonlite)\n\nLet’s start interacting with APIs using the GET() function from the httr package. We’ll extract data from the Gutendex API, offering access to Project Gutenberg ebook metadata. The code below fetches data from the API, particularly authors alive before 500 BCE (?author_year_end=-499):\n\nres &lt;- GET(\"https://gutendex.com/books?author_year_end=-499\") \n\nThe GET() function dispatches the HTTP GET request to the specified URL, and the API response gets stored in the res variable.\nInspect the response by printing the res object:\n\nprint(res)\n\nResponse [https://gutendex.com/books/?author_year_end=-499]\n  Date: 2023-12-09 02:36\n  Status: 200\n  Content-Type: application/json\n  Size: 35.2 kB\n\n\nThe next step is to parse the JSON response using jsonlite’s fromJSON() function:\n\ndata &lt;- fromJSON(rawToChar(res$content))\n\nrawToChar() transmutes the raw Unicode content of the response into a character vector. Subsequently, fromJSON() transforms the character vector into a structured R object, such as a list or a data frame.\nDive into the acquired data by accessing its elements. For instance, you can display the book titles:\n\nhead(data$results$title, n = 5)\n\n[1] \"The Iliad\"                                                                                 \n[2] \"The Odyssey: Rendered into English prose for the use of those who cannot read the original\"\n[3] \"The Iliad\"                                                                                 \n[4] \"La Odisea\"                                                                                 \n[5] \"The Odyssey\"                                                                               \n\n\nVoila! You’ve successfully made an API request in R and fetched data for further exploration.\n\n24.3.1 Incorporating Request Body and Headers\nTo add a request body with the query parameter. In this example, we want authors alive before 500 BCE, Epic Poetry in French, and we add headers using the add_headers() function. Here as an example I set Key to Your Key:\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Execute the POST request with the request body\nresponse &lt;- GET(\n  url = \"https://gutendex.com/books\",\n  query = list(\n    author_year_end = -499,\n    topic = \"Epic Poetry\",\n    languages = \"fr\"\n  ),\n  add_headers(\"Key\" = \"Your Key\")\n)\n\nresponse |&gt; \n  getElement(\"content\") |&gt; \n  rawToChar() |&gt; \n  fromJSON() |&gt; \n  getElement(\"results\") |&gt; \n  getElement(\"title\")\n\n[1] \"L'Odyssée\" \"L'Iliade\"  \"L'Odyssée\"\n\n\nBy incorporating a request body or headers, you can furnish extra data or authentication details to your API requests.\nAlways refer to the API documentation for specific requirements and the headers available.\nKeep harnessing the power of APIs to access novel and compelling data sources for your projects!"
  },
  {
    "objectID": "chapters/google.html#constructing-a-database-in-google-sheets",
    "href": "chapters/google.html#constructing-a-database-in-google-sheets",
    "title": "\n25  Google Services in R\n",
    "section": "\n25.1 Constructing a Database in Google Sheets",
    "text": "25.1 Constructing a Database in Google Sheets\nGoogle Sheets is a versatile tool, easy to use, cloud-based, and capable of storing data from Google Forms. It’s an excellent choice for data entry if your project involves physical data collection. By building a relational database in Sheets with multiple tabs, validation rules, and clear data entry instructions, you can streamline the process, minimize errors, and maintain meaningful variable names for later analysis.\nAdopt a systematic naming framework like question#_questionName_section to facilitate easy access to required questions when creating your indexes.\n\n25.1.1 Common Use Cases of googlesheets4\n\ngooglesheets4 is an R package that interacts with the Google Sheets API, offering a host of features to analysts and data scientists. Here are some common scenarios:\n\nData Input: Data is often input into Google Sheets by various people, thanks to its collaborative nature. googlesheets4 allows you to pull this data directly into R.\nReal-Time Data Analysis: If your data is continuously updated, googlesheets4 allows real-time analysis by accessing and analyzing the latest data.\nData Reporting: Results from your analysis or model predictions can be written back to Google Sheets, making them accessible to non-technical stakeholders.\nData Sharing: Google Sheets makes data sharing easy. Your data can be accessed from anywhere, anytime.\n\n25.1.2 Reading and Writing Data in Google Sheets with R\nWith R, you can effortlessly read and write data from and to Google Sheets.\n\n25.1.2.1 Reading from Google Sheets\nTo pull data from Google Sheets into R, use the read_sheet() function, providing the document’s URL or identifier and the specific sheet name.\n\nlibrary(googlesheets4)\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/your_spreadsheet_id_here\"\ndata &lt;- read_sheet(url, sheet = \"Your Sheet Name\")\nhead(data)\n\n\n25.1.2.2 Writing to Google Sheets\nWriting data back to Google Sheets is just as straightforward using the write_sheet() function.\n\ndata_to_write &lt;- data.frame(\n  Column1 = c(\"Data1\", \"Data2\"),\n  Column2 = c(\"Data3\", \"Data4\")\n)\n\nwrite_sheet(data = data_to_write, ss = url, sheet = \"Your Sheet Name\")\n\n\n25.1.3 Managing Secrets with gargle\n\nThe gargle package simplifies authentication with Google APIs and safely manages authentication tokens.\n\noptions(gargle_oauth_cache = \".secrets\")\ngs4_auth()\nlist.files(\".secrets/\")\ngs4_auth(cache = \".secrets\", email=\"name@mail.com\")\n\nUsing gargle enhances your workflow efficiency and security by storing tokens securely and simplifying re-authentication in future sessions.\n\n25.1.4 Google Drive and R\nThe googledrive package in R allows you to interact with Google Drive, making it easy to read, write, and manage files.\n\nlibrary(googledrive)\ndrive_auth(cache = \".secrets_drive\")\nprint(drive_ls())\nfile &lt;- drive_get(path = \"YourFileName\")\ndata &lt;- read_csv(drive_download(file, overwrite = TRUE))\n\nTo write files to Google Drive, save the file locally, then upload it using drive_upload():\n\ndata_to_write &lt;- data.frame(\n  Column1 = c(\"Data1\", \"Data2\"),\n  Column2 = c(\"Data3\", \"Data4\")\n)\n\nwrite.csv(data_to_write, \"data_to_write.csv\")\ndrive_upload(\"data_to_write.csv\", path = \"FolderName/data_to_write.csv\")\n\nRemember to responsibly handle your files, especially those containing sensitive data.\n\n25.1.4.1 Exploring Other Google Services\nNumerous Google services like Google Maps, Google Earth, Google Cloud, and others are readily accessible from within R. A quick online search will reveal various resources for your projects, and there’s likely a pre-existing R package to simplify your access."
  },
  {
    "objectID": "chapters/qualtrics_api.html#core-functions",
    "href": "chapters/qualtrics_api.html#core-functions",
    "title": "\n26  Qualtrics API\n",
    "section": "\n26.1 Core Functions",
    "text": "26.1 Core Functions\nCurrently, the package contains three core functions:\n\n\nall_surveys() shows surveys you can access.\n\nfetch_survey() downloads the survey.\n\nread_survey() reads CSV files you downloaded manually from Qualtrics.\n\nIt also contains a number of helper functions, including:\n\n\nqualtrics_api_credentials() stores your API key and base url in environment variables.\n\nsurvey_questions() retrieves a data frame containing questions and question IDs for a survey;\n\nextract_colmap() retrieves a similar data frame with more detailed mapping from columns to labels.\n\nmetadata() retrieves metadata about your survey, such as questions, survey flow, number of responses etc.\n\nNote that you can only export surveys that you own, or to which you have been given administration rights."
  },
  {
    "objectID": "chapters/qualtrics_api.html#connecting-to-the-api",
    "href": "chapters/qualtrics_api.html#connecting-to-the-api",
    "title": "\n26  Qualtrics API\n",
    "section": "\n26.2 Connecting to the API",
    "text": "26.2 Connecting to the API\nIf you have received API access, now you can connect to the API. To get api_key and base_url go to Qualtrics Home Page &gt; Account Settings &gt; Qualtrics IDs or click this link. Under “API” click “Generate Topic” and you will be issued a Token. Copy this string a put in “YOUR_API_KEY”. Then look at “User” module, copy “Datacenter ID” and “.qualtrics.com” after it. Your Base Url should look something like this: “lad2.qualtrics.com”.\n\nqualtrics_api_credentials(\n  api_key = \"YOUR_API_KEY\",\n  base_url = \"YOUR_BASE_URL\",\n  install = TRUE,\n  # overwrite = TRUE # If you need to update your credentials\n)\n\n\nqualtrics_api_credentials(\n  api_key = \"CEOj8Iwh6BUTNZ9J7PXDJLZpStQaXkmlwLPYzCgu\",\n  base_url = \"lad2.qualtrics.com\",\n  install = TRUE\n)\n\nAfter qualtrics_api_credintials stored your credentials, you can use all_surveys() to fetch information on your surveys.\n\n(surveys &lt;- all_surveys()) \n\n# A tibble: 8 × 6\n  id                 name                        ownerId lastM…¹ creat…² isAct…³\n  &lt;chr&gt;              &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt;  \n1 SV_0rearXjH2Ri6umq Student Satisfaction        UR_2tz… 2023-0… 2023-0… FALSE  \n2 SV_2gWVWLn2vCCDJGu Luvuyo                      UR_2tz… 2022-0… 2022-0… FALSE  \n3 SV_3h0HPHQbJStqb8a Pilot (USA) - Giver Motive… UR_2tz… 2022-0… 2022-0… FALSE  \n4 SV_3Q011O6gNqwn1Nc Pen Experiment              UR_2tz… 2022-1… 2022-1… FALSE  \n5 SV_4YDoTFLN61nKecS Pen_Showcase                UR_2tz… 2023-0… 2023-0… TRUE   \n6 SV_4Yhvzriag8syz7U Prosociality Experiment     UR_2tz… 2023-0… 2022-0… FALSE  \n7 SV_bJIs8lwz4CfAAgS test                        UR_2tz… 2023-0… 2023-0… TRUE   \n8 SV_dnBQ3YB4rLC03J4 test2                       UR_2tz… 2023-0… 2023-0… FALSE  \n# … with abbreviated variable names ¹​lastModified, ²​creationDate, ³​isActive\n\n\nOnce you select the questionnaire you want you can refer to it using id. If you want redownload the data set force_request = TRUE, otherwise it will load prior saved download.\n\n\n\n\nsurvey_data &lt;- fetch_survey(\n  surveyID = \"SV_bJIs8lwz4CfAAgS\",\n  verbose = FALSE,\n  force_request = TRUE\n)\nsurvey_data %&gt;% glimpse()\n\nRows: 22\nColumns: 89\n$ StartDate                 &lt;dttm&gt; 2023-02-07 16:00:52, 2023-02-07 16:01:17, 2…\n$ EndDate                   &lt;dttm&gt; 2023-02-07 16:04:08, 2023-02-07 16:04:08, 2…\n$ Status                    &lt;chr&gt; \"IP Address\", \"IP Address\", \"IP Address\", \"I…\n$ IPAddress                 &lt;chr&gt; \"138.202.129.171\", \"138.202.129.164\", \"138.2…\n$ Progress                  &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ `Duration (in seconds)`   &lt;dbl&gt; 196, 171, 104, 187, 189, 198, 210, 209, 180,…\n$ Finished                  &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ RecordedDate              &lt;dttm&gt; 2023-02-07 16:04:09, 2023-02-07 16:04:09, 2…\n$ ResponseId                &lt;chr&gt; \"R_3r385toH6wjBmyr\", \"R_2usVBjQ7yXrmY72\", \"R…\n$ RecipientLastName         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RecipientFirstName        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RecipientEmail            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ExternalReference         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LocationLatitude          &lt;dbl&gt; 37.78, 37.78, 37.78, 37.78, 37.78, 37.78, 37…\n$ LocationLongitude         &lt;dbl&gt; -122.465, -122.465, -122.465, -122.465, -122…\n$ DistributionChannel       &lt;chr&gt; \"anonymous\", \"anonymous\", \"anonymous\", \"anon…\n$ UserLanguage              &lt;chr&gt; \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"EN\", \"E…\n$ Consent                   &lt;ord&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ Gender                    &lt;ord&gt; Female, Female, Male, Male, Male, Male, Male…\n$ Name                      &lt;chr&gt; \"Tasha\", \"Khushboo Patel\", \"Nikita\", \"Lawren…\n$ Competitive               &lt;ord&gt; Competitive, Competitive, Competitive, Compe…\n$ Pizzas_1                  &lt;chr&gt; \"Margherita\", NA, NA, \"Margherita\", \"Margher…\n$ Pizzas_2                  &lt;chr&gt; \"Pepperoni\", NA, NA, \"Pepperoni\", \"Pepperoni…\n$ Pizzas_3                  &lt;chr&gt; NA, \"BBQ Chicken\", NA, NA, \"BBQ Chicken\", \"B…\n$ Pizzas_4                  &lt;chr&gt; \"Hawaiian\", NA, NA, NA, \"Hawaiian\", NA, \"Haw…\n$ Pizzas_5                  &lt;chr&gt; \"Veggie\", \"Veggie\", \"Veggie\", \"Veggie\", \"Veg…\n$ Pizzas_6                  &lt;chr&gt; NA, NA, NA, NA, NA, \"I also love that one:\",…\n$ Pizzas_7                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Pizzas_6_TEXT             &lt;chr&gt; NA, NA, NA, NA, NA, \"Garlic White Chinese be…\n$ Pizzas_DO_1               &lt;dbl&gt; 5, 1, 2, 4, 5, 3, 1, 1, 5, 5, 3, 1, 5, 1, 4,…\n$ Pizzas_DO_2               &lt;dbl&gt; 3, 3, 5, 1, 3, 2, 3, 3, 1, 4, 2, 5, 3, 3, 5,…\n$ Pizzas_DO_3               &lt;dbl&gt; 2, 2, 3, 5, 4, 1, 4, 5, 2, 2, 1, 3, 1, 2, 2,…\n$ Pizzas_DO_4               &lt;dbl&gt; 1, 4, 1, 3, 2, 5, 2, 2, 3, 1, 5, 4, 2, 5, 1,…\n$ Pizzas_DO_5               &lt;dbl&gt; 4, 5, 4, 2, 1, 4, 5, 4, 4, 3, 4, 2, 4, 4, 3,…\n$ Pizzas_DO_6               &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ Pizzas_DO_7               &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n$ `1_Taste`                 &lt;dbl&gt; 5, NA, NA, 5, 3, NA, 3, 4, 4, 5, 4, NA, NA, …\n$ `1_Healthiness`           &lt;dbl&gt; 5, NA, NA, 4, 4, NA, 2, 3, 3, 5, 2, NA, NA, …\n$ `1_Ease_Of_Preparation`   &lt;dbl&gt; 5, NA, NA, 4, 4, NA, 4, 3, 4, 3, 3, NA, NA, …\n$ `2_Taste`                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `2_Healthiness`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `2_Ease_Of_Preparation`   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `3_Taste`                 &lt;dbl&gt; NA, NA, NA, NA, NA, 4, 5, NA, 4, NA, NA, NA,…\n$ `3_Healthiness`           &lt;dbl&gt; NA, NA, NA, NA, NA, 3, 3, NA, 2, NA, NA, NA,…\n$ `3_Ease_Of_Preparation`   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, 4, NA, 3, NA, NA, NA,…\n$ `9_Taste`                 &lt;dbl&gt; 5, NA, NA, 5, 5, NA, 5, NA, NA, NA, 5, NA, 4…\n$ `9_Healthiness`           &lt;dbl&gt; 3, NA, NA, 3, 2, NA, 2, NA, NA, NA, 2, NA, 4…\n$ `9_Ease_Of_Preparation`   &lt;dbl&gt; 5, NA, NA, 5, 4, NA, 4, NA, NA, NA, 4, NA, N…\n$ `10_Taste`                &lt;dbl&gt; NA, 5, NA, NA, 5, 5, 3, 4, 4, NA, 4, NA, 4, …\n$ `10_Healthiness`          &lt;dbl&gt; NA, NA, NA, NA, 2, 3, 2, 3, 3, NA, 2, NA, 4,…\n$ `10_Ease_Of_Preparation`  &lt;dbl&gt; NA, NA, NA, NA, 3, 3, 2, 3, 2, NA, 3, NA, NA…\n$ `11_Taste`                &lt;dbl&gt; 5, NA, NA, NA, 4, NA, 5, 4, NA, NA, 4, 5, NA…\n$ `11_Healthiness`          &lt;dbl&gt; 4, NA, NA, NA, 2, NA, 2, 3, NA, NA, 3, 4, NA…\n$ `11_Ease_Of_Preparation`  &lt;dbl&gt; 5, NA, NA, NA, 3, NA, 3, 2, NA, NA, 2, 3, NA…\n$ `12_Taste`                &lt;dbl&gt; 5, 4, 4, 4, 4, NA, 3, NA, NA, NA, 4, NA, NA,…\n$ `12_Healthiness`          &lt;dbl&gt; 5, 2, 5, 5, 4, NA, 3, NA, NA, NA, 4, NA, NA,…\n$ `12_Ease_Of_Preparation`  &lt;dbl&gt; 4, 3, 3, 3, 2, NA, 2, NA, NA, NA, 3, NA, NA,…\n$ `match timer_First Click` &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 20.317, 0.000, 0…\n$ `match timer_Last Click`  &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 20.317, 0.000, 0…\n$ `match timer_Page Submit` &lt;dbl&gt; 8.296, 12.572, 7.955, 22.206, 21.188, 7.224,…\n$ `match timer_Click Count` &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ transfer                  &lt;dbl&gt; 50, NA, 35, NA, 49, NA, NA, 35, NA, 10, 50, …\n$ `get timer_First Click`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `get timer_Last Click`    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `get timer_Page Submit`   &lt;dbl&gt; 4.999, 15.967, 32.000, 2.787, 3.006, 10.538,…\n$ `get timer_Click Count`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ decision...67             &lt;chr&gt; NA, \"I accept A's offer.\\n(You get ${e://Fie…\n$ Q23                       &lt;ord&gt; Yes, Absolutely, Absolutely, Yes, Yes, Yes, …\n$ researcherID              &lt;chr&gt; \"hTtZNw8TA0\", \"hTtZNw8TA0\", \"hTtZNw8TA0\", \"h…\n$ studyID                   &lt;chr&gt; \"test\", \"test\", \"test\", \"test\", \"test\", \"tes…\n$ groupID                   &lt;dbl&gt; 6, 6, 5, 5, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11…\n$ participantID             &lt;chr&gt; \"R_3r385toH6wjBmyr\", \"R_2usVBjQ7yXrmY72\", \"R…\n$ groupSize                 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ numStages                 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ roles                     &lt;chr&gt; \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A,B\", \"A…\n$ participantRole           &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\",…\n$ timeOutLog                &lt;chr&gt; \"OK -- no issues\", \"OK -- no issues\", \"OK --…\n$ botMatch                  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ total                     &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100,…\n$ offer                     &lt;dbl&gt; 50, 50, 35, 35, 49, 49, 35, 35, 10, 10, 50, …\n$ decision...81             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ payoff                    &lt;dbl&gt; 50, 50, 65, 35, 51, 49, 35, 65, 10, 90, 50, …\n$ sendStage                 &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2,…\n$ sendData                  &lt;chr&gt; \"offer\", \"decision\", \"offer\", \"decision\", \"o…\n$ getStage                  &lt;dbl&gt; 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1,…\n$ getData                   &lt;chr&gt; \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\",…\n$ defaultData               &lt;dbl&gt; 2, 20, 2, 5, 2, 71, 61, 1, 83, 2, 2, 81, 1, …\n$ saveData                  &lt;chr&gt; \"decision\", \"offer\", \"decision\", \"offer\", \"d…\n$ randomPercent             &lt;dbl&gt; NA, 20, NA, 5, NA, 71, 61, NA, 83, NA, NA, 8…\n\n\nIn case you want to see text of the questions use survey_questions().\n\nsurvey_questions &lt;- survey_questions(surveyID = \"SV_bJIs8lwz4CfAAgS\")\nhead(survey_questions, n = 5)\n\n# A tibble: 5 × 4\n  qid   qname        question                                         force_resp\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;                                            &lt;lgl&gt;     \n1 QID25 Introduction \"Welcome to the &lt;strong&gt;University of San Franc… FALSE     \n2 QID26 Consent      \"Do you agree to participate in the survey?\"     TRUE      \n3 QID21 Gender       \"What is your gender\"                            TRUE      \n4 QID23 Name         \"What is your Name?\"                             FALSE     \n5 QID22 Competitive  \"Would you consider yourself competitive?\"       FALSE"
  },
  {
    "objectID": "chapters/qualtrics_api.html#example",
    "href": "chapters/qualtrics_api.html#example",
    "title": "\n26  Qualtrics API\n",
    "section": "\n26.3 Example",
    "text": "26.3 Example\n\nsurvey_data &lt;- fetch_survey(\n  surveyID = \"SV_bJIs8lwz4CfAAgS\",\n  verbose = FALSE, force_request = TRUE\n)\n\nsurvey_data &lt;- survey_data %&gt;% janitor::clean_names()\n\ngraph_data &lt;- survey_data %&gt;%\n  select(gender, offer, decision_81, participant_id, participant_role) %&gt;%\n  mutate(\n    participant_role = recode(participant_role, \"A\" = \"dictator\", \"B\" = \"recipient\"),\n    decision = recode(decision_81, \"1\" = \"Accepted\", \"2\" = \"Declined\")\n  ) %&gt;%\n  mutate(interval = cut_width(offer, width = 10, center = 45)) %&gt;%\n  filter(participant_role == \"recipient\") %&gt;%\n  count(decision, interval)\n\n\nShow the codegraph_data %&gt;%\n  ggplot(aes(x = interval, y = n, fill = as.factor(decision))) +\n  geom_col(position = position_stack()) +\n  theme_minimal(base_size = 20) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = max(graph_data$n))) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    legend.position = \"top\"\n  ) +\n  labs(x = \"Offer size\", y = \"Count\", title = \"Results of the Ultimatum Game\", fill = \"Result\")"
  },
  {
    "objectID": "chapters/documentation.html#principles-of-documentation",
    "href": "chapters/documentation.html#principles-of-documentation",
    "title": "27  Document",
    "section": "27.1 Principles of Documentation",
    "text": "27.1 Principles of Documentation\nA robust documentation system rests on several foundational principles:\n\nTraceability: Each process step should be traceable.\nHierarchical structure: Every subordinate unit should be connected to its parent unit.\nPhysical accessibility: All relevant information should be physically accessible.\nConsistent formatting: Consistency in visual and semantic format across all documentation units is crucial.\nEase of implementation: The system should be easy to maintain and implement.\nConvertibility: All documentation should be readily convertible into electronic format.\nCentralization: Documentation should be centralized with minimal ambiguity.\nRedundancy: Key information should be replicated across multiple places or formats.\nResilience: Documentation should be designed to retain crucial information even in disarray."
  },
  {
    "objectID": "chapters/documentation.html#hierarchical-structure-of-documentation",
    "href": "chapters/documentation.html#hierarchical-structure-of-documentation",
    "title": "27  Document",
    "section": "27.2 Hierarchical Structure of Documentation",
    "text": "27.2 Hierarchical Structure of Documentation\nThe documentation hierarchy consists of site, session, and subject levels, each defined by unique identifiers and variables. This hierarchy binds all materials and systems, forming the structure of your project.\n\n\n\nDocumentation Hierarchy"
  },
  {
    "objectID": "chapters/documentation.html#physical-documentation",
    "href": "chapters/documentation.html#physical-documentation",
    "title": "27  Document",
    "section": "27.3 Physical Documentation",
    "text": "27.3 Physical Documentation\nConsistent formatting and clear question design are essential in physical documents to minimize interpretation variations. Aim to limit paper usage to manage costs and logistics effectively. Essential documents like consent forms, activity packets, instructions, and questionnaires should be provided to each subject at every session’s start. They should carry unique identifiers for efficient tracking of participation agreements, which should be collated post-session for easy reference.\nA designated researcher should record session-specific information in a notebook during the session, which aids in completing session-specific fields during data collection."
  },
  {
    "objectID": "chapters/documentation.html#field-journal",
    "href": "chapters/documentation.html#field-journal",
    "title": "27  Document",
    "section": "27.4 Field Journal",
    "text": "27.4 Field Journal\nMaintaining a field journal to record observations regarding the weather, political climate, experiment rounds, site, and other relevant factors is highly advised."
  },
  {
    "objectID": "chapters/documentation.html#electronic-documentation",
    "href": "chapters/documentation.html#electronic-documentation",
    "title": "27  Document",
    "section": "27.5 Electronic Documentation",
    "text": "27.5 Electronic Documentation\nAll information should be duplicated electronically. After each session, information should be transferred into the electronic session sheet, and documents should be scanned, if possible, to prevent data loss."
  },
  {
    "objectID": "chapters/documentation.html#data-storage-and-labeling",
    "href": "chapters/documentation.html#data-storage-and-labeling",
    "title": "27  Document",
    "section": "27.6 Data Storage and Labeling",
    "text": "27.6 Data Storage and Labeling\nFor electronic data collection, storage and labeling are mostly automatic. However, for paper-based data collection, careful labeling is crucial.\n\n\n\nLabeling Example"
  },
  {
    "objectID": "chapters/documentation.html#data-entry",
    "href": "chapters/documentation.html#data-entry",
    "title": "27  Document",
    "section": "27.7 Data Entry",
    "text": "27.7 Data Entry\nEstablish a linear, mechanical process for data entry to minimize errors. Standardize the input of multiple choices, NAs, and unaccounted values."
  },
  {
    "objectID": "chapters/documentation.html#sampling-strategy",
    "href": "chapters/documentation.html#sampling-strategy",
    "title": "27  Document",
    "section": "27.8 Sampling Strategy",
    "text": "27.8 Sampling Strategy\nThe sampling strategy should be explicitly outlined, with the methodologies and technologies to be used detailed. Information regarding each location should be documented."
  },
  {
    "objectID": "chapters/documentation.html#marketing",
    "href": "chapters/documentation.html#marketing",
    "title": "27  Document",
    "section": "27.9 Marketing",
    "text": "27.9 Marketing\nEmploy suitable marketing tools to reach a substantial participant base and maintain good relations with the hosting site. The signup process for events should be standardized. Using external services like Eventbrite is recommended."
  },
  {
    "objectID": "chapters/documentation.html#administration",
    "href": "chapters/documentation.html#administration",
    "title": "27  Document",
    "section": "27.10 Administration",
    "text": "27.10 Administration\nThe administration process greatly influences research outcomes. A detailed script for the experimenter should be provided, delineating what needs to be communicated, response to queries, and other influential factors.\nOn the session day, follow the prescribed procedures to ensure speedy, reliable, and uniform sessions. Be prepared for unexpected situations and ensure they do not compromise the results. Respect the subjects’ free will.\nA flowchart can be useful to demonstrate the administrative flow, detailing participant movement, data entry, etc.\n\n\n\nAdministration Flow"
  },
  {
    "objectID": "chapters/documentation.html#accounting",
    "href": "chapters/documentation.html#accounting",
    "title": "27  Document",
    "section": "27.11 Accounting",
    "text": "27.11 Accounting\nResearch involves meticulous bookkeeping akin to running a startup. Document all transaction receipts and be diligent with currency conversion. Store these documents in a Google Sheet provided by your organization, scan/take pictures, and upload them to the cloud with names corresponding to the table entry."
  },
  {
    "objectID": "chapters/documentation.html#data-protection-privacy-and-quality-control",
    "href": "chapters/documentation.html#data-protection-privacy-and-quality-control",
    "title": "27  Document",
    "section": "27.12 Data Protection, Privacy, and Quality Control",
    "text": "27.12 Data Protection, Privacy, and Quality Control\nIn the digital era, data protection and privacy are paramount. Discuss plans to protect collected data, especially if it includes personal information about the subjects. This could include measures such as anonymizing data, storing it securely, and complying with relevant data protection laws.\nWhile consistency and standardization are important, it’s essential to elaborate on strategies for ensuring data quality. These could include training data collectors, conducting regular checks or audits, and addressing any errors or inconsistencies that arise."
  },
  {
    "objectID": "chapters/documentation.html#data-analysis-and-improvement",
    "href": "chapters/documentation.html#data-analysis-and-improvement",
    "title": "27  Document",
    "section": "27.13 Data Analysis and Improvement",
    "text": "27.13 Data Analysis and Improvement\nPlanning how the data will be analyzed can influence how you collect and store it, and it’s useful for anyone using your documentation to understand or replicate your experiment. Consider including a section on how you plan to use the results of your experiment to inform future research. This could involve reviewing and reflecting on the experiment, gathering feedback from participants or data collectors, and making improvements for the future."
  },
  {
    "objectID": "chapters/documentation.html#communication",
    "href": "chapters/documentation.html#communication",
    "title": "27  Document",
    "section": "27.14 Communication",
    "text": "27.14 Communication\nEffective communication is integral to the success of any experiment. Detailed documents should be created providing clear instructions on the use of each component of your experiment’s system.\nThese documents should include:\n\nProcedure Manuals: Outlining step-by-step processes for sampling, administration, data collection, and handling in clear, concise language.\nTraining Materials: Practical demonstrations of procedures to help your field agents familiarize with the processes before starting data collection.\nReference Guides: Quick-reference materials summarizing key points from the procedure manuals and training materials.\nData Entry Guidelines: Providing clear instructions on how to enter data into your electronic system.\nData Protection Policies: Explaining how personal data will be protected.\nQuality Control Procedures: Outlining strategies for ensuring the quality of the data collected.\nFeedback Mechanisms: Explaining how feedback will be collected and used to improve future research.\n\nBy ensuring these documents are readily accessible to your field agents, your experiment can run smoothly, generating high-quality, reproducible data.\nA well-documented experiment is the key to successful research. By adhering to these guidelines, you can ensure your research is reproducible, traceable, and of high quality."
  },
  {
    "objectID": "chapters/visualization_theory.html#perceptual-processing",
    "href": "chapters/visualization_theory.html#perceptual-processing",
    "title": "\n28  Data Visualization Fundamentals\n",
    "section": "\n28.1 Perceptual Processing",
    "text": "28.1 Perceptual Processing\nTo answer that question, we’ll start with how humans perceive the world, or more accurately, how our brains process visual information.\nWhy do we care about perceptual processing?\n\nIt helps explain why some techniques work well and others do not.\nIt informs us about the limits people have in terms of visual perception.\nIt can support design decisions.\n\nVisual perception occurs in two phases (similar to Daniel Kahneman’s systems 1 and 2):\n\n\nEarly, parallel detection of color, texture, shape, and spatial attributes:\n\nInvolves the detection of orientation, color, texture, movement, etc.\nEngages arrays of neurons working in parallel.\nOccurs “automatically” and rapidly.\nInformation is briefly held in iconic storage.\nFollows a bottom-up, data-driven model of processing.\nOften referred to as “pre-attentive” processing because it occurs without the direction of our conscious mind.\n\n\n\nSerial processing of object identification (using memory) and spatial layout and action:\n\nInvolves sequential processing.\nSplits into subsystems for object recognition and interaction with the environment.\nEvidence supports the independence of systems for symbolic object manipulation and for locomotion & action.\nThe first subsystem interfaces with the verbal linguistic portion of the brain, while the second interfaces with motor systems that control muscle movements.\nInvolves slow serial processing.\nEngages working and long-term memory.\nFollows a top-down processing model.\n\n\n\n\n28.1.1 Pre-attentive Processing\nOur main focus for now is system 1, as it is the system that “sees” and directs our attention. By catering to it, we can make our visuals more intuitive.\nHow does the human visual system analyze images? - Some things seem to be processed pre-attentively, without the need for focused attention. - This process generally takes less than 200-250 milliseconds (for reference, eye movements take about 200 milliseconds). - It seems to be handled in parallel by the low-level vision system.\nLet’s consider an example with a number wall:\n\n\n1\n2\n7\n9\n8\n3\n\n\n5\n6\n4\n1\n7\n9\n\n\n6\n7\n4\n2\n3\n6\n\n\n2\n7\n5\n7\n9\n0\n\n\n1\n3\n8\n5\n8\n3\n\n\n2\n0\n0\n3\n7\n4\n\n\nNow, with highlights, finding the number 2 becomes much faster, doesn’t it?\n\n\n1\n2\n7\n9\n8\n3\n\n\n5\n6\n4\n1\n7\n9\n\n\n6\n7\n4\n2\n3\n6\n\n\n2\n7\n5\n7\n9\n0\n\n\n1\n3\n8\n5\n8\n3\n\n\n2\n0\n0\n3\n7\n4\n\n\nWhat kinds of tasks can the pre-attentive system perform? - Target detection: Users rapidly and accurately detect the presence or absence of a “target” element with a unique visual feature within a field of distractor elements. - Boundary detection: Users rapidly and accurately detect a texture boundary between two groups of elements, where all the elements in each group have a common visual property. - Region tracking: Users track one or more elements with a unique visual feature as they move in time and space. - Counting and estimation: Users count or estimate the number of elements with a unique visual feature.\nExample: Can you rapidly detect the presence of a red circle?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-attentive processing allows our visual system to rapidly and accurately detect limited visual properties before we are consciously aware of them:\n\nWe can easily identify the presence or absence of a target within a visual field.\nWe can effortlessly detect a texture boundary between two groups of elements.\nWe can smoothly track an element with a unique visual feature as it moves through space and time.\n\nHere are some types of preattentive processing we can perform:\n\n\n\n\n\nLine (blob) orientation\n\n\n\n\n\nLength, width\n\n\n\n\n\nClosure\n\n\n\n\n\nSize\n\n\n\n\n\n\n\nCurvature\n\n\n\n\n\nDensity, contrast\n\n\n\n\n\nNumber, estimation\n\n\n\n\n\nColor (hue)\n\n\n\n\n\n\n\nIntensity, luminosity, binocular lustre\n\n\n\n\n\nIntersection\n\n\n\n\n\nTerminators\n\n\n\n\n\n2D depth cues\n\n\n\n\n\n\n\nFlicker\n\n\n\n\n\nDirection of motion\n\n\n\n\n\nVelocity of motion\n\n\n\n\n\nLighting direction\n\n\n\n\nWe struggle to have multiple categories \n\n28.1.1.1 Feature Hierarchy\nMultiple features such as color and shape can represent multiple types of data in a single image. But, it’s important to make sure that these visual features don’t mix up and hide the data we want to show. Think of it like trying to find a red apple in a bowl full of green apples – it’s easy because the color stands out.\nSometimes, our eyes like one visual feature more than another. For example, when we’re looking at shapes, colors can be distracting and make it harder to see the shape patterns. But, if the colors are all the same, the shapes stand out clearly. We struggle to perceive more then two categories. Try to find the groups of points below, which groups do you spot first?\n\n\n\n\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n(d)\n\n\n\nFigure 28.1: Examples for Feature Hierarchy\n\n\nCheck out more and play the game here: https://www.csc2.ncsu.edu/faculty/healey/PP/#jscript_search\nSo, when we’re deciding how to visually represent our data, we should pick the features that make the most important information stand out. This way, we avoid hiding the data we want to show.\nCommon ways to visually encode numbers in order from most easily perceived to least:\n\nposition along a common scale, axis, and baseline\nposition along non-aligned axes\nLength, direction, angles of relative lines/ slope\nArea\nVolume, curvature, arcs / angles within a shape\nColor or shading\n\n28.1.1.2 Integral vs Separable\nCan you develp a set of unique symbols that can be placed on a display and be rapidly recieved and differentiated? If we talked privously about picking a number off a visual depiction and match the intended encoded number, here we talk whether they can be rapidly percieved and differentiated from each other.\nSuppose that we use two different visual properties to encode two different variables in a discrete dataset (one is easy, three is hard): - color, size, share, lightness\nWill the two different properies interact so that they are more/less difficult to untangle? - Integral - two properies are viewed holistically - Separable - judge each dimension independently\n\n\nWare (2021)\n\n\n\n28.1.1.3 Nonlinear Perception\nPerception is not uniformly linear. There are some things we perceive accurately, such as length, while there are others that we tend to underestimate, such as the true difference between two values due to our ability to sense ratios.\nFor example, you are pretty good at estimating lengths and temperatures after a little practice. However, in some domains, we tend to underestimate differences and miss the ratios. Our perception adjusts to the strength of the signal; for instance, our eyes adjust to bright daylight and to darkness in a room.\nVisualization is about turning numbers into pictures. However, the goal is for the user to be able to translate these pictures back into numbers accurately.\nOne tricky task in visualization is translating numbers into areas. This is not only difficult to decipher but also to encode. What are we comparing when we look at areas—radius, area, or sensation?\nLet’s consider an example. If you have three red circles and one green circle, which red circle represents a number that is twice as big as the green one?\n\n\n\n\n\nFunny enough, all three are correct! The second circle has twice the area of the original, the third circle appears to have twice the area according to Stevens’ Law (which we will discuss shortly), and the fourth circle has twice the radius. Yes, it is indeed confusing! The way we perceive proportional differences in sensation is not a one-to-one relationship with the measurement.\nOur goal in data visualization is to transform visuals into numbers in a way that makes it easy for the reader to understand.\nNow, let’s talk about Stevens’ power law. Stevens was interested in this exact question and formulated Stevens’ Law in 1960.\n\\[\ns(x) = ax^{b}\n\\tag{28.1}\\]\n\\(s\\) is sensation \\(x\\) is intensity of the attribute \\(a\\) is a multiplicative constant \\(b\\) is the power\n\\(b &gt; 1\\): overestimate;\n\\(b &lt; 1\\): underestimate\n\n\n\n\n\nExperimental results for (b), the exponent in Stevens’ Law, range from 0.9 to 1.1 for lengths, 0.6 to 0.9 for area, and 0.5 to 0.8 for volume. As a rule of thumb, (b ).\nSo, how would we apply this apparent scaling in practice? Let’s consider an example where we want to draw circles of different areas. Imagine the largest circle has an area twelve times bigger than the smallest one. To counteract our tendency to underestimate, we could increase the area by approximately (). However, it’s important to consider the context and whether these adjustments will truly benefit your visualization. Nonetheless, if you were to make these adjustments, this is how you would do it.\n\n\n\n\n\nTurning color into numbers is complicated, as it is affected by a myriad of factors, from lighting to individual perception. For example, consider the chess pieces in the image below. Do they appear to be the same color?\n\n\nby Barton L. Anderson and Jonathan Winawer\n\nDespite appearances, they are actually the exact same color!\nPurposes of Using Color:\n\nCall attention to specific data points\nEnhance appeal and memorability\nRepresent discrete categories\n\nWhen using color:\n\nOpt for pastel shades.\nAvoid high saturation.\nBe mindful of spectral colors as they can cause afterimages.\nUtilize color for grouping and searching.\n\n28.1.1.4 Gestalt Principles\nGestalt Principles explain how the human brain perceives visual patterns from grouped elements. These principles encompass concepts like proximity, similarity, continuity, closure, connection, and enclosure.\n\n\n\n\n\n\n\n\nProximity: When objects are close together, we often perceive them as a group\n\n\n\n\nSimilarity: When objects share similar attributes (color, shape, etc.), we often perceive them as a group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnclosure: When objects are surrounded by a boundary, we often perceive them as a group\n\n\n\n\nClosure: Sometimes partially open structures can still be perceived as a grouping metaphor (e.g., “[…]”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnectivity: When you draw curves or lines through data elements, this is often perceived as creating a connection between them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28.1.2 Visual Encoding\nAfter learning about the basics of how our eyes and brain quickly process visuals, it’s time to delve into visual encoding, which is about turning data into visuals that are easy to understand.\nWe need to tackle when visually encoding information we need to tackle the following taks:\n\nTurning numeric data into visuals\nTurning categorical data into visuals\nShowing the differences between pieces of information\nShowing how data or information relates to some context\n\nHumans have different types of memory like long-term, working, verbal, and visual memory, each stored in various parts of the brain. Our working memory, which temporarily holds information, can only keep around three chunks of information at a time. Visualizations can help group or “chunk” information together, making it easier for us to process and remember.\nIt’s essential to keep related information close together in a visualization to avoid fragmentation, which is when we separate things that should be remembered together. By doing this, we help people remember and understand the information better. We can highlight or annotate important points to draw attention to them.\nGood design in visualizations helps people quickly understand what they’re looking at. It’s not about just putting numbers into shapes, but making those shapes tell a story. A well-designed visualization will help people easily scan through the information and also delve deeper if they want to.\n\nThe goal is to make it easy for the reader to decode the visual information without making errors.\n\nDifferent visual attributes like position, length, angle, or color help represent data. Some attributes, like position and length, are better for showing precise data, while others like color or size are less precise. It’s crucial to match the right attribute with the type of data we’re showing.\nUsing familiar chart types, intuitive colors, and shapes help make the visualization easy to understand. Avoid making people remember too many new symbols or having large legends, as it can be overwhelming.\nLastly, knowing who will be looking at the visualization will inform your decisions resulting in visuals that are easy to understand, remember, and interpret."
  },
  {
    "objectID": "chapters/visualization_theory.html#evaluating-your-graphs",
    "href": "chapters/visualization_theory.html#evaluating-your-graphs",
    "title": "\n28  Data Visualization Fundamentals\n",
    "section": "\n28.2 Evaluating your Graphs",
    "text": "28.2 Evaluating your Graphs\nHow do we evaluate our graphics to enlighten and engage our audience, rather than deceive them? Several practical frameworks have been proposed for this purpose.\n\n28.2.1 Data Ink Ratio (Tufte 2001):\n\nTufte, Edward R. 2001. The Visual Display of Quantitative Information. 2nd ed. Cheshire, Conn: Graphics Press.\nOne of the popular ideas in data visualization is the Data-ink ratio, introduced by Edward Tufte. This idea is all about keeping things simple and getting rid of any extras that don’t help convey the main message. As Tufte suggests, it’s good to “erase non-data-ink, within reason” and “erase redundant data-ink, within reason.” It might be tempting to remove too much, but it’s better to take it slow. Trust your gut feeling on whether the chart still makes sense. The suggestions we’ll discuss next are based on having clean and clear graphics.\n\n\n28.2.2 Levers of Chart-Making (Ware 2021):\n\nWare, Colin. 2021. Information Visualization: Perception for Design. Fourth edition. Cambridge, MA: Morgan Kaufmann, Inc.\n\nSpeed to primary insight: How fast the audience can extract insight from a graph.\nGranularity: The level of detail in the data shown in a chart.\nExplore or explain: Whether the visualization allows users to explore the data themselves or is accompanied by an explanation.\nDry or emotional: The seriousness or informality of the data presentation. We can make presentation more emotional to attract less data savvy audience.\nAmbiguity vs. accuracy: The balance between clear accuracy and intended ambiguity in the chart.\n\n28.2.3 Cognitive Load (Sibinga and Waldron 2021):\n\nSibinga, Eva, and Erin Waldron. 2021. “Cognitive Load as a Guide: 12 Spectrums to Improve Your Data Visualizations | Nightingale.” https://nightingaledvs.com/cognitive-load-as-a-guide-12-spectrums-to-improve-your-data-visualizations/.\nThis framework is divided into three categories:\n\nIntrinsic load: Concerned with the complexity of the data itself.\n\n\nMeasurement: The type of data (quantitative vs. qualitative).\nKnowability: The certainty of the data (certain vs. uncertain).\nSpecificity: The clarity of data categories (precise vs. ambiguous).\nRelatability: How relatable the data is to everyday life (concrete vs. abstract).\n\n\nGermane load: Concerned with the audience’s readiness to process the information.\n\n\nConnection: How the audience first encounters the visualization (intentional vs. coincidental).\nPace: The time the audience has to view the visualization (slow vs. fast).\nKnowledge: The audience’s familiarity with the subject (expert vs. novice).\nConfidence: The audience’s familiarity with the data reporting format (confident vs. anxious).\n\n\nExtraneous load: Concerned with how new information is presented.\n\n\nChart type: The commonality of the chart type (common vs. rare).\nInterpretation: The precision of the chart’s values (accurate vs. approximate).\nComposition: The density of information on the page (concise vs. detailed).\nDelivery: Whether the data report is self-explanatory or requires exploration (explanatory vs. exploratory).\n\nNo framework is likely to replace the others; instead, they complement each other to cover the vast territory of the data visualization domain. Data-ink ratio principles remain a good starting point for most business contexts, but considering emerging frameworks can make the practice more nuanced for tackling different needs, messages, and audiences. The final determinant of how to incorporate the three frameworks will depend on the context of the visualization, with a clear understanding of the audience, the message, and the medium being key.\n\nFinally, the most tried and true method of testing graphics is asking others to have a look at it!"
  },
  {
    "objectID": "chapters/data_viz.html#types-of-data-visualisation",
    "href": "chapters/data_viz.html#types-of-data-visualisation",
    "title": "29  Data Visualization",
    "section": "29.1 Types of Data Visualisation",
    "text": "29.1 Types of Data Visualisation\nData visualization is an essential tool for understanding and communicating complex information. There are two main types of visualization:\n\n29.1.1 Exploratory\nIt is common to look at summary statistics such as mean and standard deviation. But these numbers obscure the datapoints hiding the form of our datasets. Matejka and Fitzmaurice generated datasets with Identical Statistics that look distinctly different. You can access all 12 patterns with datasauRus package. It is important to see the structure to move your analyses forward.\n\n\n\n\n\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\n\n54.26\n47.83\n16.77\n26.94\n-0.06\n\n\n\n\n\n\n\n\n\nShow the code\ndatasauRus::datasaurus_dozen %&gt;%\n  filter(dataset %in% c(\"away\", \"dino\", \"star\")) %&gt;%\n  mutate(dataset = str_to_upper(dataset)) %&gt;%\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_void(base_size = 18) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_text(face = \"bold\")\n  ) +\n  facet_wrap(~dataset, ncol = 3) +\n  coord_fixed(ratio = 0.8)\n\n\n\n\n\n\n\n29.1.2 Explanatory\nSo, you got your results together and now you need to not only present them, but also convince non-techical audience. They don’t care whether your model user cross-validation or how you optimized your gradient boosted forest, all they want is a convincing simple message. That is why you won’t see fency overloaded graphs in forward facing presentation it all about the message. Look at the graph Apple used to show their M1 MacBooks are better. \nR offers a variety of packages for creating visually appealing and informative plots. One of the most popular and versatile packages for data visualization in R is ggplot2. We will explore the basics of using ggplot2 to create different types of plots and customize them to suit your needs. We can load it separately libary(ggplot2) or with libary(tidyverse)."
  },
  {
    "objectID": "chapters/data_viz.html#grammar-of-graphics",
    "href": "chapters/data_viz.html#grammar-of-graphics",
    "title": "29  Data Visualization",
    "section": "29.2 Grammar of Graphics",
    "text": "29.2 Grammar of Graphics\nThe Grammar of Graphics is a concept in data visualization that was developed by Leland Wilkinson in his book “The Grammar of Graphics” (The Grammar of Graphics 2005) in 1999. The Grammar of Graphics is essentially a system of rules that describes how to represent data visually using a set of graphical elements and mappings between data variables and visual properties.\n\nThe Grammar of Graphics. 2005. Statistics and Computing. New York: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n“Excel Enjoyers” are familiar with the Excel plotting workflow: you select a plot you want and it just produces one for you.\n\n\n\n“Excel GUI”\n\n\nUnder this framework scatter plot and bar plots appear completely different:\n\n\nShow the code\npoint_plot &lt;- data_raven %&gt;%\n  count(pr_correct, name = \"count\") %&gt;%\n  ggplot(aes(x = pr_correct, y = count)) +\n  geom_point(size = 3) +\n  theme_minimal()\ncol_plot &lt;- data_raven %&gt;%\n  count(pr_correct, name = \"count\") %&gt;%\n  ggplot(aes(x = pr_correct, y = count)) +\n  geom_col() +\n  theme_minimal()\npoint_plot + col_plot\n\n\n\n\n\nHowever, under the Grammar of Graphics we see how similar these graphics are! They are exactly the same in terms everything, but geometries! The first one use “points” while the second uses “columns” to display the data.\nThe Grammar of Graphics provides a framework for creating complex visualizations by breaking down the visualization process into a set of components.\n\n\n\n“Grammar of Graphics Visual, from QCBS R Workshop 3”\n\n\n\nData: The information that is being visualized. To explore how grammar of graphics works in ggplot2 we will use iris dataset, which is a built-in dataset of measurements of different parts of iris flowers.\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe Data layer of the graph is just a blank canvas. Becase we have not specified any graphing elements yet.\n\ndata_layer &lt;- ggplot(data = iris)\ndata_layer\n\n\n\n\n\nAesthetics: The visual properties used to represent the data, such as x, y, color or size. Once we add aethetics we see out plotting area being set up and if we check mapping we see that Sepal.Length was assigned to x and Sepal.Width was assigned to y.\n\n\naes_layer &lt;- ggplot(\n  data = iris,\n  aes(x = Sepal.Length, y = Sepal.Width)\n)\naes_layer\n\n\n\naes_layer$mapping\n\nAesthetic mapping: \n* `x` -&gt; `Sepal.Length`\n* `y` -&gt; `Sepal.Width`\n\n\n\nGeometries: The visual elements used to represent the data, such as points or bars. Once we add geometry we start seeing our data!\n\n\ngeometry_layer &lt;- aes_layer + geom_point()\ngeometry_layer\n\n\n\n\n\nScales: The mapping between the data and the aesthetics, such as how numeric values are mapped to positions on a graph. There are different scales for color, fill, size, log(x), etc. Here we added scale color. Checking the mapping we see Species is mapped to colour.\n\n\nscales_layer &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  # We case scale function to edit the scale for example we can set our own color manually\n  scale_color_manual(values = c(\"red\", \"orange\", \"pink\"))\nscales_layer\n\n\n\nscales_layer$mapping\n\nAesthetic mapping: \n* `x`      -&gt; `Sepal.Length`\n* `y`      -&gt; `Sepal.Width`\n* `colour` -&gt; `Species`\n\n\n\nStatistics: Mathematical transformations applied to the data before visualization, such as summary statistics or new variables. Histogram for example splits data into bins and counts observations.\n\n\nstat_layer &lt;- ggplot(data = iris, aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20, color = \"white\")\nstat_layer\n\n\n\n\n\nFacets: Ways of dividing the data into subgroups and creating separate visualizations for each subgroup.\n\n\nfacets_layer &lt;- geometry_layer + facet_wrap(vars(Species), ncol = 3)\nfacets_layer\n\n\n\n\n\nTheme: Adding Polishing touches to your visual and making it look exactly the way you want.\n\n\ntheme_layer &lt;- facets_layer + theme_minimal(base_size = 18) +\n  geom_point(size = 2, color = \"#ffb86c\") +\n  theme(\n    plot.background = element_rect(fill = \"#282a36\", color = \"#44475A\"),\n    axis.text = element_text(color = \"#f8f8f2\"),\n    axis.title = element_text(color = \"#f8f8f2\"),\n    strip.text = element_text(color = \"#f8f8f2\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(colour = \"#44475a\")\n  ) +\n  labs(x = \"Sepal Length\", y = \"Sepal Width\")\ntheme_layer\n\n\n\n\n\nApply the theme_minimal function to start with a blank slate but with minimal default aesthetics. The base_size argument specifies the base font size.\nAdd points to the plot with the geom_point function. The color and size of the points are customized.\nSet the background color of the plot with element_rect inside the theme function.\nCustomize the color of the axis text with element_text.\nCustomize the color of the axis title with element_text.\nCustomize the color of the facet label text with element_text.\nRemove minor grid lines with element_blank.\nCustomize the color of major grid lines with element_line.\nSet the labels for the x and y axes with labs."
  },
  {
    "objectID": "chapters/data_viz.html#ggplot",
    "href": "chapters/data_viz.html#ggplot",
    "title": "29  Data Visualization",
    "section": "29.3 ggplot()",
    "text": "29.3 ggplot()\nIf you used R before then you are familiar with the default graphing function plot,hist, etc. ggplot2 has it own version of quickly making a graph qplot(). To learn about qplot() check out this vignette.\n\ndata_raven %&gt;%\n  pull(pr_correct) %&gt;%\n  hist()\n\n\n\ndata_raven %&gt;%\n  qplot(pr_correct,\n    data = .,\n    geom = \"histogram\",\n    bins = length(unique(data_raven$pr_correct))\n  )\n\n\n\n\nThe ggplot() function sets up the basic structure of a plot, and additional layers, such as points, lines, and facets, can be added using + operator (like %&gt;%, but for +). This makes it easy to understand, modify the code, and build complex plots by adding layers. This allows for easy creation of plots that reveal patterns in the data. In contrast, the basic R plotting functions and qplot() have a simpler and less expressive syntax, making it harder to create complex and multi-layered plots. Mastering ggplot() is well worth your time and effort as it will teach you how to think about graphs and what goes into building them. For example, let’s improve the histogram from earlier!\n\ndata_raven %&gt;%\n  count(pr_correct) %&gt;% # I prefer calculating statistics myself\n  ggplot(aes(x = as.factor(pr_correct), y = n)) + # We use aes to set x and y\n  geom_col(fill = \"steelblue\") +\n  theme_minimal(base_size = 15) +\n  theme(\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(linewidth = 0.5, linetype = 2, color = \"grey\")\n  ) +\n  labs(\n    x = \"Number of Correct Answers\",\n    y = \"Subject Count\",\n    title = \"Distribution of Correct Answers in Piece-rate Game\"\n  )\n\n\n\n\nAh much better! We added labels, removed unnecessary grid lines, and added some color. If you want to learn more about ggplot check out ggplot2: Elegant Graphics for Data Analysis (Hadley 2016) and the cheatsheet.\n\nHadley, Wickham. 2016. Ggplot2. New York, NY: Springer Science+Business Media, LLC.\nWe can use an amazing package esquisse to build our plots with drag-and-drop!\n\n# install.packages('esquisse')\nlibrary(esquisse)\n\nYou can access esquisse by going to “Addins” in the top panel or with esquisser(your_data). Now go learn more about this package here.\nAnd then we can style with them with a GUI from ggThemeAssist!\n\n# install.packages(\"ggThemeAssist\")\nlibrary(ggThemeAssist)\n\nYou can access ggThemeAssist by selecting the code for your plot and going to “Addins”. You can see more about the package here.\nI see. Let’s adjust the annotations accordingly."
  },
  {
    "objectID": "chapters/data_viz.html#interactive-plots",
    "href": "chapters/data_viz.html#interactive-plots",
    "title": "29  Data Visualization",
    "section": "29.4 Interactive Plots",
    "text": "29.4 Interactive Plots\nThe ggplotly function in Plotly offers an easy way to convert ggplot2 figures into interactive visuals. This feature is particularly handy when you’re dealing with a large number of data points that would be difficult to distinguish in a static plot.\n\n1library(plotly)\nlibrary(ggplot2)\n\np &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 1, alpha = 1) +\n2  theme_minimal()\n\n3pp &lt;- ggplotly(p)\n\n4pp\n\n\n1\n\nLoad the necessary libraries.\n\n2\n\nCreate a scatter plot using ggplot2. The plot is of Sepal Length against Sepal Width, with data points colored by Species and sized by Petal Length.\n\n3\n\nConvert the ggplot2 plot to a plotly plot using the ggplotly function.\n\n4\n\nDisplay the interactive plot.\n\n\n\n\n\n\n\n\n\n29.4.1 Building Plots with Plotly\nggplotly lets you convert ggplot2 figures into interactive plots, but Plotly also lets you create these visuals from scratch. This method can be more flexible in some cases and allows you to access additional features that may not be available when converting from ggplot2.\n\n1library(plotly)\n\n2p &lt;- plot_ly(\n  data = iris,\n  x = ~Sepal.Length,\n  y = ~Sepal.Width,\n  color = ~Species,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(size = 4)\n)\n\n3p &lt;- layout(\n  p,\n  title = \"Sepal Measurements (with Petal Length as size)\",\n  xaxis = list(title = \"Sepal Length\"),\n  yaxis = list(title = \"Sepal Width\"),\n  hovermode = \"closest\"\n)\n\n4p\n\n\n1\n\nLoad the plotly library.\n\n2\n\nCreate a scatter plot using plot_ly, where Sepal Length is plotted against Sepal Width, and color and size of points are determined by the Species and Petal Length respectively.\n\n3\n\nCustomize the plot layout, including the title and axis labels, and setting the hover mode to display information about the nearest point.\n\n4\n\nDisplay the plot.\n\n\n\n\n\n\n\n\n\n\n29.4.2 ECharts4r\nECharts4r is an R wrapper for the ECharts JavaScript library, which is used for interactive data visualization. ECharts provides a rich set of chart types, including bar, line, scatter, pie, radar, and more.\n\n1library(echarts4r)\n\ne_chart &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n2  e_charts(Sepal.Length) %&gt;%\n3  e_scatter(Sepal.Width, symbol_size = 7) %&gt;%\n4  e_y_axis(min = 1.5, max = 5) %&gt;%\n5  e_x_axis(min = 4, max = 8) %&gt;%\n  e_axis_labels(\n    x = \"Sepal Width\",\n    y = \"Sepal Length\"\n6  ) %&gt;%\n7  e_tooltip(\n    trigger = \"item\", \n    formatter = htmlwidgets::JS(\"\n      function(params){\n        return('Sepal Length: ' + params.value[0] + '&lt;br /&gt;Sepal Width: ' + params.value[1])\n      } \n    \") \n  ) \n\ne_chart\n\n\n1\n\nLoad the echarts4r library.\n\n2\n\nGroup the data by the Species variable.\n\n3\n\nInitialize an Echarts plot with Sepal.Length as the x-axis.\n\n4\n\nAdd a scatter plot to the existing Echarts plot with Sepal.Width as the y-axis.\n\n5\n\nSet the range for the y-axis (Sepal.Length) from 1.5 to 5.\n\n6\n\nSet the range for the x-axis (Sepal.Width) from 4 to 8.\n\n7\n\nLabel the x-axis as “Sepal Width” and the y-axis as “Sepal Length”.\n\n\n\n\n\n\n\n\nECharts is particularly powerful for creating complex, multi-series, interactive charts and supports a broad range of customization options."
  },
  {
    "objectID": "chapters/data_viz.html#tips",
    "href": "chapters/data_viz.html#tips",
    "title": "29  Data Visualization",
    "section": "29.5 Tips",
    "text": "29.5 Tips\n\n29.5.1 group\nUsually ggplot groups your data by one the aesthetics you provided such as color and fill; however, sometimes it fails to do so. When that happens it is worth specifying group argument on your own.\nNotice how labels for years 2020, 2021, 2022 are all over the place.\n\n\nShow the code\nspending_plot_data %&gt;%\n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) +\n  geom_col(position = \"fill\", show.legend = T) +\n  scale_fill_manual(\n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nIf we specify group aesthetic everything goes back to its place!\n\n\nShow the code\nspending_plot_data %&gt;%\n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) +\n  geom_col(position = \"fill\", show.legend = T) +\n  scale_fill_manual(\n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(aes(group = agency), size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nThis error is very common in line charts too.\n\n\nShow the code\ngroup_line_data &lt;- tibble(\n  measure = c(rep(\"hot\", 5), rep(\"cool\", 5)),\n  date = rep(seq(2000, 2004, by = 1), 2),\n  value = c(89, 111, 100, 130, 159, 24, 37, 88, 69, 105)\n)\n\n\n\n\nShow the code\ngroup_line_data %&gt;% ggplot(aes(x = date, y = value)) +\n  geom_line() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  theme_minimal()\n\n\n\n\n\nShow the code\ngroup_line_data %&gt;% ggplot(aes(x = date, y = value, group = measure)) +\n  geom_line() +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  theme_minimal()"
  },
  {
    "objectID": "chapters/color.html#color-and-categorical-data",
    "href": "chapters/color.html#color-and-categorical-data",
    "title": "\n30  Color Data\n",
    "section": "\n30.1 Color and Categorical Data",
    "text": "30.1 Color and Categorical Data\n\n30.1.1 Emphasizing Significant Details\n\nCodegapminder1997eu &lt;- gapminder %&gt;% filter((year == 1997) & (continent == \"Europe\"))\ncontinents &lt;- gapminder %&gt;%\n  select(-country) %&gt;%\n  mutate(gdp = gdpPercap * pop) %&gt;%\n  group_by(year, continent) %&gt;%\n  summarize(total_pop_mil = sum(pop) / 10^6, total_gdp = sum(gdp), total_gdppc = total_gdp / (total_pop_mil * 10^6)) %&gt;%\n  ungroup()\nmalay_miracle &lt;- gapminder %&gt;% filter(country %in% c(\"Malaysia\", \"Vietnam\", \"Indonesia\", \"Thailand\"))\nasian_tigers &lt;- gapminder %&gt;%\n  filter(country %in% c(\"Hong Kong, China\", \"Taiwan\", \"Singapore\", \"Korea, Rep.\")) %&gt;%\n  mutate(country = recode(country, \"Hong Kong, China\" = \"Hong Kong\", \"Korea, Rep.\" = \"South Korea\"))\n\n\nColor is employed to underscore specific data and provide context. Consider the two graphs below, both of which compare the GDP of European countries in 1997. The first graph assigns each country a distinct color, resulting in a visual cacophony akin to an “explosion at a candy factory”. The second graph improves upon this by minimizing distractions. It highlights “Greece” and greys out the remaining countries. The use of “red” for Greece serves as a signal, hinting that its economic performance may be subpar.\n\nCodep1 &lt;- gapminder1997eu %&gt;%\n  mutate(is.Greece = country == \"Greece\") %&gt;%\n  ggplot(aes(y = fct_reorder(country, gdpPercap), x = gdpPercap)) +\n  geom_segment(aes(yend = country, xend = 0, color = country), size = 1, show.legend = FALSE) +\n  geom_point(aes(color = country), show.legend = FALSE, size = 3) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  # ggplot2::scale_color_manual(values = c(\"#7286D3\",\"#D37286\")) +\n  labs(x = \"GDP per Capita\", y = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\")\n\np2 &lt;- gapminder1997eu %&gt;%\n  mutate(is.Greece = country == \"Greece\") %&gt;%\n  ggplot(aes(y = fct_reorder(country, gdpPercap), x = gdpPercap)) +\n  geom_segment(aes(yend = country, xend = 0, color = is.Greece), size = 1, show.legend = FALSE) +\n  geom_point(aes(color = is.Greece), show.legend = FALSE, size = 3) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  ggplot2::scale_color_manual(values = c(\"grey\", \"#D37286\")) +\n  labs(x = \"GDP per Capita\", y = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\")\n\np1 + p2 + plot_annotation(title = \"Countries in Europe by GDP per Capita\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.2 Comparing Two Things\n\n30.1.2.1 Complementary Harmony with a Positive/Negative Connotation\nComplementary Harmony involves the use of colors directly opposite each other on the color wheel, creating a stark contrast. This method effectively conveys a positive/negative connotation, ideal for emphasizing differences. While colors located near each other on the wheel can also complement each other, those placed in opposition offer the most substantial reinforcement for a key color. The illustration below compares the population growth of Asia and Europe. Here, the use of bright purple underscores the remarkable population surge in Asia, while the green tone underlines the comparatively slower growth in Europe.\n\nCodep3 &lt;- continents %&gt;%\n  filter(continent %in% c(\"Asia\", \"Europe\")) %&gt;%\n  ggplot(aes(x = year, y = total_pop_mil, color = continent)) +\n  geom_line(linewidth = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#70DA74\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n\n\ncomplementary + p3 + plot_annotation(title = \"Complementary Harmony with a Positive/Negative Connotation\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.2.2 Near Complementary Harmony for Highlighting Two Series Where One Is the Primary Focus\nNear Complementary Harmony is a color scheme that achieves substantial contrast without resorting to using colors diametrically opposite on the color wheel. Instead, it involves choosing a color located 33% around the wheel from the principal color, rather than a full 50% away. This method is effective when highlighting two series, one of which is the primary focus. It’s preferable to use warm colors for the key series and cool colors for the complementary ones. If required, the intensity of the complementary colors can be subdued by reducing their saturation or modifying their lightness, thereby lowering the contrast with the background. The example below underscores the significance of Asia’s population growth, while Europe is neutrally depicted as a comparative reference, not a region with slow growth.\n\nCodep4 &lt;- continents %&gt;%\n  filter(continent %in% c(\"Asia\", \"Europe\")) %&gt;%\n  ggplot(aes(x = year, y = total_pop_mil, color = continent)) +\n  geom_line(size = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#70D6DA\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n\ntriadic + p4 + plot_annotation(title = \"Near Complementary Harmony for Highlighting \\nTwo Series Where One Is the Primary Focus\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.3 Color Palettes for Comparing Three Things\n\n30.1.3.1 Analogous/Triadic Harmony for Highlighting Three Series\nAnalogous Complementary is a color scheme that employs four colors. It combines the key color and its complementary color with two colors adjacent to the complementary color on the color wheel. This scheme retains analogous harmony while forming a quartet of colors, suitable for highlighting one main data series and its three sub-components. The similarities among the three complementary colors render the key color more prominent. The example below depicts the “Malaysian Economic Miracle” by contrasting Malaysia’s economic growth with that of its three neighboring countries.\n\nCodep5 &lt;- continents %&gt;%\n  filter(continent %in% c(\"Asia\", \"Europe\", \"Americas\")) %&gt;%\n  ggplot(aes(x = year, y = total_pop_mil, color = continent)) +\n  geom_line(size = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#DA70A1\", \"#A970DA\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"Population in Millions (log10)\")\n\nanalogous + p5 + plot_annotation(title = \"Analogous/Triadic Harmony for Highlighting Three Series\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.3.2 Highlighting One Series Against Two Related Series\nThe Near Complementary Harmony color scheme is adept at highlighting one series against others. The following chart clearly emphasizes Asia’s GDP, represented by a vibrant purple. Conversely, Europe and the Americas, depicted in harmonizing greens, play a more subsidiary role in this narrative.\n\nCodep6 &lt;- continents %&gt;%\n  filter(continent %in% c(\"Asia\", \"Europe\", \"Americas\")) %&gt;%\n  ggplot(aes(x = year, y = total_gdppc, color = continent)) +\n  geom_line(size = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#A1DA70\", \"#DA70D6\", \"#70DAA9\")) +\n  geom_dl(aes(label = continent), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\ncomplementary_3 + p6 + plot_annotation(title = \"Highlighting One Series Against Two Related Series\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.4 Color Palettes for Comparing Four Things\n\n30.1.4.1 Analogous Complementary for One Main Series and Its Three Secondary\nThe Analogous Complementary scheme, involving four distinct colors, provides an excellent platform for highlighting a primary series along with three related components. With this scheme, the key color stands out due to the similarities among the three complementary colors. An illustration of this can be seen in the subsequent example showcasing the ‘Malaysian Economic Miracle’ alongside three neighboring countries.\n\nCodep7 &lt;- malay_miracle %&gt;% ggplot(aes(x = year, y = gdpPercap, color = country)) +\n  geom_line(linewidth = 1.5) +\n  theme_minimal() +\n  # scale_y_log10() +\n  theme(legend.position = \"none\") +\n  scale_color_manual(values = c(\"#A1DA70\", \"#DA70D6\", \"#70DA74\", \"#70DAA9\")) +\n  geom_dl(aes(label = country), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\ncomplementary_4 + p7 + plot_annotation(title = \"Analogous Complementary for One Main Series and \\nIts Three Components\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.4.2 Double Complementary for Two Pairs Where One Pair Is Dominant\nThe Double Complementary Harmony scheme is ideal for visualizing four data series divided into two distinct pairs. It involves the key color, an adjacent color, and their respective opposites on the color wheel. Warmer colors are suggested for the key and adjacent colors, with their complementary counterparts in cooler tones. This color arrangement effectively highlights one pair over the other. As demonstrated below, the 1952 GDPs of Switzerland and Norway form one group denoted in purple hues, while Bosnia and Albania, differentiated in green-blue, form the other group.\n\nCodep8 &lt;- gapminder %&gt;%\n  filter((continent == \"Europe\")) %&gt;%\n  mutate(bot_top = case_when(\n    country %in% c(\"Albania\") ~ \"Albania\",\n    country %in% c(\"Bosnia and Herzegovina\") ~ \"Bosnia\",\n    country %in% c(\"Switzerland\") ~ \"Switzerland\",\n    country %in% c(\"Norway\") ~ \"Norway\",\n    T ~ \"other\"\n  )) %&gt;%\n  ggplot(aes(x = year, y = gdpPercap, color = bot_top, group = country, alpha = bot_top)) +\n  geom_line(linewidth = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\", legend.title = element_blank()) +\n  scale_y_log10() +\n  geom_dl(aes(label = bot_top), method = \"smart.grid\") +\n  scale_color_manual(values = c(\"#70DA74\", \"#70D6DA\", \"#DA70D6\", \"grey\", \"#DA7470\")) +\n  scale_alpha_manual(values = c(1, 1, 1, 0.3, 1)) +\n  labs( # title = \"Growth of the bottom 2 and \\ntop 2 countries by GDP in 1952\",\n    x = element_blank(), y = \"GDP per Capita (log10)\"\n  )\n\n\ntetradic + p8 + plot_annotation(title = \"Double Complementary for Two Pairs Where One Pair Is Dominant\", theme = theme(plot.title = element_text(size = 16)))\n\n\n\n\n\n30.1.4.3 Rectangular or Square Complementary for Four Series of Equal Emphasis\nThe Rectangular or Square Complementary scheme suits data visualization of four series with equal emphasis. Differing from the double complementary scheme, it includes the key color, its complement, and two additional colors to form a rectangle or square on the color wheel. This results in distinctive colors for each of the four series. While similar to double complementary, this scheme is optimal when all series share equal importance. For instance, in the graph below, the “Four Asian Tigers”—Hong Kong, Singapore, South Korea, and Taiwan—are shown. These economies, rapidly developed from the 1960s to 1990s, are all equally significant in illustrating the dynamism of East Asia’s growth.\n\nCodep9 &lt;- asian_tigers %&gt;% ggplot(aes(x = year, y = gdpPercap, color = country)) +\n  geom_line(linewidth = 1.5) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#DA70D6\", \"#DAA970\", \"#70DA74\", \"#70A1DA\")) +\n  geom_dl(aes(label = country), method = \"smart.grid\") +\n  labs(x = element_blank(), y = \"GDP per Capita (log10)\")\n\nsquare + p9 + plot_annotation(title = \"Rectangular or Square Complementary for \\nFour Series of Equal Emphasis\", theme = theme(plot.title = element_text(size = 16)))"
  },
  {
    "objectID": "chapters/color.html#sequential-and-divergent",
    "href": "chapters/color.html#sequential-and-divergent",
    "title": "\n30  Color Data\n",
    "section": "\n30.2 Sequential and Divergent",
    "text": "30.2 Sequential and Divergent\nSequential colors utilize a gradient from light to dark, mapping numeric values based on hue or lightness. Depending on the background, lower values receive lighter colors, while higher ones get darker shades. You can use a single hue or a sequence thereof.\nLet’s apply our beloved purple to GDP of different countries.\n\n30.2.1 Sequential\n\nCodelibrary(\"sf\")\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\") %&gt;% mutate(gdppc = gdp_md_est)\n\n\n\nCode(sequential | p10) +\n  patchwork::plot_layout(widths = c(1, 3)) +\n  plot_annotation(\n    title = \"Sequential\",\n    theme = theme(plot.title = element_text(size = 16))\n  )\n\n\n\n\n\n30.2.2 Divergent\nDiverging color schemes are employed when the numeric variable possesses a significant central value like zero. This scheme combines two sequential palettes with a common end, centering on the central value. Positive values receive colors from one side of the spectrum, while negative ones are designated colors from the other. The central value should ideally be a light shade, allowing darker colors to signify greater deviation from the center. Simplicity is key here to prevent diluting the intended meaning and confusing viewers. Rightly chosen, colors can minimize cognitive load, facilitating comprehension of complex data.\n\nCode(divergent | p11) +\n  patchwork::plot_layout(widths = c(1, 3)) +\n  plot_annotation(\n    title = \"Divergent\",\n    theme = theme(plot.title = element_text(size = 16))\n  )\n\n\n\n\n\n30.2.3 Prebuilt\nPrebuilt color scales such as “Viridis” are crafted for perceptual uniformity, ensuring visual appeal and ease of interpretation. These provide a standard, uniform color scheme, thus obviating the need for custom creation and testing. Moreover, they aid individuals with color blindness in interpreting data visualizations, owing to their consistent visual contrast. By employing prebuilt color scales, data visualizations can be made accessible to a broad audience.\n\nCode(viridis | p12) +\n  patchwork::plot_layout(widths = c(1, 3)) +\n  plot_annotation(\n    title = \"Virdis\",\n    theme = theme(plot.title = element_text(size = 16))\n  )"
  },
  {
    "objectID": "chapters/color_scheme.html#hsl",
    "href": "chapters/color_scheme.html#hsl",
    "title": "31  Color Systems",
    "section": "31.1 HSL",
    "text": "31.1 HSL\nThe HSL color system characterizes colors employing three parameters: hue, saturation, and lightness. Hue, designated by a value from 0 to 360 degrees on the color wheel, determines the fundamental color of the pixel. Saturation denotes the purity of the hue, representing the degree of gray mixed into the color. Values for saturation range from 0% (gray) to 100% (pure hue). Lightness, conversely, symbolizes the proportion of white or black mixed with the color, with 0% being black, 50% being the pure color, and 100% being white. Despite its usefulness in graphic design and web development, the HSL system has some constraints, such as not being perceptually uniform, which means that changes in the numeric values of the parameters may not correlate to equal changes in the perceived color."
  },
  {
    "objectID": "chapters/color_scheme.html#hsv",
    "href": "chapters/color_scheme.html#hsv",
    "title": "31  Color Systems",
    "section": "31.2 HSV",
    "text": "31.2 HSV\nThe HSV color system describes colors using three parameters: hue, saturation, and value. Hue and saturation function the same way as in the HSL system. Value signifies the brightness of the pixel, with 0% being black and 100% being the brightest possible color. Like the HSL system, the HSV system also suffers from a lack of perceptual uniformity."
  },
  {
    "objectID": "chapters/color_scheme.html#hcl",
    "href": "chapters/color_scheme.html#hcl",
    "title": "31  Color Systems",
    "section": "31.3 HCL",
    "text": "31.3 HCL\nThe HCL color system, a perceptually uniform color space, is frequently employed in data visualization and scientific applications. It comprises three values: hue, chroma, and lightness, which symbolize the color, saturation, and brightness of a color respectively. Owing to its capability to emulate human perception of color, the HCL color space is garnering increased popularity in design and user interface applications."
  },
  {
    "objectID": "chapters/color_scheme.html#lab",
    "href": "chapters/color_scheme.html#lab",
    "title": "31  Color Systems",
    "section": "31.4 LAB",
    "text": "31.4 LAB\nThe LAB color system, a device-independent color space designed to represent colors accurately across various devices and environments, comprises three parameters: L (lightness), a (position between red/magenta and green), and b (position between yellow and blue). The LAB color space finds frequent use in professional printing and color management applications due to its ability to enable accurate color matching across diverse devices and environments. Some of the newer LAB color spaces (e.g., OKLAB) are perceptually uniform, meaning that equal distances in the LAB color space correspond to equal increments in perceived color difference."
  },
  {
    "objectID": "chapters/color_scheme.html#oklab",
    "href": "chapters/color_scheme.html#oklab",
    "title": "31  Color Systems",
    "section": "31.5 OKLAB",
    "text": "31.5 OKLAB\nOKLAB is a color space engineered to be more perceptually uniform than other color spaces like sRGB or LAB. It employs an opponent color model, encoding color information as L – perceived lightness, a – the green/red(magenta) aspect of the color, and b – the blue/yellow aspect of the color. This allows the OKLAB color space to represent colors accurately while maintaining perceptual uniformity. OKLAB is growing in popularity in digital design and data visualization due to its enhanced accuracy and consistency in color representation.\nAlso known as Pac-Man illusion is an of afterimage complementary color (green as opposite to magenta or a in lab). Follow the movement of the rotating pink dot with your eyes and the dots will remain only one color, pink. But if you stare at the black + in the center, the moving dot will turn green.\n\n\n\nJeremy L. Hinton\n\n\nFor a deeper understanding of OKLAB and other color spaces, the blog posts on OKLAB and Colorpicker offer excellent insights."
  },
  {
    "objectID": "chapters/color_scheme.html#perceptual-uniformity",
    "href": "chapters/color_scheme.html#perceptual-uniformity",
    "title": "31  Color Systems",
    "section": "31.6 Perceptual Uniformity",
    "text": "31.6 Perceptual Uniformity\nHumans perceive colors differently than machines. For instance, a color that seems similar to a machine might not appear so to a human. The images below illustrate this concept using two color wheels, one RGB (perceptually non-uniform) and the other HCL (uniform). When viewed in grayscale, the non-uniform nature of the RGB color wheel becomes apparent. Technically, a perceptually uniform color space ensures that the difference between two colors, as perceived by the human eye, is proportional to the Euclidean distance within the given color space.\n\n\n\nuniform perception"
  },
  {
    "objectID": "chapters/color_scheme.html#warning-colormaps-might-increase-risk-of-death",
    "href": "chapters/color_scheme.html#warning-colormaps-might-increase-risk-of-death",
    "title": "31  Color Systems",
    "section": "31.7 Warning: Colormaps Might Increase Risk of Death!",
    "text": "31.7 Warning: Colormaps Might Increase Risk of Death!\nIn the 1990s, data visualization specialists adopted the Rainbow Color Map, the most renowned variation being the Jet default palette. However, researchers expressed concerns over its non-uniform nature, which introduced transitions that could be misperceived.\nRogowitz and Treinish voiced their concerns about the Rainbow Color Map in their 1998 article, “Data Visualization: The End of the Rainbow” (Rogowitz and Treinish 1998), and Borland and Taylor highlighted further concerns in their 2007 paper, “Rainbow Color Map (Still) Considered Harmful” (Borland and Taylor Ii 2007). In 2011, Borkin et al. conducted user studies on various color maps, including the Rainbow Color map, within medical visualization contexts. Their findings, published in “Evaluation of Artery Visualizations of Heart Disease Diagnosis” (Borkin et al. 2011), demonstrated that a perceptually uniform color map resulted in fewer diagnostic errors than the Rainbow Color map. Simply put, using a proper color palette could decrease diagnostic errors. Yet, as outlined by Crameri, Shephard, and Heron in their 2020 article, “The misuse of colour in science communication” (Crameri, Shephard, and Heron 2020), the improper use of color persists in science, making this a must-read article for any scientist.\n\nRogowitz, B. E., and L. A. Treinish. 1998. “Data Visualization: The End of the Rainbow.” IEEE Spectrum 35 (12): 52–59. https://doi.org/10.1109/6.736450.\n\nBorland, David, and Russell M. Taylor Ii. 2007. “Rainbow Color Map (Still) Considered Harmful.” IEEE Computer Graphics and Applications 27 (2): 14–17. https://doi.org/10.1109/MCG.2007.323435.\n\nBorkin, M., K. Gajos, A. Peters, D. Mitsouras, S. Melchionna, F. Rybicki, C. Feldman, and H. Pfister. 2011. “Evaluation of Artery Visualizations for Heart Disease Diagnosis.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2479–88. https://doi.org/10.1109/TVCG.2011.192.\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The Misuse of Colour in Science Communication.” Nature Communications 11 (1): 5444. https://doi.org/10.1038/s41467-020-19160-7.\n\n\n\nImage from the “Rainbow Color Map (Still Considered Harmful)\n\n\nThese issues intensify when considering colorblind individuals. Approximately 8% of all men and 0.5% of all women are colorblind. There are three main forms of colorblindness: protan (red), deutan (green), and tritan (blue), each corresponding to color-sensitive cones in our eyes. To check whether your visualization is colorblind-friendly, use Coblis (“Coblis  Color Blindness Simulator  Colblindor,” n.d.).\n\n“Coblis  Color Blindness Simulator  Colblindor.” n.d. https://www.color-blindness.com/coblis-color-blindness-simulator/.\nImproving the readability of your colors involves varying their value and hue, but avoid including both red and green in your graphics as red-green color blindness is the most common form.\n\n\n\nColor Blind Rainbow Flower"
  },
  {
    "objectID": "chapters/color_scheme.html#so-what-should-you-use",
    "href": "chapters/color_scheme.html#so-what-should-you-use",
    "title": "31  Color Systems",
    "section": "31.8 So, What Should You Use?",
    "text": "31.8 So, What Should You Use?\nA simple and correct answer would be to use a scientific color map that you find appealing and make it your default. If you need help, this graph from “The misuse of colour in science communication” might be useful.\n\n\n\nchoosing color map\n\n\nIf you want to select colors yourself, use HSL, as it is the most intuitive and easiest to use in creating color palettes. You might also want to experiment with OKHSL, a child of OKLAB and HSL that produces a perceptually uniform HSL space. Try out both of them and observe the difference here1.1 https://bottosson.github.io/misc/colorpicker/"
  },
  {
    "objectID": "chapters/color_scheme.html#where-can-i-find-color-waves",
    "href": "chapters/color_scheme.html#where-can-i-find-color-waves",
    "title": "31  Color Systems",
    "section": "31.9 Where Can I Find Color Waves?",
    "text": "31.9 Where Can I Find Color Waves?\nAdobe Color - Adobe Color allows you to create color palettes using different color harmony rules and color modes. You can also select colors from your image, create gradients from images, and test for accessibility.\nPaletton - This is a fantastic tool for creating color palettes.\nColor Brewer - Color Brewer provides perceptually uniform color schemes for maps and data visualizations.\nColor Thief - Color Thief lets you extract colors from your image to create nature-inspired palettes.\nViz Palette - Viz Palette can be used to check your color palettes before creating visualizations. It allows you to view color sets in example plots, simulate color deficiencies, and modify the colors of your palette.\nScientific colour maps - This is a collection of uniform and readable color maps for scientific use."
  },
  {
    "objectID": "chapters/which_graph.html#category-comparison",
    "href": "chapters/which_graph.html#category-comparison",
    "title": "32  A Graph for The Job",
    "section": "32.1 Category Comparison",
    "text": "32.1 Category Comparison\nGraphs for category comparison are a type of data visualization that are used to compare and contrast different categories or groups. The most common of them is bar chart! The one below shows the top 5 countries by GDP per Capita in 1997. You can easily see that Norway is first and Switzerland is 5th!\n\n\nCode\ngapminder %&gt;%\n  filter(year == 1997) %&gt;%\n  slice_max(gdpPercap, n = 5) %&gt;%\n  ggplot(aes(x = fct_reorder(country, gdpPercap, .desc = T), y = gdpPercap)) +\n  geom_col(fill = \"steelblue\") +\n  theme_minimal(base_size = 18) +\n  labs(x = NULL, y = NULL, title = \"Top 5 countries by GDP per Capita in 1997\") +\n  geom_text(aes(label = round(gdpPercap, 0)), vjust = 10, color = \"white\", size = 5) +\n  theme(panel.grid = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\nVertical bar charts are great to provide a quick comparison for a small number of categories (less than 7). But if need to show ranking of more things, flip the axis of the bar chart! Additional bonus, horizontal bar charts are great if you have long names to display. Below are the results of the 2021 London election. British YouTuber Niko Omilana finished 5th for the memes! Max Fosh, another YouTuber, also passed the cut off!\n\n\nCode\nbarh_data &lt;- tibble(\n  Candidate = c(\n    \"Sadiq Khan\", \"Shaun Bailey\", \"Siân Berry\",\n    \"Luisa Porritt\", \"Niko Omilana\", \"Laurence Fox\", \"Brian Rose\",\n    \"Richard Hewison\", \"Count Binface\", \"Mandu Reid\", \"Piers Corbyn\",\n    \"Vanessa Hudson\", \"Peter Gammons\", \"Farah London\", \"David Kurten\",\n    \"Nims Obunge\", \"Steve Kelleher\", \"Kam Balayev\", \"Max Fosh\", \"Valerie Brown\"\n  ),\n  Percentage = c(\n    40.0, 35.3, 7.8, 4.4, 2.0, 1.9,\n    1.2, 1.1, 1.0, 0.8, 0.8, 0.7, 0.6, 0.5, 0.4,\n    0.4, 0.3, 0.3, 0.2, 0.2\n  )\n) %&gt;% mutate(is.youtuber = case_when(\n  Candidate == \"Niko Omilana\" ~ 1,\n  Candidate == \"Max Fosh\" ~ 2,\n  T ~ 0\n))\n\n\n\n\nCode\nbarh_data %&gt;%\n  ggplot(aes(x = fct_reorder(Candidate, Percentage), y = Percentage, fill = as.factor(is.youtuber))) +\n  geom_col() +\n  theme_minimal(base_size = 16) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, title = \"London Mayor Elections (2021) by % of Votes\") +\n  theme(panel.grid = element_blank(), legend.position = \"none\", plot.caption.position = \"plot\") +\n  scale_y_discrete(expand = c(0, 0, 0, 3)) +\n  geom_text(aes(label = Percentage), nudge_y = 0.3, hjust = \"left\") +\n  scale_fill_manual(values = c(\"#c0bfff\", \"#f3e408\", \"#D96161\"))\n\n\n\n\n\n\n32.1.1 Lolipop Chart\nThe lollipop chart is a personal favorite, particularly when compared to traditional bar plots. Its distinct advantage lies in its ability to effectively convey the position of the final data point in a two-dimensional space.\n\n\nCode\nbarh_data %&gt;%\n  ggplot(aes(x = fct_reorder(Candidate, Percentage), y = Percentage, color = as.factor(is.youtuber))) +\n  geom_segment(aes(x = fct_reorder(Candidate, Percentage),\n                   xend= fct_reorder(Candidate, Percentage),\n                   y = 0,\n                   yend = Percentage\n                   )) +\n  geom_point() +\n  theme_minimal(base_size = 16) +\n  coord_flip(clip = \"off\") +\n  labs(x = NULL, y = NULL, title = \"London Mayor Elections (2021) by % of Votes\") +\n  theme(panel.grid = element_blank(), legend.position = \"none\", plot.caption.position = \"plot\") +\n  scale_y_discrete(expand = c(0, 0, 0, 3)) +\n  geom_text(aes(label = Percentage), color = 'black', nudge_y = 0.4, hjust = \"left\") +\n  scale_color_manual(values = c(\"#c0bfff\", \"#f3e408\", \"#D96161\"))\n\n\n\n\n\n\n\n32.1.2 Bullet Graph\nThe bullet graph is a powerful tool designed for comparing performance against a predefined target zone. Let’s test it with visualizing Net Promoter Score (NPS). The bullet graph clearly shows the target, various performance levels, and where our results stand in relation to the goal.\n\n\nCode\nggplot() +\n  geom_col(\n    aes(x = \"NPS\", y = 200),\n    fill = \"#A9A9A9\",\n    width = 0.6,\n    alpha = 0.9\n  )  +\n  geom_col(\n    aes(x = \"NPS\", y = 170),\n    fill = \"#808080\",\n    width = 0.6,\n    alpha = 0.9\n  ) +\n  geom_col(\n    aes(x = \"NPS\", y = 130),\n    fill = \"#696969\",\n    width = 0.6,\n    alpha = 0.9\n  ) +\n  geom_col(\n    aes(x = \"NPS\", y = 180),\n    fill = \"black\",\n    color = NA,\n    width = 0.2\n  ) +\n  geom_errorbar(\n    aes(x = \"NPS\", ymin = 160, ymax = 160),\n    color = \"red\",\n    width = 0.45,\n    size = 2\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    x = element_blank(),\n    y = element_blank(),\n    title = \"Net Promoter Score for 2023\",\n    subtitle = \"We beat the Target!\"\n  ) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_text(face = \"bold\")\n  ) +\n  annotate(\n    \"text\",\n    x = c(rep((1 + .45), 3),0.55),\n    y = c(\n      130 * 0.5,\n      130 + (170 - 130) * 0.5,\n      170+(200-170)*0.5 ,\n      160\n    ),\n    label = c(\"Poor\", \"Good\", \"Great\", \"Target\"),\n    color = c(rep(\"black\", 3), \"red\")\n  ) +\n  ylim(c(-100,100)) +\n  scale_y_continuous(breaks = seq(0, 200, 10), labels = seq(-100,100,10)) +\n  \n  NULL"
  },
  {
    "objectID": "chapters/which_graph.html#distribution",
    "href": "chapters/which_graph.html#distribution",
    "title": "32  A Graph for The Job",
    "section": "32.2 Distribution",
    "text": "32.2 Distribution\n\n32.2.1 Histogram\nWhat if you want to show the distribution of the data? We can use a variation of a bar chart – histogram! Histograms show the distribution of continuous data by grouping it into bins and displaying the frequency or proportion of observations that fall into each bin. They are great if you want to show the shape of the distribution, but they are very sensitive to the bins you choose. Notice how the shape of the distribution changes for each number of bins. It is important to strike a balance between too few and too many. 6 bins makes our distribution look pretty normal while 30 bins make it all over the place. 15 bins seems about right it preserves the bimodal feature of the distribution, while keeping the picture legible.\n\n\nCode\nbase &lt;- iris %&gt;% ggplot(aes(x = Sepal.Length)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), axis.text.y = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  labs(y = NULL, x = NULL)\nhist_1 &lt;- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 6) + labs(subtitle = \"6 bins\")\nhist_2 &lt;- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 15) + labs(subtitle = \"15 bins\")\nhist_3 &lt;- base + geom_histogram(fill = \"steelblue\", color = \"white\", bins = 30) + labs(subtitle = \"30 bins\")\n\n(hist_1 + hist_2 + hist_3) + plot_annotation(title = \"iris Sepal Length Distribution Histograms with Varying Bins\")\n\n\n\n\n\n\n\n32.2.2 Density Plot\nUnlike histograms, density plots use a continuous line to represent the data instead of bars. This smooth curve provides a more detailed and nuanced representation of the distribution of the data, allowing for easier detection of patterns and trends. The density plot constructs this line by placing many small normal distributions at each point in the data, which are then used to weigh all points within their respective range and draw a curve connecting them. The width of these curves is controlled by the bandwidth of the density plot, which determines how wide the curves span. A larger bandwidth will consider more points, resulting in a smoother curve, while a smaller bandwidth will lead to a jagged line.\n\n\nCode\nbase &lt;- iris %&gt;% ggplot(aes(x = Sepal.Length)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), axis.text.y = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  labs(y = NULL, x = NULL)\ndens_1 &lt;- base + geom_density(color = \"steelblue\", linewidth = 2, bw = 0.3) + labs(subtitle = \"Band Width 0.3\")\ndens_2 &lt;- base + geom_density(color = \"steelblue\", linewidth = 2, bw = 0.1) + labs(subtitle = \"Band Width 0.1\")\ndens_3 &lt;- base + geom_density(color = \"steelblue\", linewidth = 1, bw = 0.03) + labs(subtitle = \"Band Width 0.03\")\n\n(dens_1 + dens_2 + dens_3) + plot_annotation(title = \"iris Sepal Length Distribution Density Plots with Varying Band Widths\")\n\n\n\n\n\n\n\n32.2.3 Frequency Polygon\nIt is similar to a histogram, but instead of bars, it uses a continuous line to connect the points representing the frequencies. Frequency polygons are particularly useful when comparing two or more data sets on the same plot. Just like histogram it relies on the selection of bins.\n\n\nCode\nfreq_1 &lt;- base + geom_histogram(fill = \"grey\", color = \"white\", bins = 15, alpha = 0.5) + geom_freqpoly(color = \"steelblue\", bins = 15, linewidth = 1.5) + labs(subtitle = \"15 bins\")\n\nfreq_2 &lt;- iris %&gt;% ggplot(aes(x = Sepal.Length)) +\n  geom_histogram(aes(fill = Species), position = \"dodge\", color = \"white\", bins = 15, alpha = 0.3) +\n  geom_freqpoly(aes(color = Species), bins = 15, linewidth = 1.5) +\n  labs(subtitle = \"15 bins\") +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(), axis.text.y = element_blank(),\n    legend.position = c(.95, .95),\n    legend.justification = c(\"right\", \"top\"),\n    legend.box.just = \"right\",\n    legend.margin = margin(6, 6, 6, 6)\n  ) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  labs(y = NULL, x = NULL)\n\nfreq_1 + freq_2\n\n\n\n\n\n\n\n32.2.4 Box Plot\n\nBoxplots provide a summary of the distribution of a dataset, show the median, the lower and upper quartiles, and the minimum and maximum values of a dataset. The box in the middle represents the interquartile range (IQR), which is the range of the middle 50% of the data. The line in the box represents the median, which is the midpoint of the data. The whiskers on the top and bottom extend to the minimum and maximum values, excluding outliers. It is incredible how much information boxplots contain! With just one plot, you can quickly identify outliers and gain a visual understanding of the distribution of the data.\nIn the context of the iris dataset, the boxplot of Sepal Length across different species provides a clear picture of the distribution of this variable. However, like real boxes, boxplots can also hide important information. To illustrate this point, we can use a dataset with the same summary statistics but different distributions. In the second graph, three identical boxplots are displayed. However, once we add data points to the plot, it becomes evident that the distributions are quite different.\n\n\nCode\nset.seed(1337)\n\ndata_dist &lt;- tibble(\n  group = factor(c(rep(\"Group 1\", 100), rep(\"Group 2\", 250), rep(\"Group 3\", 25))),\n  value = c(\n    seq(0, 20, length.out = 100),\n    c(rep(0, 5), rnorm(30, 2, .1), rnorm(90, 5.4, .1), rnorm(90, 14.6, .1), rnorm(30, 18, .1), rep(20, 5)),\n    rep(seq(0, 20, length.out = 5), 5)\n  )\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(value = if_else(group == \"Group 2\", value + rnorm(1, 0, .4), value))\n\n\n\n\nCode\nbase2 &lt;- iris %&gt;% ggplot(aes(y = Sepal.Length, x = Species)) +\n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  labs(y = NULL, x = NULL)\nbox_1 &lt;- base2 + geom_boxplot()\n\nbase_dist &lt;- data_dist %&gt;% ggplot(aes(y = value, x = group)) +\n  theme_minimal() +\n  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  labs(y = NULL, x = NULL)\nbox_2 &lt;- base_dist + geom_boxplot()\nbox_3 &lt;- base_dist + geom_boxplot() + geom_point(color = \"orange\", size = 1.5, alpha = 0.25, position = position_jitter(width = 0.1))\n\nbox_1 + box_2 + box_3\n\n\n\n\n\n\n\n32.2.5 Violin Plot\nOne solution is to use violin plots. In its essence it is a vertical density plot. Look how much more we know about out data distribution of iris species! We can see the density distribution, points and quantiles!\n\n\nCode\nbase2 + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), bw = 0.15) + geom_jitter(alpha = 0.2, position = position_jitter(width = 0.1))\n\n\n\n\n\nGoing back to our new data set, notice how different the datasets look, clearly there are some patterns. Group 1, which has four times more observations, appears to be nearly identical to Group 3. This is because the default setting “scale =”area”” is a little misleading. We can fix that by changing it to “scale =”count””\n\n\nCode\ndist_1 &lt;- base_dist + geom_violin(scale = \"area\") + labs(subtitle = \"scale = 'area'\")\ndist_2 &lt;- base_dist + geom_violin(scale = \"count\") + labs(subtitle = \"scale = 'count'\")\ndist_1 + dist_2\n\n\n\n\n\nDo you remember how band width is extremely important when making density plots? Setting an apprpriate band width reveals the true distribution!\n\n\nCode\nbase_dist + geom_violin(scale = \"count\", bw = .3, color = NA, fill = \"steelblue\")\n\n\n\n\n\n\n\n32.2.6 Bee Hive Plot\nThe bee hive plot is a scatter plot that arranges data points as dots to minimize overlap. It’s ideal for visualizing small datasets because it creates patterns like a density plot without hiding individual data points.\n\n\nCode\nbee_1 &lt;- base2 + geom_beeswarm()\nbee_2 &lt;- base_dist + geom_beeswarm()\nbee_1 + bee_2\n\n\n\n\n\nAll of the plots we have covered so far have their advantages: 1. Box plot shows important statistics 2. Density plot provides high-level view of data shape 3. Bee hive plot “shows” the actual datapoints While combining these plots might make for a crowded visual, with some modifications, it’s possible to create a hybrid plot that captures the strengths of each.\n\n\nCode\nbase_dist + geom_violin(fill = \"steelblue\", alpha = .4, scale = \"count\", bw = .4) + geom_boxplot(fill = NA, width = .1) + geom_beeswarm(color = \"orange\", alpha = .8, cex = 1)\n\n\n\n\n\n\n\n32.2.7 Margins\nMarginal histograms are a method for visualizing data distributions in relation to two variables. In this visualization, a histogram for each variable is positioned on the scatterplot’s edges. This allows us to examine both the individual distributions of each variable and the relationship between the two.\n\n\nCode\n1library(ggExtra)\n\n2p &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n3  geom_point(aes(color = Species)) +\n4  theme_minimal()+\n5  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n6p_marginal &lt;- ggMarginal(p, type = \"density\", groupFill = TRUE)\n\n7p_marginal\n\n\n\n1\n\nLoad the ggExtra package, which provides functions to enhance ggplot2 plots, such as adding marginal histograms.\n\n2\n\nCreate a base scatter plot of Sepal Length against Sepal Width using the ggplot function from the ggplot2 package. The aes function is used to specify the x and y variables.\n\n3\n\nAdd points to the scatter plot using the geom_point function, with color representing different Species.\n\n4\n\nApply a minimal theme to the plot using the theme_minimal function, which removes most non-data ink from the plot.\n\n5\n\nAdjust the legend position to the bottom and remove the legend title using the theme function.\n\n6\n\nAdd marginal histograms to the scatter plot using the ggMarginal function, and set the type argument to \"density\" to create density plots instead of histograms. The groupFill argument is set to TRUE to fill the densities with different colors based on group (Species).\n\n7\n\nDisplay the plot.\n\n\n\n\n\n\n\n\n\n32.2.8 Rain Cloud Plot\nRain Cloud Plot combines elements of box plots, violin plots, and density plots. It uses a density plot to show the distribution of the data, a box plot to display the statistics, and individual data points are represented as rain drops. The result is a visually appealing and informative way to visualize a large number of distributions side-by-side, allowing for easy comparisons and identification of patterns.\nIsn’t this beautiful? We have a box plot, density plot, and jittered points all in the same graph without looking cluttered.\n\n\nCode\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3, # bw\n    width = .6,\n    .width = 0,\n    justification = -.2,\n    point_colour = NA\n  ) +\n  geom_boxplot(\n    width = .15,\n    outlier.shape = NA\n  ) +\n  ## add justified jitter from the {gghalves} package\n  gghalves::geom_half_point(\n    ## draw jitter on the left\n    side = \"l\",\n    ## control range of jitter\n    range_scale = .4,\n    ## add some transparency\n    alpha = .3\n  ) +\n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\")\n\n\n\n\n\nOne alternative option to the boxplot is to stack the data points and use a minimal boxplot representation. While this alternative can be visually appealing, it is important to ensure that your audience understands the visualization and the meaning behind the stacked data points. It may be necessary to provide additional context or include a note explaining the meaning of the stacked slabs to avoid confusion.\n\n\nCode\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3,\n    width = .6,\n    ## set slab interval to show IQR and 95% data range\n    .width = c(.5, .95)\n  ) +\n  ggdist::stat_dots(\n    side = \"left\",\n    dotsize = .8,\n    justification = 1.05,\n    binwidth = .3\n  ) +\n  coord_cartesian(xlim = c(1.2, NA))\n\n\n\n\n\nMy personal favorite is the rain cloud plot, which combines vertical lines and a bar plot that is rotated horizontally to resemble actual rain clouds.\n\n\nCode\nbase_dist +\n  ggdist::stat_halfeye(\n    adjust = .3,\n    width = .6,\n    .width = 0,\n    justification = -.2,\n    point_colour = NA\n  ) +\n  geom_boxplot(\n    width = .15,\n    outlier.shape = NA\n  ) +\n  geom_half_point(\n    ## draw horizontal lines instead of points\n    shape = \"|\",\n    side = \"l\",\n    size = 5,\n    alpha = .2,\n    transformation = position_identity()\n  ) +\n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") + coord_flip()"
  },
  {
    "objectID": "chapters/which_graph.html#proportions",
    "href": "chapters/which_graph.html#proportions",
    "title": "32  A Graph for The Job",
    "section": "32.3 Proportions",
    "text": "32.3 Proportions\nAnother big collection of graphs is concerned with communicating proportions and composition.\n\n32.3.1 Stacked Bar Charts\n\n\nCode\nus_spending &lt;- read_csv(\"data/USFR_StmtNetCost_2017_2022.csv\") %&gt;%\n  janitor::clean_names() %&gt;%\n  filter((restatement_flag == \"N\") & (agency_name != \"Total\")) %&gt;%\n  select(year = statement_fiscal_year, agency_name, net_cost_in_billions) %&gt;%\n  mutate(net_cost_in_billions = as.numeric(net_cost_in_billions)) %&gt;%\n  group_by(year) %&gt;%\n  mutate(proportion = round(net_cost_in_billions / sum(net_cost_in_billions), 2)) %&gt;%\n  ungroup()\n\nspending_plot_data &lt;- us_spending %&gt;%\n  group_by(year) %&gt;%\n  mutate(rank = rank(-1 * net_cost_in_billions), agency = ifelse(rank &gt;= 5, \"Other\", agency_name)) %&gt;%\n  count(year, agency, wt = net_cost_in_billions) %&gt;%\n  mutate(other = agency == \"Other\") %&gt;%\n  group_by(other) %&gt;%\n  arrange(desc(n), .by_group = T) %&gt;%\n  ungroup() %&gt;%\n  mutate(order = -1 * row_number()) %&gt;%\n  mutate(agency = recode(agency,\n    \"Department of Veterans Affairs\" = \"Veterans Affairs\",\n    \"Department of Health and Human Services\" = \"HHS\",\n    \"Department of Defense\" = \"Defense\",\n    \"Social Security Administration\" = \"SSA\",\n    \"Department of the Treasury\" = \"Treasury\",\n    \"Interest on Treasury Securities Held by the Public\" = \"i on Treasuries\"\n  )) %&gt;%\n  mutate(agency = factor(agency, c(\"Other\", \"i on Treasuries\", \"Veterans Affairs\", \"Defense\", \"Treasury\", \"SSA\", \"HHS\")))\n\n\nA stacked bar chart is a type of graph used to visualize the distribution of a categorical variable. It is similar to a regular bar chart, but in a stacked bar chart, each bar is divided into sections, with each section representing a different category within the variable. The height of each section corresponds to the proportion or frequency of the category within that bar. Stacked bar charts are particularly useful when comparing the distribution of a variable across different subgroups or time periods, as they allow for easy visualization of both the overall distribution as well as the relative proportions of each subgroup or category within the variable.\nAs an example we will use US Expernditures across departments. Only top four departments are shown, the rest are collected into “other”. The graph below shows absolute values and its components across years.\n\n\nCode\nspending_plot_data %&gt;%\n  ggplot(aes(x = year, y = n, fill = agency, label = agency)) +\n  geom_col(position = \"stack\", show.legend = F) +\n  scale_fill_manual(values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(y = \"Millions Spent\") +\n  geom_label(size = 3, aes(group = agency), position = position_stack(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nWhat if we are not concerned with absolute values, but relative proportions? We can use percentage stacked chart.\n\n\nCode\nspending_plot_data %&gt;%\n  ggplot(aes(x = year, y = n, fill = agency)) +\n  geom_col(position = \"fill\", show.legend = T) +\n  scale_fill_manual(\n    values = c(\"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#006837\", \"#F7DC6F\", \"#00FFFF\", \"#FFC0CB\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(aes(x = year, y = n, label = agency, group = agency), size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\nStacked charts are useful for visualizing the distribution of categorical variables, but they can be challenging to compare categories in the middle. Typically, the easiest categories to compare are the ones at the top and bottom of the stack. For example, suppose we want to compare the trend of the Department of Defense and the Social Security Administration (SSA) over time. In this case, we can move these categories to the top and bottom positions of the stacked chart to make it easier to compare their relative sizes and trends.\n\n\nCode\nspending_plot_data %&gt;%\n  mutate(agency = factor(agency, c(\"Defense\", \"Other\", \"i on Treasuries\", \"Veterans Affairs\", \"Treasury\", \"HHS\", \"SSA\"))) %&gt;%\n  ggplot(aes(x = year, y = n, fill = agency)) +\n  geom_col(position = \"fill\", show.legend = T) +\n  scale_fill_manual(\n    values = c(\"#006837\", \"#5E5E5E\", \"#EF3B2C\", \"#2CA25F\", \"#F7DC6F\", \"#FFC0CB\", \"#00FFFF\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(y = \"Millions Spent\", fill = \"Department\") +\n  geom_label(aes(x = year, y = n, label = agency, group = agency), size = 3, position = position_fill(vjust = 0.5), fill = \"white\", alpha = 0.5)\n\n\n\n\n\n\n\n32.3.2 Pie Chart\ndata used from: The Growth Lab at Harvard University. The Atlas of Economic Complexity. http://www.atlas.cid.harvard.edu.\nAs an example dataset we will be used Japan’s export basket from 2020.\n\n\nCode\njapan_export &lt;- read_csv(\"data/japan_export_2020.csv\") %&gt;%\n  rename(\"export\" = `Gross Export`) %&gt;%\n  janitor::clean_names()\n\njapan_sectors &lt;- japan_export %&gt;% count(sector, wt = share)\n\n\nPie charts are a variation of bar charts where each category is represented as a slice of a circle. While pie charts can effectively communicate when one category is significantly larger or smaller than the others, they become difficult to read and compare accurately when there are many categories or the differences between them are small. Comparing angles and areas of the slices can be confusing, leading to misinterpretation of the data.\n\n\nCode\npie_1 &lt;- japan_sectors %&gt;%\n  ggplot(aes(x = \"\", y = n, fill = sector)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\npie_2 &lt;- japan_sectors %&gt;%\n  ggplot(aes(x = \"\", y = n, fill = sector)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  theme(panel.background = element_rect(fill = \"white\"))\n\npie_1 + pie_2\n\n\n\n\n\nBut if absolutely must use a pie chart here are some rules to keep in mind: 1. Limit the number of categories to 5-7 at most. 2. Consider grouping small categories into an “Other” category to avoid clutter. 3. Arrange the slices in decreasing order of size, starting at 12 o’clock to aid in comparing them. 4. Include the category labels directly on the chart instead of relying solely on a legend. 5. Add separators between slices to help with distinguishing between them. However, keep in mind that this can also add visual clutter, so use with discretion.\n\n\nCode\njapan_sectors %&gt;%\n  mutate(sector = ifelse(n &lt; 11, \"Other\", sector)) %&gt;%\n  count(sector, wt = n) %&gt;%\n  mutate(other = sector == \"Other\") %&gt;%\n  group_by(other) %&gt;%\n  arrange(desc(n), .by_group = T) %&gt;%\n  ungroup() %&gt;%\n  mutate(prop = n / sum(japan_sectors$n) * 100) %&gt;%\n  mutate(ypos = cumsum(prop) - 0.5 * prop) %&gt;%\n  mutate(order = -1 * row_number()) %&gt;%\n  ggplot(aes(x = \"\", y = n, fill = fct_reorder(sector, order))) +\n  geom_bar(\n    stat = \"identity\", width = 1, color =\n      \"white\"\n  ) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = ypos, label = sector), color = c(\"white\", \"#333333\", rep(\"white\", 4)), size = 6) +\n  scale_fill_manual(values = c(\"grey\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\"))\n\n\n\n\n\n\n\n32.3.3 Waffle Chart\nOne alternative to a pie chart could be a waffle chart (these food names make me hungry). It is a grid-like visualization that resembles a waffle or a checkerboard. Each square in the grid represents a proportion of the total data, making it a useful way to visualize proportions or percentages in a visually appealing way. However, they are also vulnerable to large numbers of categories. But what they are truly great at is giving the sense of proportions and sizes. Waffle chart will significantly benefit from interactivity.\n\n\nCode\n# waffle_data &lt;- waffle_iron(iris, aes_d(group = Species))\n#\n# ggplot(waffle_data, aes(x, y, fill = group)) +\n#   geom_waffle() +\n#   coord_equal() +\n#   scale_fill_viridis_d() +\n#   theme_waffle() +\n#   theme(legend.position = \"top\", legend.title = element_blank()) +\n#   labs(x = element_blank(), y = element_blank())\n\njapan_sectors %&gt;%\n  # mutate(sector = ifelse(n&lt;11,\"Other\",sector)) %&gt;%\n  # count(sector, wt = n) %&gt;%\n  # mutate(other = sector == \"Other\") %&gt;%\n  # group_by(other) %&gt;%\n  # arrange(desc(n),.by_group = T) %&gt;%\n  # ungroup() %&gt;%\n  mutate(other = sector == \"Other\") %&gt;%\n  uncount(weights = round(n, 0)) %&gt;%\n  group_by(other) %&gt;%\n  arrange(desc(n), .by_group = T) %&gt;%\n  ungroup() %&gt;%\n  waffle_iron(aes_d(group = sector)) %&gt;%\n  ggplot(aes(x, y, fill = group)) +\n  geom_waffle() +\n  coord_equal() +\n  # scale_fill_viridis_d() +\n  theme_waffle() +\n  theme(legend.position = \"top\", legend.title = element_blank()) +\n  labs(x = element_blank(), y = element_blank()) +\n  scale_fill_manual(values = c(\"#CF8F00\", \"#E52B50\", \"#003366\", \"#228B22\", \"#1E90FF\", \"#FFD700\", \"#666666\", \"#800000\", \"#9932CC\", \"#8B0000\", \"#FFA07A\"))\n\n\n\n\n\n\n\n32.3.4 Tree Maps\nWhat if we have a lot of data hierarchical data? Treemaps!\nTreemaps are a type of visualization that allows you to display hierarchical data in a way that is easy to understand. Each node in the hierarchy is represented by a rectangle, and the size of the rectangle corresponds to the proportion of the total data. The nodes are organized in a way that preserves the hierarchy, with parent nodes containing smaller child nodes. This allows you to quickly identify which nodes are the largest and which are the smallest, as well as the relationships between them. Tree maps are especially useful for displaying large amounts of data in a compact and intuitive way. Tree maps can become very cluttered and interactivity is almost always necessary for such detailed plots. Check out the same plot from the source website.\n\n\nCode\nlibrary(treemap)\ntreemap(japan_export,\n  index = c(\"sector\", \"name\"),\n  vSize = \"export\",\n  type = \"index\",\n  fontsize.labels = c(14, 10),\n  fontcolor.labels = c(\"black\", \"white\"),\n  fontface.labels = c(2, 1),\n  bg.labels = 0,\n  align.labels = list(\n    c(\"center\", \"center\"),\n    c(\"left\", \"top\")\n  ),\n  border.col = c(\"black\", \"white\"),\n  border.lwds = c(3, 1),\n  title = \"Japans Export in 2020\",\n  fontsize.title = 14\n)"
  },
  {
    "objectID": "chapters/which_graph.html#correlation",
    "href": "chapters/which_graph.html#correlation",
    "title": "32  A Graph for The Job",
    "section": "32.4 Correlation",
    "text": "32.4 Correlation\nIn addition to understanding the distribution of individual variables, it is important to examine the relationship between pairs of variables. Correlation plots are a useful tool for visualizing many aspects of data: relationships between variables (or lack there of), clustering, outliers, etc.\n\n32.4.1 Scatter Plot\nThe most common visualization is scatter plot! It is not a secret for anyone that scatter plots are amazing and perhaps the most persuasive types of plot. We can add a fitted lines to the plot to better show the relationships between the variables.\n\n\nCode\nscatter_1 &lt;- iris %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(show.legend = F) +\n  geom_dl(aes(label = Species), method = \"smart.grid\") +\n  theme_minimal() +\n  labs(x = \"Sepal Length\", y = \"Sepal Width\")\n\nscatter_2 &lt;- scatter_1 + geom_smooth(se = F, fullrange = F, show.legend = F, method = \"lm\", linewidth = 2) + theme(\n  axis.title.y = element_blank(),\n  axis.text.y = element_blank(),\n  axis.ticks.y = element_blank()\n)\n\nscatter_1 + scatter_2\n\n\n\n\n\n\n\n32.4.2 Correlograms\nCorrelograms serve as efficient tools to visualize the relationships within a dataset swiftly. They allow us to visualize correlations between all pairs of variables, offering valuable insights into the data. Understanding these correlations is critical during the analysis and exploration of multidimensional data. Several ways exist to structure correlograms, but we will present the most common one below.\n\n\nCode\nlibrary(GGally)\n\nGGally::ggpairs(\n1  iris,\n2  mapping = ggplot2::aes(color = Species),\n3  progress = FALSE\n)\n\n\n\n1\n\nSelect the dataset for which to create a correlogram, in this case, the iris dataset.\n\n2\n\nMap the color of points to the Species variable.\n\n3\n\nDisable the display of a progress bar during the plot’s creation."
  },
  {
    "objectID": "chapters/which_graph.html#change-over-time",
    "href": "chapters/which_graph.html#change-over-time",
    "title": "32  A Graph for The Job",
    "section": "32.5 Change over Time",
    "text": "32.5 Change over Time\nWe have already seen plots that incorporate time change. Time series plots typically have time on the x-axis and the variable being measured on the y-axis. They can show trends, patterns, and seasonal fluctuations in the data.\n\n32.5.1 Line Chart\nMost common\nS&P 500 stock market index since 1927. Historical data is inflation-adjusted using the headline CPI and each data point represents the month-end closing value.\n\n\nCode\nsp500 &lt;- tribble(\n  ~Year, ~Average_Closing_Price, ~Year_Open, ~Year_High, ~Year_Low, ~Year_Close, ~Annual_Percent_Change,\n  2023, 4020.94, 3824.14, 4179.76, 3808.10, 3970.04, 3.40,\n  2022, 4097.49, 4796.56, 4796.56, 3577.03, 3839.50, -19.44,\n  2021, 4273.41, 3700.65, 4793.06, 3700.65, 4766.18, 26.89,\n  2020, 3217.86, 3257.85, 3756.07, 2237.40, 3756.07, 16.26,\n  2019, 2913.36, 2510.03, 3240.02, 2447.89, 3230.78, 28.88,\n  2018, 2746.21, 2695.81, 2930.75, 2351.10, 2506.85, -6.24,\n  2017, 2449.08, 2257.83, 2690.16, 2257.83, 2673.61, 19.42,\n  2016, 2094.65, 2012.66, 2271.72, 1829.08, 2238.83, 9.54,\n  2015, 2061.07, 2058.20, 2130.82, 1867.61, 2043.94, -0.73,\n  2014, 1931.38, 1831.98, 2090.57, 1741.89, 2058.90, 11.39,\n  2013, 1643.80, 1462.42, 1848.36, 1457.15, 1848.36, 29.60,\n  2012, 1379.61, 1277.06, 1465.77, 1277.06, 1426.19, 13.41,\n  2011, 1267.64, 1271.87, 1363.61, 1099.23, 1257.60, 0.00,\n  2010, 1139.97, 1132.99, 1259.78, 1022.58, 1257.64, 12.78,\n  2009, 948.05, 931.80, 1127.78, 676.53, 1115.10, 23.45,\n  2008, 1220.04, 1447.16, 1447.16, 752.44, 903.25, -38.49,\n  2007, 1477.18, 1416.60, 1565.15, 1374.12, 1468.36, 3.53,\n  2006, 1310.46, 1268.80, 1427.09, 1223.69, 1418.30, 13.62\n)\n\n\n\n\nCode\nsp500_scatter &lt;- sp500 %&gt;%\n  select(-c(Average_Closing_Price, Year_Open, Annual_Percent_Change)) %&gt;%\n  pivot_longer(-Year) %&gt;%\n  mutate(year_close = name != \"Year_Close\") %&gt;%\n  ggplot(aes(x = Year, y = value, color = name)) +\n  geom_point(size = 2) +\n  geom_dl(aes(label = name), method = \"smart.grid\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = element_blank(), y = \"S&P 500\")\n\nsp500_line &lt;- sp500 %&gt;%\n  select(-c(Average_Closing_Price, Year_Open, Annual_Percent_Change)) %&gt;%\n  pivot_longer(-Year) %&gt;%\n  mutate(year_close = name != \"Year_Close\") %&gt;%\n  ggplot(aes(x = Year, y = value, color = name)) +\n  geom_line(aes(linetype = year_close), linewidth = 1.5) +\n  scale_color_manual(values = c(\"steelblue\", \"grey\", \"grey\")) +\n  geom_dl(aes(label = name), method = \"smart.grid\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = element_blank(), y = \"S&P 500\")\n\nsp500_scatter + sp500_line"
  },
  {
    "objectID": "chapters/which_graph.html#waterfall-graph",
    "href": "chapters/which_graph.html#waterfall-graph",
    "title": "32  A Graph for The Job",
    "section": "32.6 Waterfall Graph",
    "text": "32.6 Waterfall Graph\nWaterfall charts, also known as bridge charts, are a type of bar chart used to visualize the cumulative effect of sequentially introduced positive or negative values. The graph is named “waterfall” because it resembles a series of falling water droplets. Each bar in the chart represents a value and is color-coded to indicate whether it contributes to an increase or decrease in the cumulative total. They are useful for visualizing the relative contributions of positive and negative factors that affect the net change in the value being analyzed.\n\n\nCode\nwaterfall_data &lt;- tribble(\n  ~year, ~bank, ~change,\n  2017, 2000, 2000,\n  2018, 1745, -255,\n  2019, 1930, 185,\n  2020, 2197, 267,\n  2021, 2453, 256,\n  2022, 2300, -153,\n) %&gt;% transmute(as.character(year), change)\n\nlibrary(waterfalls)\n\nwaterfall(waterfall_data,\n  calc_total = TRUE,\n  total_rect_color = \"orange\",\n  total_rect_text_color = \"white\"\n) +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  labs(y = \"Money in Bank\", x = NULL)"
  },
  {
    "objectID": "chapters/tables.html#gt-tables",
    "href": "chapters/tables.html#gt-tables",
    "title": "33  Make Tables",
    "section": "33.1 gt Tables",
    "text": "33.1 gt Tables\n\n33.1.1 Installing and Loading the gt Package\nTo start, ensure you have the gt package installed. We’ll also use the gtExtras package to expand the gt package’s capabilities with custom themes, conditional formatting, and more. In addition, we’ll use the emojifont package for accessing emojis and dplyr from tidyverse for data manipulation.\n\n# install.packages(c(\"gt\",\"gtExtras\",\"emojifont\"))\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(emojifont)\n\nAs an example we will use built-in gtcars dataset which contains information on various automobiles. Let’s look at it using gt().\n\ndplyr::sample_n(gt::gtcars, size = 4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      year\n      trim\n      bdy_style\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n      mpg_c\n      mpg_h\n      drivetrain\n      trsmn\n      ctry_origin\n      msrp\n    \n  \n  \n    Mercedes-Benz\nSL-Class\n2016\nSL400 Convertible\nconvertible\n329\n5250\n354\n1600\n20\n27\nrwd\n7am\nGermany\n85050\n    Audi\nS8\n2016\nBase Sedan\nsedan\n520\n5800\n481\n1700\n15\n25\nawd\n8am\nGermany\n114900\n    Audi\nR8\n2015\n4.2 (Manual) Coupe\ncoupe\n430\n7900\n317\n4500\n11\n20\nawd\n6m\nGermany\n115900\n    Tesla\nModel S\n2017\n75D\nsedan\n259\n6100\n243\nNA\nNA\nNA\nawd\n1dd\nUnited States\n74500\n  \n  \n  \n\n\n\n\n\n\n33.1.2 Prepare your data\nThe gt package works seamlessly with dplyr, allowing us to utilize familiar verbs for table formatting. Let’s start by focusing on the top auto industry country, Germany. We’ll group cars by their manufacturer (mfr) and sort them based on their price (msrp). During table creation, the rowname_col parameter allows us to designate a column as row names - for this example, we’ll use the ‘model’ column.\n\n1(table_grouped &lt;- gt::gtcars %&gt;%\n2  dplyr::filter(ctry_origin == \"Germany\") %&gt;%\n3  dplyr::group_by(mfr) %&gt;%\n4  dplyr::arrange(desc(msrp)) %&gt;%\n5  dplyr::slice_head(n = 2) %&gt;%\n6  dplyr::filter(mfr %in% c(\"Audi\", \"BMW\")) %&gt;%\n7  gt::gt(rowname_col = \"model\"))\n\n\n1\n\nStart with the gtcars dataset from the gt package.\n\n2\n\nFilter the data to include only cars that originated from Germany.\n\n3\n\nGroup the data by the manufacturer.\n\n4\n\nArrange the data in descending order based on the Manufacturer’s Suggested Retail Price (MSRP).\n\n5\n\nSelect only the top two cars (based on the previous sorting).\n\n6\n\nFilter to include only cars from Audi and BMW.\n\n7\n\nStart a new gt table using the car models as row names.\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      year\n      trim\n      bdy_style\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n      mpg_c\n      mpg_h\n      drivetrain\n      trsmn\n      ctry_origin\n      msrp\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n4.2 (Manual) Coupe\ncoupe\n430\n7900\n317\n4500\n11\n20\nawd\n6m\nGermany\n115900\n    S8\n2016\nBase Sedan\nsedan\n520\n5800\n481\n1700\n15\n25\nawd\n8am\nGermany\n114900\n    \n      BMW\n    \n    i8\n2016\nMega World Coupe\ncoupe\n357\n5800\n420\n3700\n28\n29\nawd\n6am\nGermany\n140700\n    M6\n2016\nBase Coupe\ncoupe\n560\n6000\n500\n1500\n15\n22\nrwd\n7a\nGermany\n113400\n  \n  \n  \n\n\n\n\nThe table contains many columns, and to make it more readable, we can hide some columns using cols_hide. Why use cols_hide instead of dropping the columns with dplyr? Sometimes, we may need to utilize a column in conditional statements but we do not want to display it. To group performance-related columns together, we can use cols_move. Though this could be done with dplyr, integrating it into our table-creation process makes the workflow more streamlined. We can distinguish these grouped columns by adding a header spanner with tab_spanner, specifying the columns and setting the label.\n\n1(table_span &lt;- table_grouped %&gt;%\n2  gt::cols_hide(columns = c(trim, bdy_style, drivetrain, ctry_origin, trsmn)) %&gt;%\n3  gt::cols_move(columns = c(msrp, trsmn, mpg_c, mpg_h), after = year) %&gt;%\n4  gt::tab_spanner(columns = c(mpg_c, mpg_h, hp, hp_rpm, trq, trq_rpm), label = \"Performance\"))\n\n\n1\n\nStart with the previously created table_grouped.\n\n2\n\nHide certain columns that aren’t needed in the final table.\n\n3\n\nMove specific columns to appear after the year column.\n\n4\n\nCreate a spanner header for performance-related columns.\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      year\n      msrp\n      \n        Performance\n      \n    \n    \n      mpg_c\n      mpg_h\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n115900\n11\n20\n430\n7900\n317\n4500\n    S8\n2016\n114900\n15\n25\n520\n5800\n481\n1700\n    \n      BMW\n    \n    i8\n2016\n140700\n28\n29\n357\n5800\n420\n3700\n    M6\n2016\n113400\n15\n22\n560\n6000\n500\n1500\n  \n  \n  \n\n\n\n\nAt times, we may want to combine pairs of related columns into a single column. For that, we can use the cols_merge() function. We’ll merge horsepower (hp) with associated rpm (hp_rpm), torque (trq) with associated rpm (trq_rpm), and city miles per gallon (mpg_c) with highway miles per gallon (mpg_h). The function takes columns, which can be referenced in text with {#} of column. Since we are working with HTML tables, we can use HTML tags, specifically &lt;br&gt;, to add a line break. Note that the joined column will use the first column’s name, and the labels are purely cosmetic and can’t be used as reference. We’ll then use the cols_label() function to assign custom labels to the columns, using the column_name = \"column_label\" syntax.\n\n1(table_merge &lt;- table_span %&gt;%\n2  cols_merge(columns = c(hp, hp_rpm), pattern = \"{1}&lt;br&gt;@{2}rpm\") %&gt;%\n3  cols_merge(columns = c(trq, trq_rpm), pattern = \"{1}&lt;br&gt;@{2}rpm\") %&gt;%\n4  cols_merge(columns = c(mpg_c, mpg_h), pattern = \"{1}c&lt;br&gt;{2}h\") %&gt;%\n5  cols_label(\n    year = \"Year\",\n    msrp = \"MSRP\",\n    mpg_c = \"MPG\",\n    hp = \"Horse Power\",\n    trq = \"Torque\"\n  ))\n\n\n1\n\nStart with the previously created table_span.\n\n2\n\nMerge the horsepower (hp) and horsepower rpm (hp_rpm) columns.\n\n3\n\nMerge the torque (trq) and torque rpm (trq_rpm) columns.\n\n4\n\nMerge the city mpg (mpg_c) and highway mpg (mpg_h) columns.\n\n5\n\nSet custom labels for specific columns.\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      Year\n      MSRP\n      \n        Performance\n      \n    \n    \n      MPG\n      Horse Power\n      Torque\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n115900\n11c20h\n430@7900rpm\n317@4500rpm\n    S8\n2016\n114900\n15c25h\n520@5800rpm\n481@1700rpm\n    \n      BMW\n    \n    i8\n2016\n140700\n28c29h\n357@5800rpm\n420@3700rpm\n    M6\n2016\n113400\n15c22h\n560@6000rpm\n500@1500rpm\n  \n  \n  \n\n\n\n\nThe gt package provides a variety of fmt_* formatting functions that are useful for adjusting the display of numeric and text columns. For example, we can set the msrp column to display currency in USD without decimals. We can adjust the alignment of columns using cols_align, allowing us to select columns and set alignments to “right”, “left”, or “center”. Since the merged columns appear bulky with two lines, we can decrease the text size using tab_style. This function utilizes a style definition, provided by helper functions such as cell_styles that include supported style information. In this example, we use cell_text to set the text size to “12px”. The second argument, locations, uses another helper function cells_*(). To target cells in the body, we apply cells_body to the columns mpg_c, hp, and trq. To enhance our tables with color, we can use the data_color function, which supports the creation of a simple gradient or the application of a solid color to the data. The function can specify a domain and use prebuilt palettes. For more functionality, consider gt_color_rows() from gtExtras. For more information, refer to the package’s documentation.\n\n(table_format &lt;- table_merge %&gt;% \n1  fmt_currency(columns = msrp, decimals = 0, currency = \"USD\") %&gt;%\n2  cols_align(columns = c(mpg_c, hp, trq), align = \"center\") %&gt;%\n3  tab_style(\n    style = cell_text(size = \"12px\"),\n    locations = cells_body(columns = c(mpg_c, hp, trq))\n  ) %&gt;%\n4  data_color(columns = msrp, colors = c(\"white\", \"aquamarine\")))\n\n\n1\n\nFormat the msrp column as currency\n\n2\n\nCenter align specific columns\n\n3\n\nApply cell text style to specific columns\n\n4\n\nApply a data-driven color scale to the msrp column\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      Year\n      MSRP\n      \n        Performance\n      \n    \n    \n      MPG\n      Horse Power\n      Torque\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n$115,900\n11c20h\n430@7900rpm\n317@4500rpm\n    S8\n2016\n$114,900\n15c25h\n520@5800rpm\n481@1700rpm\n    \n      BMW\n    \n    i8\n2016\n$140,700\n28c29h\n357@5800rpm\n420@3700rpm\n    M6\n2016\n$113,400\n15c22h\n560@6000rpm\n500@1500rpm\n  \n  \n  \n\n\n\n\nThe table is now quite good looking! We can set a title and subtitle using the tab_header() function. In this example, we add a title “German Automobiles” and an emoji-inclusive subtitle using the emojifont package. To clarify the term “MSRP” for unfamiliar readers, we can use a footnote. Footnotes can be added to individual cells either by specifying the row number or using expressions - here, the most expensive car is the BMW i8, which is also electric! Like tab_style, you need to provide the location and footnote text. Don’t forget to add your data source with the tab_source_note function. Wrapping text in md() allows us to utilize Markdown syntax for formatting, including adding links.\n\n1(table_header &lt;- table_format %&gt;%\n2  tab_header(title = \"German Automobiles\", subtitle = paste0(\"These are some nice \", emojifont::emoji(\"car\"), \"s\")) %&gt;%\n3  tab_footnote(locations = cells_column_labels(columns = msrp), footnote = \"Manufacturer's Suggested Retail Price in USD\") %&gt;%\n4  tab_footnote(locations = cells_body(msrp, msrp == max(msrp)), footnote = \"Electric cars used to be expensive\") %&gt;%\n5  tab_source_note(source_note = md(\"Source: **gtcars** [dataset from gt package](https://gt.rstudio.com/articles/gt-datasets.html)\")))\n\n\n1\n\nStart with the previously formatted table.\n\n2\n\nAdd a title and a subtitle to the table with an emoji.\n\n3\n\nAdd a footnote to clarify what MSRP stands for.\n\n4\n\nAdd a specific footnote to the most expensive car, noting that electric cars used to be expensive.\n\n5\n\nAdd a source note to cite the source of the data.\n\n\n\n\n\n\n\n\n  \n    \n      German Automobiles\n    \n    \n      These are some nice 🚗s\n    \n    \n      \n      Year\n      MSRP1\n      \n        Performance\n      \n    \n    \n      MPG\n      Horse Power\n      Torque\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n$115,900\n11c20h\n430@7900rpm\n317@4500rpm\n    S8\n2016\n$114,900\n15c25h\n520@5800rpm\n481@1700rpm\n    \n      BMW\n    \n    i8\n2016\n2 $140,700\n28c29h\n357@5800rpm\n420@3700rpm\n    M6\n2016\n$113,400\n15c22h\n560@6000rpm\n500@1500rpm\n  \n  \n    \n      Source: gtcars dataset from gt package\n    \n  \n  \n    \n      1 Manufacturer's Suggested Retail Price in USD\n    \n    \n      2 Electric cars used to be expensive\n    \n  \n\n\n\n\nTo take your gt tables a step further, you can apply themes using the gtExtras package. This package offers a variety of pre-built themes that can be easily added to your tables, providing them with a consistent and polished look. After installing and loading the gtExtras package, you can use one of its theme functions to apply a specific style to your table. This approach makes it simple to create visually appealing tables with minimal effort. Let’s add a theme similar to the Five Thirty Eight website.\n\n(table_themed &lt;- table_header %&gt;%\n1  gtExtras::gt_theme_538())\n\n\n1\n\nAdd Five Thirty Eight theme to the table\n\n\n\n\n\n\n\n\n  \n    \n      German Automobiles\n    \n    \n      These are some nice 🚗s\n    \n    \n      \n      Year\n      MSRP1\n      \n        Performance\n      \n    \n    \n      MPG\n      Horse Power\n      Torque\n    \n  \n  \n    \n      Audi\n    \n    R8\n2015\n$115,900\n11c20h\n430@7900rpm\n317@4500rpm\n    S8\n2016\n$114,900\n15c25h\n520@5800rpm\n481@1700rpm\n    \n      BMW\n    \n    i8\n2016\n2 $140,700\n28c29h\n357@5800rpm\n420@3700rpm\n    M6\n2016\n$113,400\n15c22h\n560@6000rpm\n500@1500rpm\n  \n  \n    \n      Source: gtcars dataset from gt package\n    \n  \n  \n    \n      1 Manufacturer's Suggested Retail Price in USD\n    \n    \n      2 Electric cars used to be expensive\n    \n  \n\n\n\n\nAfter creating a visually appealing table, you’ll likely want to save it. Although there isn’t a direct method for saving to Excel, the gt package supports saving the table in various formats such as .html for embedding in websites or emails, .png for images, .tex for LaTeX documents, .pdf for PDF files, .rtf for RTF files, and .docx for Microsoft Word documents.\n\n# Save the `gt` table as an HTML file using `gtsave()`. Use the `inline_css = TRUE` option for inlining CSS, which is useful when embedding the table in an HTML email. Without the `inline_css` option, the HTML file will have embedded CSS styles instead.\n\ntable_themed %&gt;% gtsave(filename = \"tab_1.html\", inline_css = TRUE)\ntable_themed %&gt;% gtsave(filename = \"tab_1.html\")\n\n# Saving a table as a PNG file creates a cropped image of the HTML table, and you can adjust the whitespace around it using the expand option. You also need to install.package(\"webshot2\").\ntable_themed %&gt;% gtsave(\"tab_1.png\", expand = 10)\n\n# LaTeX\ntable_themed %&gt;% gtsave(\"tab_1.tex\")\n# RTF\ntable_themed %&gt;% gtsave(\"tab_1.rtf\")\n# Microsoft Word\ntable_themed %&gt;% gtsave(\"tab_1.doc\")\n\nAnd the final output will look something like this:\nHere’s how your code can be transferred into an annotated version:\n\n1library(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(emojifont)\n\n# Create a formatted table of German automobiles \n2gt::gtcars %&gt;%\n3  filter(ctry_origin == \"Germany\") %&gt;%\n4  group_by(mfr) %&gt;%\n5  arrange(desc(msrp)) %&gt;%\n6  gt(rowname_col = \"model\") %&gt;%\n7  cols_hide(columns = c(trim, bdy_style, drivetrain, ctry_origin, trsmn)) %&gt;%\n8  cols_move(columns = c(msrp, trsmn, mpg_c, mpg_h), after = trim) %&gt;%\n9  tab_spanner(columns = c(mpg_c, mpg_h, hp, hp_rpm, trq, trq_rpm), label = \"Performance\") %&gt;%\n10  cols_merge(columns = c(hp, hp_rpm), pattern = \"{1}&lt;br&gt;@{2}rpm\") %&gt;%\n11  cols_merge(columns = c(trq, trq_rpm), pattern = \"{1}&lt;br&gt;@{2}rpm\") %&gt;%\n12  cols_merge(columns = c(mpg_c, mpg_h), pattern = \"{1}c&lt;br&gt;{2}h\") %&gt;%\n13  cols_label(\n    year = \"Year\",\n    msrp = \"MSRP\",\n    mpg_c = \"MPG\",\n    hp = \"Horse Power\",\n    trq = \"Torque\"\n  ) %&gt;%\n14  fmt_currency(columns = msrp, decimals = 0, currency = \"USD\") %&gt;%\n15  cols_align(columns = c(mpg_c, hp, trq), align = \"center\") %&gt;%\n16  tab_style(\n    style = cell_text(size = \"12px\"),\n    locations = cells_body(columns = c(mpg_c, hp, trq))\n  ) %&gt;%\n17  data_color(columns = msrp, colors = c(\"white\", \"aquamarine\")) %&gt;%\n18  tab_header(title = \"German Automobiles\", subtitle = paste0(\"These are some nice \", emojifont::emoji(\"car\"), \"s\")) %&gt;%\n19  tab_footnote(locations = cells_column_labels(columns = msrp), footnote = \"Manufacturer's Suggested Retail Price in USD\") %&gt;%\n20  tab_footnote(locations = cells_body(msrp, msrp == max(msrp)), footnote = \"Electric cars used to be expensive\") %&gt;%\n21  tab_source_note(source_note = md(\"Source: **gtcars** [dataset from gt package](https://gt.rstudio.com/articles/gt-datasets.html)\")) %&gt;%\n22  gtExtras::gt_theme_538()\n\n\n1\n\nLoad the necessary libraries for data manipulation and visualization.\n\n2\n\nLoad the data.\n\n3\n\nFilter the data to include only cars that originated from Germany.\n\n4\n\nGroup the data by the manufacturer.\n\n5\n\nArrange the data in descending order based on the Manufacturer’s Suggested Retail Price (MSRP).\n\n6\n\nStart a new gt table using the car models as row names.\n\n7\n\nHide certain columns that aren’t needed in the final table.\n\n8\n\nMove specific columns to appear after the trim column.\n\n9\n\nCreate a spanner header for performance-related columns.\n\n10\n\nMerge the horsepower (hp) and horsepower rpm (hp_rpm) columns.\n\n11\n\nMerge the torque (trq) and torque rpm (trq_rpm) columns.\n\n12\n\nMerge the city mpg (mpg_c) and highway mpg (mpg_h) columns.\n\n13\n\nSet custom labels for specific columns.\n\n14\n\nFormat the MSRP column as currency in USD.\n\n15\n\nCenter align specific columns.\n\n16\n\nApply a specific text style to certain columns.\n\n17\n\nApply a data-driven color scale to the MSRP column.\n\n18\n\nAdd a title and subtitle to the table.\n\n19\n\nAdd a footnote to clarify what MSRP stands for.\n\n20\n\nAdd a footnote to the most expensive car, noting that electric cars used to be expensive.\n\n21\n\nAdd a source note to cite the source of the data.\n\n22\n\nApply a theme to the table for a more polished look.\n\n\n\n\n\n\n\n\n  \n    \n      German Automobiles\n    \n    \n      These are some nice 🚗s\n    \n    \n      \n      Year\n      MSRP1\n      \n        Performance\n      \n    \n    \n      MPG\n      Horse Power\n      Torque\n    \n  \n  \n    \n      BMW\n    \n    i8\n2016\n2 $140,700\n28c29h\n357@5800rpm\n420@3700rpm\n    M6\n2016\n$113,400\n15c22h\n560@6000rpm\n500@1500rpm\n    M5\n2016\n$94,100\n15c22h\n560@6000rpm\n500@1500rpm\n    6-Series\n2016\n$77,300\n20c30h\n315@5800rpm\n330@1400rpm\n    M4\n2016\n$65,700\n17c24h\n425@5500rpm\n406@1850rpm\n    \n      Mercedes-Benz\n    \n    AMG GT\n2016\n$129,900\n16c22h\n503@6250rpm\n479@1750rpm\n    SL-Class\n2016\n$85,050\n20c27h\n329@5250rpm\n354@1600rpm\n    \n      Audi\n    \n    R8\n2015\n$115,900\n11c20h\n430@7900rpm\n317@4500rpm\n    S8\n2016\n$114,900\n15c25h\n520@5800rpm\n481@1700rpm\n    RS 7\n2016\n$108,900\n15c25h\n560@5700rpm\n516@1750rpm\n    S7\n2016\n$82,900\n17c27h\n450@5800rpm\n406@1400rpm\n    S6\n2016\n$70,900\n18c27h\n450@5800rpm\n406@1400rpm\n    \n      Porsche\n    \n    911\n2016\n$84,300\n20c28h\n350@7400rpm\n287@5600rpm\n    Panamera\n2016\n$78,100\n18c28h\n310@6200rpm\n295@3750rpm\n    718 Boxster\n2017\n$56,000\n21c28h\n300@6500rpm\n280@1950rpm\n    718 Cayman\n2017\n$53,900\n20c29h\n300@6500rpm\n280@1950rpm\n  \n  \n    \n      Source: gtcars dataset from gt package\n    \n  \n  \n    \n      1 Manufacturer's Suggested Retail Price in USD\n    \n    \n      2 Electric cars used to be expensive"
  },
  {
    "objectID": "chapters/tables.html#dt-tables",
    "href": "chapters/tables.html#dt-tables",
    "title": "33  Make Tables",
    "section": "33.2 DT Tables",
    "text": "33.2 DT Tables\nWhen you need to present data to important stakeholders, static HTML tables can sometimes fall short. The DT package in R can be a great solution for this, as it lets you include interactive tables in your reports or analyses. DT is an interface to the JavaScript library DataTables, which is a plug-in for the jQuery JavaScript library. It’s a flexible tool for displaying data in tables, with lots of customization options. Unlike the gt package, which is designed more for creating tables ready for publication, DT is really about making tables interactive for better data exploration.\nThe DT package lets you sort, filter, and paginate tables, which is really useful when you’re dealing with larger datasets. It also has features like filters for individual columns, and the ability to hide or rearrange columns. Plus, there are several options for how you present your data. We’ll show you how to use the DT package to create a table with a dataset of German automobiles in the next section.\n\n# Load the necessary libraries\nlibrary(DT)\nlibrary(dplyr)\n\n# Filter the gtcars dataset for German cars only \ngerman_cars &lt;- gt::gtcars %&gt;%\n1  dplyr::filter(ctry_origin == \"Germany\")\n\n# Display the data as a DT table\ndatatable(\n2  german_cars,\n3  extensions = 'Buttons',\n  options = list(\n4    dom = 'Bfrtip',\n5    buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),\n6    pageLength = 5,\n7    autoWidth = TRUE,\n8    order = list(list(1, 'asc')),\n9    lengthMenu = list(c(5, 10, 15, -1), c('5 rows', '10 rows', '15 rows', 'Show all')),\n10    searchHighlight = TRUE,\n11    searching = TRUE,\n12    lengthChange = TRUE\n  )\n)\n\n\n1\n\nFilter the gtcars dataset to include only cars that originate from Germany.\n\n2\n\nPass the german_cars dataframe to the datatable function to display it as a DT table.\n\n3\n\nEnable the Buttons extension, which adds various functionalities to the table.\n\n4\n\nThe dom option is used to define the table control elements to be displayed on the page and their order.\n\n5\n\nAdd buttons to the table that allow for various actions, such as copying the table data and exporting it in different formats.\n\n6\n\nSet the number of rows to be displayed per page.\n\n7\n\nAutomatically adjust the column width to the content.\n\n8\n\nSet the initial sorting order for the table.\n\n9\n\nSpecify the options for the “entries per page” dropdown and their display text.\n\n10\n\nHighlight any text in the table that matches the filter.\n\n11\n\nEnable the ability to filter data in the table.\n\n12\n\nAllow the user to change the number of entries shown per page.\n\n\n\n\n\n\n\n\n\n\nExtensions: The Buttons extension has been enabled which adds buttons to the table for various actions, such as copy, csv, excel, pdf, and print.\ndom: The dom option defines the table control elements to appear on the page and in what order. The value 'Bfrtip' means Buttons, filtering input, information summary, table, and pagination controls respectively.\nautoWidth: This option will adjust the column widths automatically to the content.\norder: This option sets the initial sorting order for the table. It takes a list where each element is a list containing a column index and a sort direction (‘asc’ for ascending and ‘desc’ for descending).\nlengthMenu: This option specifies the options for entries per page dropdown and their display text.\nsearchHighlight: When this option is set to TRUE, any text in the table that matches the filter will be highlighted.\nsearching: This option enables or disables the ability to filter data in the table.\nlengthChange: This option allows or disallows the user from changing the number of entries shown per page.\n\nWith these options, you can customize your DT tables to fit your exact needs."
  },
  {
    "objectID": "chapters/epilogue.html",
    "href": "chapters/epilogue.html",
    "title": "Epilogue",
    "section": "",
    "text": "In the realm of data science and analytics, the path to mastery is not a sprint but a marathon. Acquiring the skills and tools is not a one-time affair; it’s a continuous journey of exploration, curiosity, and growth. Just as a martial artist doesn’t stop training upon receiving their black belt, a data enthusiast doesn’t halt their learning once they’ve grasped the foundational tools. The black belt signifies the beginning of true mastery, not its culmination.\nIt’s tempting, in our fast-paced world, to focus on immediate gains, to seek the quick wins that will propel us forward in the short term. But true expertise, the kind that sets you apart and allows you to make meaningful contributions, requires a longer view. It demands that we look beyond the horizon of the next month and envision where we want to be in 5 years, or even a decade.\nAs you embark on this journey, remember that the landscape of data science is ever-evolving. New tools emerge, methodologies adapt, and the questions we seek to answer become more complex. Embrace the continuous learning process, nurture your curiosity, and set your sights on the long-term. For in this field, the journey itself is the destination, and every step you take enriches your understanding and expertise."
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Abadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge.\n2017. “When Should You Adjust Standard Errors for\nClustering?” Cambridge, MA. https://doi.org/10.3386/w24003.\n\n\nBai, Jushan. 2009. “Panel Data Models with Interactive Fixed\nEffects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ecta6135.\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nBansak, Kirk, Jens Hainmueller, Daniel J Hopkins, and Teppei Yamamoto.\n2021. “Beyond the Breaking Point? Survey Satisficing in Conjoint\nExperiments.” Political Science Research and Methods 9\n(1): 53–71.\n\n\nBorkin, M., K. Gajos, A. Peters, D. Mitsouras, S. Melchionna, F.\nRybicki, C. Feldman, and H. Pfister. 2011. “Evaluation of Artery\nVisualizations for Heart Disease Diagnosis.” IEEE\nTransactions on Visualization and Computer Graphics 17 (12):\n2479–88. https://doi.org/10.1109/TVCG.2011.192.\n\n\nBorland, David, and Russell M. Taylor Ii. 2007. “Rainbow Color Map\n(Still) Considered Harmful.” IEEE Computer Graphics and\nApplications 27 (2): 14–17. https://doi.org/10.1109/MCG.2007.323435.\n\n\n“Coblis  Color Blindness Simulator \nColblindor.” n.d. https://www.color-blindness.com/coblis-color-blindness-simulator/.\n\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The\nMisuse of Colour in Science Communication.” Nature\nCommunications 11 (1): 5444. https://doi.org/10.1038/s41467-020-19160-7.\n\n\nGroves, Robert M, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski,\nEleanor Singer, and Roger Tourangeau. 2011. Survey Methodology.\nJohn Wiley & Sons.\n\n\nHadley, Wickham. 2016. Ggplot2. New York, NY: Springer\nScience+Business Media, LLC.\n\n\nHester, Jenny Bryan, the STAT 545 TAs, Jim. n.d. Let’s\nGit Started | Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nHoerger, Michael. 2010. “Participant Dropout as a Function of\nSurvey Length in Internet-Mediated University Studies: Implications for\nStudy Design and Voluntary Participation in Psychological\nResearch.” Cyberpsychology, Behavior, and Social\nNetworking 13 (6): 697–700.\n\n\nJakobsen, Janus Christian, Christian Gluud, Jørn Wetterslev, and Per\nWinkel. 2017. “When and How Should Multiple Imputation Be Used for\nHandling Missing Data in Randomised Clinical Trials  a\nPractical Guide with Flowcharts.” BMC Medical Research\nMethodology 17 (1): 162. https://doi.org/10.1186/s12874-017-0442-1.\n\n\nJarrett, Caroline. 2021. Surveys That Work: A Practical Guide for\nDesigning and Running Better Surveys. Rosenfeld Media.\n\n\nJarrett, Caroline, and Gerry Gaffney. 2009. Forms That Work:\nDesigning Web Forms for Usability. Morgan Kaufmann.\n\n\nJohn, Leslie K, Alessandro Acquisti, and George Loewenstein. 2011.\n“Strangers on a Plane: Context-Dependent Willingness to Divulge\nSensitive Information.” Journal of Consumer Research 37\n(5): 858–73.\n\n\nKrosnick, Jon A. 2018. “Questionnaire Design.” The\nPalgrave Handbook of Survey Research, 439–55.\n\n\nKrosnick, Jon A, Allyson L Holbrook, Matthew K Berent, Richard T Carson,\nW Michael Hanemann, Raymond J Kopp, Robert Cameron Mitchell, Stanley\nPresser, Paul A Ruud, and V Kerry Smith. 2002. “The Impact\nof\" No Opinion\" Response Options on Data\nQuality: Non-Attitude Reduction or an Invitation to Satisfice?”\nPublic Opinion Quarterly 66 (3): 371–403.\n\n\nPopper, K. R. 2002. The Logic of Scientific Discovery. ISSR\nLibrary. Routledge. https://books.google.com/books?id=Yq6xeupNStMC.\n\n\nRogowitz, B. E., and L. A. Treinish. 1998. “Data Visualization:\nThe End of the Rainbow.” IEEE Spectrum 35 (12): 52–59.\nhttps://doi.org/10.1109/6.736450.\n\n\nSibinga, Eva, and Erin Waldron. 2021. “Cognitive Load as a Guide:\n12 Spectrums to Improve Your Data Visualizations | Nightingale.”\nhttps://nightingaledvs.com/cognitive-load-as-a-guide-12-spectrums-to-improve-your-data-visualizations/.\n\n\nThe Grammar of Graphics. 2005. Statistics and Computing. New\nYork: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n\n\nTourangeau, Roger, Lance J Rips, and Kenneth Rasinski. 2000. “The\nPsychology of Survey Response.”\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative\nInformation. 2nd ed. Cheshire, Conn: Graphics Press.\n\n\nVagias, Wade M. 2006. “Likert-Type Scale Response Anchors.”\nClemson International Institute for Tourism & Research\nDevelopment, Department of Parks, Recreation and Tourism Management.\nClemson University.\n\n\nVriesema, Christine Calderon, and Hunter Gehlbach. 2021.\n“Assessing Survey Satisficing: The Impact of Unmotivated\nQuestionnaire Responding on Data Quality.” Educational\nResearcher 50 (9): 618–27.\n\n\nWare, Colin. 2021. Information Visualization: Perception for\nDesign. Fourth edition. Cambridge, MA: Morgan Kaufmann, Inc.\n\n\nWickham, Hadley. 2014. “Tidy Data.”\nJournal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\nWigmore, Steve. 2022. “What Is a Good Survey Length for Online\nResearch?” https://www.kantar.com/north-america/inspiration/research-services/what-is-a-good-survey-length-for-online-research-pf.\n\n\nWilson, Seth. 2011. “Inkfumes: Poster Designs: Color, Design,\nTypography Theory.” http://inkfumes.blogspot.com/2011/10/poster-designs-color-design-typography.html."
  }
]